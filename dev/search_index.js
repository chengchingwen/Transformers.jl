var documenterSearchIndex = {"docs":
[{"location":"getstarted/#Get-Started","page":"Get Started","title":"Get Started","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Transformers.jl contain multiple functionalities, from basic building block for a transformer model to using pretrained  model from huggingface. Each of them is put under different submodule in Transformers.jl.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"You should find more examples in the example folder.","category":"page"},{"location":"getstarted/#Create-a-N-layered-Transformer-model","page":"Get Started","title":"Create a N-layered Transformer model","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"All model building block are in the Layers module. Here we create a simple 3 layered vanilla transformer  (multi-head self-attention + MLP) model, where each attention have 4 heads:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"using Flux\nusing Transformers\n\nnum_layer = 3\nhidden_size = 128\nnum_head = 4\nhead_hidden_size = div(hidden_size, num_head)\nintermediate_size = 4hidden_size\n\ntrf_blocks = Transformer(Layers.TransformerBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"The Transformer layer, by default take a NamedTuple as input, and always return a NamedTuple.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x = randn(Float32, hidden_size, 10 #= sequence length =#, 2 #= batch size =#);\n\njulia> y = trf_blocks(\n    (; hidden_state = x) ); # The model would be apply on the `hidden_state` field.\n\njulia> keys(y)\n(:hidden_state,)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"It also works on high dimension input data:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x2 = reshape(x, hidden_size, 5, 2 #= width 5 x heigh 2 =#, 2 #= batch size =#);\n\njulia> y2 = trf_blocks( (; hidden_state = x2) );\n\njulia> y.hidden_state ≈ reshape(y2.hidden_state, size(y.hidden_state))\ntrue","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Some times you might want to see how the attention score looks like, this can be done by creating a model that return  the attention score as well. The attention score would usually be in shape (key length, query length, head,  batch size):","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"# creating new model\ntrf_blocks_ws = Transformer(Layer.TransformerBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size;\n    return_score = true)\n\n# or transform an old model\ntrf_blocks_ws = Layers.WithScore(trf_blocks)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> y = trf_blocks_ws( (; hidden_state = x) );\n\njulia> keys(y)\n(:hidden_state, :attention_score)\n","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"The model can also take an attention mask to avoid attention looking at the padding tokens. The attention mask would need  construct with NeuralAttentionlib.Masks:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> mask = Masks.LengthMask([5, 7]); # specify the sequence length of each sample in the batch\n\njulia> y3 = trf_blocks_ws( (; hidden_state = x, attention_mask = mask) );\n\njulia> keys(y3)\n(:hidden_state, :attention_mask, :attention_score)\n","category":"page"},{"location":"getstarted/#Create-a-N-layered-Transformer-Decoder-model","page":"Get Started","title":"Create a N-layered Transformer Decoder model","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"For constructing the transformer decoder in the encoder-decoder architecture:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"trf_dec_blocks_ws = Transformer(Layers.TransformerDecoderBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size;\n    return_score = true)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x3 = x = randn(Float32, hidden_size, 7 #= sequence length =#, 2 #= batch size =#);\n\njulia> z = trf_dec_blocks_ws( (; hidden_state = x3, memory = y.hidden_state #= encoder output =#) );\n\njulia> keys(z)\n(:hidden_state, :memory, :cross_attention_score)\n\njulia> size(z.cross_attention_score) # (key length, query length, head, batch size)\n(10, 7, 4, 2)\n","category":"page"},{"location":"getstarted/#Preprocessing-Text","page":"Get Started","title":"Preprocessing Text","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Text processing functionalities are in the TextEncoders module. The TransformerTextEncoder take a tokenize function  and a list of String as the vocabulary. If the tokenize function is omitted, it would use WordTokenizers.tokenize  as the default. Here we create a text encoder that split on every Char and only know 4 characters.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"using Transformers.TextEncoders\n\nchar_tenc = TransformerTextEncoder(Base.Fix2(split, \"\"), map(string, ['A', 'T', 'C', 'G']))","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> char_tenc\nTransformerTextEncoder(\n├─ TextTokenizer(WordTokenization(split_sentences = WordTokenizers.split_sentences, tokenize = Base.Fix2{typeof(split), String}(split, \"\"))),\n├─ vocab = Vocab{String, SizedArray}(size = 8, unk = <unk>, unki = 6),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := TextEncodeBase.with_head_tail(<s>, </s>)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\njulia> data = encode(char_tenc, \"ATCG\")\n(token = Bool[0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 1], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6]))\n\njulia> data.token\n8x6 OneHotArray{8, 2, Vector{OneHot{0x00000008}}}:\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  0\n 0  0  0  0  0  0\n 1  0  0  0  0  0\n 0  0  0  0  0  1\n\njulia> decode(char_tenc, data.token)\n6-element Vector{String}:\n \"<s>\"\n \"A\"\n \"T\"\n \"C\"\n \"G\"\n \"</s>\"\n\njulia> data2 = encode(char_tenc, [\"ATCG\", \"AAAXXXX\"])\n(token = [0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 0;;; 0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 1], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6, 9]))\n\njulia> decode(char_tenc, data2.token)\n9×2 Matrix{String}:\n \"<s>\"    \"<s>\"\n \"A\"      \"A\"\n \"T\"      \"A\"\n \"C\"      \"A\"\n \"G\"      \"<unk>\"\n \"</s>\"   \"<unk>\"\n \"<pad>\"  \"<unk>\"\n \"<pad>\"  \"<unk>\"\n \"<pad>\"  \"</s>\"\n","category":"page"},{"location":"getstarted/#Using-(HuggingFace)-Pre-trained-Models","page":"Get Started","title":"Using (HuggingFace) Pre-trained Models","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Use the HuggingFace module for loading the pretrained model. The @hgf_str return a text encoder of the model, and  the model itself.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> bertenc, bert_model = hgf\"bert-base-cased\";\n\njulia> bert_model(encode(bertenc, \"Peter Piper picked a peck of pickled peppers\"))\n(hidden_state = [0.54055643 -0.3517502 … 0.2955708 1.601667; 0.05538677 -0.1114391 … -0.2139448 0.3692414; … ; 0.34500372 0.38523915 … 0.2224255 0.7384993; -0.18260899 -0.05137573 … -0.2833455 -0.23427412;;;], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[13]), pooled = Float32[-0.6727301; 0.42062035; … ; -0.902852; 0.99214816;;])\n","category":"page"},{"location":"getstarted/#GPU","page":"Get Started","title":"GPU","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Transformers relies on CUDA.jl for the GPU stuffs. In Flux we normally use Flux.gpu to convert model or data to  the device. In Transformers, we provide another 2 api (enable_gpu and todevice) for this. If enable_gpu(true) is  set, todevice will be moving data to GPU device, otherwise it is copying data on CPU. notice: enable_gpu should  only be called in script, it cannot be used during precompilation.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"enable_gpu\ntodevice","category":"page"},{"location":"getstarted/#Transformers.enable_gpu","page":"Get Started","title":"Transformers.enable_gpu","text":"enable_gpu(t=true)\n\nEnable gpu for todevice, disable with enable_gpu(false).\n\n\n\n\n\n","category":"function"},{"location":"getstarted/#Transformers.todevice","page":"Get Started","title":"Transformers.todevice","text":"todevice(x)\n\nMove data to device, only when gpu is enable with enable_gpu, basically equal Flux.gpu. Otherwise just Flux.cpu.\n\n\n\n\n\n","category":"function"},{"location":"textencoders/#Transformers.TextEncoders","page":"TextEncoders","title":"Transformers.TextEncoders","text":"","category":"section"},{"location":"textencoders/","page":"TextEncoders","title":"TextEncoders","text":"Text processing module.","category":"page"},{"location":"textencoders/#API-Reference","page":"TextEncoders","title":"API Reference","text":"","category":"section"},{"location":"textencoders/","page":"TextEncoders","title":"TextEncoders","text":"Modules = [Transformers.TextEncoders]\nOrder = [:type, :function]","category":"page"},{"location":"textencoders/#Transformers.TextEncoders.BertTextEncoder","page":"TextEncoders","title":"Transformers.TextEncoders.BertTextEncoder","text":"BertTextEncoder\n\nThe text encoder for Bert model (WordPiece tokenization).\n\n\n\n\n\n","category":"type"},{"location":"textencoders/#Transformers.TextEncoders.GPT2TextEncoder","page":"TextEncoders","title":"Transformers.TextEncoders.GPT2TextEncoder","text":"GPT2TextEncoder\n\nThe text encoder for GPT2 model (ByteLevel BytePairEncoding tokenization).\n\n\n\n\n\n","category":"type"},{"location":"textencoders/#Transformers.TextEncoders.T5TextEncoder","page":"TextEncoders","title":"Transformers.TextEncoders.T5TextEncoder","text":"T5TextEncoder\n\nThe text encoder for T5 model (SentencePiece tokenization).\n\n\n\n\n\n","category":"type"},{"location":"textencoders/#Transformers.TextEncoders.TransformerTextEncoder","page":"TextEncoders","title":"Transformers.TextEncoders.TransformerTextEncoder","text":"struct TransformerTextEncoder{\n    T<:AbstractTokenizer, V<:AbstractVocabulary{String}, P\n} <: AbstractTransformerTextEncoder\n    tokenizer::T\n    vocab::V\n    process::P\n    startsym::String\n    endsym::String\n    padsym::String\n    trunc::Union{Nothing, Int}\nend\n\nThe text encoder for general transformers. Taking a tokenizer, vocabulary, and a processing function, configured with  a start symbol, an end symbol, a padding symbol, and a maximum length.\n\nTransformerTextEncoder(tokenze, vocab, process; trunc = nothing,\n                       startsym = \"<s>\", endsym = \"</s>\", unksym = \"<unk>\", padsym = \"<pad>\")\n\ntokenize can be any tokenize function from WordTokenizers. vocab is either a list of word or a Vocab.  process can be omitted, then a predefined processing pipeline will be used. When vocab is a list, those  special symbol (e.g. padsym) would be added to the word list.\n\nTransformerTextEncoder(f, e::TransformerTextEncoder)\n\nTake a text encoder and create a new text encoder with same configuration except the processing function.  f is a function that take the encoder and return a new process function. This is useful for changing part of  the procssing function.\n\nExample\n\njulia> textenc = TransformerTextEncoder(labels; startsym, endsym, unksym,\n                                        padsym = unksym, trunc = 100)\nTransformerTextEncoder(\n├─ TextTokenizer(default),\n├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = </unk>, unki = 1),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = </unk>,\n├─ trunc = 100,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := TextEncodeBase.with_head_tail(<s>, </s>)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(10))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(10, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\njulia> TransformerTextEncoder(ans) do enc\n           enc.process[1] |> TextEncoders.Pipelines(enc.process[4:5]) |> TextEncoders.PipeGet{(:token,)}()\n       end\nTransformerTextEncoder(\n├─ TextTokenizer(default),\n├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = </unk>, unki = 1),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = </unk>,\n├─ trunc = 100,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(10, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token)\n)\n\n\n\n\n\n\n","category":"type"},{"location":"textencoders/#TextEncodeBase.decode-Tuple{Transformers.TextEncoders.AbstractTransformerTextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.decode","text":"decode(e::AbstractTransformerTextEncoder, x::Union{\n    Integer,\n    OneHotArray,\n    AbstractArray{<:Integer}\n})\n\nDecode the one-hot encoding or indices into String (or Array{String}) from the bound vocabulary.\n\ndecode(e::AbstractTransformerTextEncoder, x::AbstractArray)\n\nPerform argmax(x; dims = 1) and then decode. x should be collected beforehand if it's on GPU.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.decode-Tuple{Transformers.TextEncoders.BertTextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.decode","text":"decode(bertenc::BertTextEncoder, x)\n\nConvert indices back to string with bert vocabulary.\n\nSee also: encode\n\nExample\n\njulia> token = encode(bertenc, [[\"this is a sentence\", \"and another\"]]).token;\n\njulia> decode(bertenc, token)\n9×1 Matrix{String}:\n \"[CLS]\"\n \"this\"\n \"is\"\n \"a\"\n \"sentence\"\n \"[SEP]\"\n \"and\"\n \"another\"\n \"[SEP]\"\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.decode-Tuple{Transformers.TextEncoders.GPT2TextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.decode","text":"decode(bertenc::GPT2TextEncoder, x)\n\nConvert indices back to string with gpt2 vocabulary. This would also map the bytes back to the normal code ranges,  so the string is not directly the one in the vocabulary.\n\nSee also: encode\n\nExample\n\njulia> token = encode(gpt2enc, [[\"this is a sentence\", \"and another\"]]).token;\n\njulia> decode(gpt2enc, token)\n6×1 Matrix{String}:\n \"this\"\n \" is\"\n \" a\"\n \" sentence\"\n \"and\"\n \" another\"\n\njulia> TextEncodeBase.decode_indices(gpt2enc, token)\n6×1 Matrix{String}:\n \"this\"\n \"Ġis\"\n \"Ġa\"\n \"Ġsentence\"\n \"and\"\n \"Ġanother\"\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.decode-Tuple{Transformers.TextEncoders.T5TextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.decode","text":"decode(bertenc::T5TextEncoder, x)\n\nConvert indices back to string with t5 vocabulary.\n\nSee also: encode\n\nExample\n\njulia> token = encode(t5enc, [[\"this is a sentence\", \"and another\"]]).token;\n\njulia> decode(t5enc, token)\n9×1 Matrix{String}:\n \"▁this\"\n \"▁is\"\n \"▁\"\n \"a\"\n \"▁sentence\"\n \"</s>\"\n \"▁and\"\n \"▁another\"\n \"</s>\"\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.encode-Tuple{Transformers.TextEncoders.AbstractTransformerTextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.encode","text":"encode(e::AbstractTransformerTextEncoder, input::Union{\n    String,                         # single sentence\n    Vector{String},                 # batch of sentences\n    Vector{Vector{String}},         # batch of multi-segment sentences\n    Vector{Vector{Vector{String}}}  # batch of multi-sample multi-segment sentences\n})\n\nTokenize the input and apply the processing function on the tokenized result. The input can be either a single  String (1 sample) or a nested vector of String up to depth 3 (batch of samples). How batch input is transformed  is defined by the bound processing function. The result of the processing function (first if return tuple) would be  converted into one-hot encoding with the bound vocabulary.\n\nencode(e::AbstractTransformerTextEncoder, src, trg)\n\nApply encode on src and trg and build the cross attention mask. This is just a convenient function for doing  encoder-decoder tasks. Return a @NamedTuple{encoder_input, decoder_input} where encoder_input is just  encode(e, src) and decoder_input is encode(e, trg) + the cross attention mask.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.encode-Tuple{Transformers.TextEncoders.BertTextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.encode","text":"encode(::BertTextEncoder, ::String)\n\nEncode a single sentence with bert text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 1}, segment::Vector{Int}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::BertTextEncoder, ::Vector{String})\n\nEncode a batch of sentences with bert text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 2}, segment::Matrix{Int}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::BertTextEncoder, ::Vector{Vector{String}})\n\nEncode a batch of segments with bert text encoder. Segments would be concatenate together as batch of sentences with  separation token and correct indicator in segment. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 2}, segment::Matrix{Int}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::BertTextEncoder, ::Vector{Vector{Vector{String}}})\n\nEncode a batch of multi-sample segments with bert text encoder. The number of sample per data need to be the same.  (e.g. length(batch[1]) == length(batch[2])). The default pipeline returning  @NamedTuple{token::OneHotArray{K, 3}, segment::Array{Int, 3}, attention_mask::LengthMask{2, Matrix{Int32}}}.  notice: If you want each sample to be independent to each other, this need to be reshaped before feeding to  transformer layer or make sure the attention is not taking the end-1 dimension as another length dimension.\n\nSee also: decode, LengthMask\n\nExample\n\njulia> bertenc = HuggingFace.load_tokenizer(\"bert-base-cased\")\nBertTextEncoder(\n├─ TextTokenizer(MatchTokenization(WordPieceTokenization(bert_cased_tokenizer, WordPiece(vocab_size = 28996, unk = [UNK], max_char = 100)), 5 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 28996, unk = [UNK], unki = 101),\n├─ startsym = [CLS],\n├─ endsym = [SEP],\n├─ padsym = [PAD],\n├─ trunc = 512,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[(token, segment)] := SequenceTemplate{String}([CLS]:<type=1> Input[1]:<type=1> [SEP]:<type=1> (Input[2]:<type=2> [SEP]:<type=2>)...)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(512))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(512, [PAD], tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target[segment] := TextEncodeBase.trunc_and_pad(512, 1, tail, tail)(target.segment)\n  ╰─ target[segment] := TextEncodeBase.nested2batch(target.segment)\n  ╰─ target := (target.token, target.segment, target.attention_mask)\n)\n\njulia> e = encode(bertenc, [[\"this is a sentence\", \"and another\"]])\n(token = [0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;;], segment = [1; 1; … ; 2; 2;;], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[9]))\n\njulia> typeof(e)\nNamedTuple{(:token, :segment, :attention_mask), Tuple{OneHotArray{0x00007144, 2, 3, Matrix{OneHot{0x00007144}}}, Matrix{Int64}, NeuralAttentionlib.LengthMask{1, Vector{Int32}}}}\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.encode-Tuple{Transformers.TextEncoders.GPT2TextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.encode","text":"encode(::GPT2TextEncoder, ::String)\n\nEncode a single sentence with gpt2 text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 1}, attention_mask::RevLengthMask{1, Vector{Int32}}}.\n\nencode(::GPT2TextEncoder, ::Vector{String})\n\nEncode a batch of sentences with gpt2 text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 2}, attention_mask::RevLengthMask{1, Vector{Int32}}}.\n\nencode(::GPT2TextEncoder, ::Vector{Vector{String}})\n\nEncode a batch of segments with gpt2 text encoder. Segments would be concatenate together as batch of sentences.  The default pipeline returning @NamedTuple{token::OneHotArray{K, 2}, attention_mask::RevLengthMask{1, Vector{Int32}}}.\n\nencode(::GPT2TextEncoder, ::Vector{Vector{Vector{String}}})\n\nEncode a batch of multi-sample segments with gpt2 text encoder. The number of sample per data need to be the same.  (e.g. length(batch[1]) == length(batch[2])). The default pipeline returning  @NamedTuple{token::OneHotArray{K, 3}, attention_mask::RevLengthMask{2, Matrix{Int32}}}.  notice: If you want each sample to be independent to each other, this need to be reshaped before feeding to  transformer layer or make sure the attention is not taking the end-1 dimension as another length dimension.\n\nSee also: decode, RevLengthMask\n\nExample\n\njulia> gpt2enc = HuggingFace.load_tokenizer(\"gpt2\")\nGPT2TextEncoder(\n├─ TextTokenizer(MatchTokenization(CodeNormalizer(BPETokenization(GPT2Tokenization, bpe = CachedBPE(BPE(50000 merges))), codemap = CodeMap{UInt8 => UInt16}(3 code-ranges)), 1 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 50257, unk = <unk>, unki = 0),\n├─ codemap = CodeMap{UInt8 => UInt16}(3 code-ranges),\n├─ startsym = <|endoftext|>,\n├─ endsym = <|endoftext|>,\n├─ padsym = <|endoftext|>,\n├─ trunc = 1024,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[token] := SequenceTemplate{String}((Input:<type=1>)...)(Val{1}(), target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.RevLengthMask ∘ Transformers.TextEncoders.getlengths(1024))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(1024, <|endoftext|>, head, head)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\njulia> e = encode(gpt2enc, [[\"this is a sentence\", \"and another\"]])\n(token = [0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;;], attention_mask = NeuralAttentionlib.RevLengthMask{1, Vector{Int32}}(Int32[6]))\n\njulia> typeof(e)\nNamedTuple{(:token, :attention_mask), Tuple{OneHotArray{0x0000c451, 2, 3, Matrix{OneHot{0x0000c451}}}, NeuralAttentionlib.RevLengthMask{1, Vector{Int32}}}}\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#TextEncodeBase.encode-Tuple{Transformers.TextEncoders.T5TextEncoder, Any}","page":"TextEncoders","title":"TextEncodeBase.encode","text":"encode(::T5TextEncoder, ::String)\n\nEncode a single sentence with t5 text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 1}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::T5TextEncoder, ::Vector{String})\n\nEncode a batch of sentences with t5 text encoder. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 2}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::T5TextEncoder, ::Vector{Vector{String}})\n\nEncode a batch of segments with t5 text encoder. Segments would be concatenate together as batch of sentences with a  separation token. The default pipeline returning  @NamedTuple{token::OneHotArray{K, 2}, attention_mask::LengthMask{1, Vector{Int32}}}.\n\nencode(::T5TextEncoder, ::Vector{Vector{Vector{String}}})\n\nEncode a batch of multi-sample segments with t5 text encoder. The number of sample per data need to be the same.  (e.g. length(batch[1]) == length(batch[2])). The default pipeline returning  @NamedTuple{token::OneHotArray{K, 3}, attention_mask::LengthMask{2, Matrix{Int32}}}.  notice: If you want each sample to be independent to each other, this need to be reshaped before feeding to  transformer layer or make sure the attention is not taking the end-1 dimension as another length dimension.\n\nSee also: decode, LengthMask\n\nExample\n\njulia> t5enc = HuggingFace.load_tokenizer(\"t5\")\nT5TextEncoder(\n├─ TextTokenizer(MatchTokenization(PrecompiledNormalizer(WordReplaceNormalizer(UnigramTokenization(EachSplitTokenization(splitter = isspace), unigram = Unigram(vocab_size = 32100, unk = <unk>)), pattern = r\"^(?!▁)(.*)$\" => s\"▁\u0001\"), precompiled\n= PrecompiledNorm(...)), 103 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 32100, unk = <unk>, unki = 3),\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[(token, segment)] := SequenceTemplate{String}(Input[1]:<type=1> </s>:<type=1> (Input[2]:<type=1> </s>:<type=1>)...)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\njulia> e = encode(t5enc, [[\"this is a sentence\", \"and another\"]])\n(token = [0 0 … 0 0; 0 0 … 0 1; … ; 0 0 … 0 0; 0 0 … 0 0;;;], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[9]))\n\njulia> typeof(e)\nNamedTuple{(:token, :attention_mask), Tuple{OneHotArray{0x00007d64, 2, 3, Matrix{OneHot{0x00007d64}}}, NeuralAttentionlib.LengthMask{1, Vector{Int32}}}}\n\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.bert_cased_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.bert_cased_tokenizer","text":"bert_cased_tokenizer(input)\n\nGoogle bert tokenizer which remain the case during tokenization. Recommended for multi-lingual data.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.bert_uncased_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.bert_uncased_tokenizer","text":"bert_uncased_tokenizer(input)\n\nGoogle bert tokenizer which do lower case on input before tokenization.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.gpt_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.gpt_tokenizer","text":"gpt_tokenizer(x)\n\nAn alternative for origin tokenizer (spacy tokenizer) used in gpt model.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.text_standardize-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.text_standardize","text":"text_standardize(text)\n\nThe function in the origin gpt code. Fixes some issues the spacy tokenizer had on books corpus also does  some whitespace standardization.\n\n\n\n\n\n","category":"method"},{"location":"changelog/#ChangeLogs-(from-0.1.x-to-0.2.0)","page":"ChangeLogs","title":"ChangeLogs (from 0.1.x to 0.2.0)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"v0.2 is a rewrite of the whole package. Most layers and API in 0.1 is removed or changed. Some of them are replaced  with new one. The basic policy is, if a functionality is achievable with a well-maintained package easily, or there  isn't much gain by self-hosting/maintaining it, then we remove the functionality from Transformers.jl.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Here is list of the changes with brief explanation:","category":"page"},{"location":"changelog/#Transformers.Pretrain","page":"ChangeLogs","title":"Transformers.Pretrain","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Pretrain module is entirely removed, due to the duplication of functionality v.s. Transformers.HuggingFace.  We do not host the small list of the origin official released pretrained weights anymore. All use that require a  pretrained weight should refer to HuggingFace module. This is a table of the old pretrain name and corresponding  huggingface model name:","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"old pretrain name corresponding huggingface model name\ncased_L-12_H-768_A-12 bert-base-cased\nuncased_L-12_H-768_A-12 bert-base-uncased\nchinese_L-12_H-768_A-12 bert-base-chinese\nmulti_cased_L-12_H-768_A-12 bert-base-multilingual-cased\nmultilingual_L-12_H-768_A-12 bert-base-multilingual-uncased\ncased_L-24_H-1024_A-16 bert-large-cased\nuncased_L-24_H-1024_A-16 bert-large-uncased\nwwm_cased_L-24_H-1024_A-16 bert-large-cased-whole-word-masking\nwwm_uncased_L-24_H-1024_A-16 bert-large-uncased-whole-word-masking\nscibert_scivocab_cased allenai/scibert_scivocab_cased\nscibert_scivocab_uncased allenai/scibert_scivocab_uncased\nscibert_basevocab_cased N/A\nscibert_basevocab_uncased N/A\nOpenAIftlm openai-gpt","category":"page"},{"location":"changelog/#Transformers.Stacks","page":"ChangeLogs","title":"Transformers.Stacks","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Stacks module is entirely removed. Stacks provide a small DSL for creating nontrivial Chain of layers.  However, the DSL isn't intuitive enough and it also doesn't seems worth maintaining a DSL. We don't provide  direct replacement for this, but for the specific use case of building transformer models, we have a few new  constructors/layers in Transformers.Layers.","category":"page"},{"location":"changelog/#Transformers.Basic","page":"ChangeLogs","title":"Transformers.Basic","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Basic module is now destructed and most of the elements in Basic is separated to other module/package.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Transformer and TransformerDecoder: The Transformer/TransformerDecoder layer is replaced with the new  implementation in Layers (the Layers.TransformerBlock, Layers.TransformerDecoderBlock, and friends).\nMultiheadAttention: The implementation of attention operations are move out to  NeuralAttentionlib. In NeuralAttentionlib, we can use  multihead_qkv_attention to do the same computation. Since most transformer variant only use a modified version  of self or cross attention, we do not provied the MultiheadAttention layer type. One should be able to redefine  the MultiheadAttention layer type with Flux and NeuralAttentionlib easily. For example:\nusing Flux, Functors\nusing NeuralAttentionlib: multihead_qkv_attention, CausalMask\n\nstruct MultiheadAttention{Q,K,V,O}\n    head::Int\n    future::Bool\n    iqproj::Q\n    ikproj::K\n    ivproj::V\n    oproj::O\nend\n@functor MultiheadAttention (iqproj, ikproj, ivporj, oproj)\nMultiheadAttention(head, hidden_size, head_size; future = true) =\n    MultiheadAttention(head, future,\n        Dense(hidden_size, head_size * head),\n        Dense(hidden_size, head_size * head),\n        Dense(hidden_size, head_size * head),\n        Dense(head_size * head, hidden_size),\n    )\n\n(mha::MultiheadAttention)(q, k, v) = mha.oproj(multihead_qkv_attention(mha.head,\n    mha.iqproj(q), mha.ikproj(k), mha.ivproj(v), mha.future ? nothing : CausalMask()))\nTransformerModel: This is just a Flux layer with embedding layer, transformer layer, and classifier layer   bundle together. One can define this easily with Flux/Functors API, thus removed.\nPositionwise, PwFFN, and @toNd: This was originally designed for applying Flux.Dense on 3-dim arrays,  but since Flux.Dense support multi-dim input now. This isn't needed and thus removed.\nEmbeddingDecoder: Replaced with Layers.EmbedDecoder. Name change and support extra trainable bias parameter.\nPositionEmbedding: This is replace with Layers.SinCosPositionEmbed and Layers.FixedLenPositionEmbed for  the old trainable keyword argument setting.\ncrossentropy with masking: We extend Flux.logitcrossentropy and Flux.crossentropy with 3-args  input (the prediction, label, and mask) and 4-args input (sum or mean, prediciton, label, and mask).\nkldivergence: In our use case (i.e. training language model), this is equivalent to cross-entropy, thus removed.\nlogcrossentropy/logkldivergence: This is a fault design. Originally I would put a logsoftmax at the head of  the prediction head. However, that is not only unnecessary but also increasing the amount of memory needed.  One should use Flux.logitcrossentropy without the logsoftmax directly.\nVocabulary: Replaced with TextEncodeBase.Vocab.\nwith_firsthead_tail/segment_and_concat/concat: These can be implemented with TextEncodeBase.SequenceTemplate   and friends thus removed.\ngetmask: The attention mask functionality is moved to NeuralAttentionlib. Manually construct attention mask   should use constructor in NeuralAttentionlib.Masks.","category":"page"},{"location":"changelog/#Transformers.Layers-(new)","page":"ChangeLogs","title":"Transformers.Layers (new)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Layers module is a new module introduced in v0.2.0. It provide a set layer types for construct transformer  model variants.","category":"page"},{"location":"changelog/#Transformers.TextEncoders-(new)","page":"ChangeLogs","title":"Transformers.TextEncoders (new)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The TextEncoders module is a new module introduced in v0.2.0. Basically all old functionality about text preprocessing  are moved to this module, including WordPiece, Unigram, BertTextEncoder, GPT2TextEncoder, etc.","category":"page"},{"location":"changelog/#Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain","page":"ChangeLogs","title":"Transformers.BidirectionalEncoder / Transformers.GenerativePreTrain","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"These modules are removed since we are switching to the Transformers.HuggingFace for the pretrained model. The text  encoder are moved to Transformers.TextEncoders. Weight loading and conversion functionality are removed. If you  need that, use the tools that huggingface transformers python package provided and make sure the model can be loaded  with pytorch. Then we can use the weight in pytorch format.","category":"page"},{"location":"changelog/#Transformers.HuggingFace","page":"ChangeLogs","title":"Transformers.HuggingFace","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The changes in Transformers.HuggingFace are mainly about the configurations and models. The tokenizer/textencoder part  are mostly the same, except the process functions.","category":"page"},{"location":"changelog/#Configuration","page":"ChangeLogs","title":"Configuration","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"For the configuration, the loading mechanism is changed. In previous version, each model type need to define a specific  HGF<XXModelType>Config struct where XXModelType is the model type name. The reason for that is, for some reason,  huggingface transformers doesn't serialize all the configuration values into the file, but rely on their constructor  with pre-defined default values instead. As a result, some model only need the configuration file, while some need the  python code for the defaults as well. The hgf config struct was more like a interal data carrier. You usually  won't (and actually can't) manipulate the model with it.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"In v0.2, we tried to make the process for adding model more automatic, and enable the ability to build model with  different configurations. The struct for holding the configuration is now changed to a parametric struct depending  on a Symbol parameter specifying the model type (e.g. HGFConfig{:bert}). With this, the specific  HGF<XXModelType>config can be constructed on the fly. The HGFConfig has 2 field, one for storing the read-only  deserialized object loaded from the configuration file, and another for the overwritten values. This should turn the  config struct into a user level interface.","category":"page"},{"location":"changelog/#Model","page":"ChangeLogs","title":"Model","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"For the model part, the main change is that we do not make a 1-1 mapping between the python model/layer class and our  julia layer struct. When one wants to add a new model type, there are actually 2 things need to be done. One is  defining a model forward method that can do the same computation as the python model, and another is defining a  mapping between the python model and the julia model (so that the model parameters/weights can be transferred between  2 language). In the previous version, we chose to make a 1-1 mapping between the model, so that the parameters/weights  loading process can be fully automatic. However, for some reason, huggingface transformers is not reusing their  attention or transformer implementation for each model type. Which means for different model type, even if they are  actually doing the same computation (i.e. the computation graph is the same), the model layout can be different  (e.g. consider the differences between Chain(Chain(dense1, dense2), dense3) and Chain(dense1, dense2, dense3)).  As a result, these make implementing the model forward method a real pain, and also it's hard to apply optimizations.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"We noticed that the model forward method is more important and difficult than the model mapping. On the other hand,  though manually defining model mapping is tedious, it's less prone to go wrong. So instead of making a 1-1 mapping for  fully automatic model loading, we choose to reduce the work needed for forward method. In v0.2, the attention  implementation is switched to NeuralAttentionlib's modulated implementation and we build all internal layers with layer  from Transformers.Layers. As a result, layers like FakeTH<XXLayer> or HGF<XXModelType>Attention/MLP/... are  removed, only the outer-most types remain (e.g. HGFBertModel, HGFGPT2LMHeadModel...).","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Since we want to make it possible to finetune a pretrained model on new dataset/task easily, the model loading would  be a combination of initialization and parameters/weights loading. In normal Flux workflow, you would build a complete  new model and then inplace load the parameter/weight values into the specific layers/arrays in the model. In v0.2, we  combine the 2 step into one load_model function, which take the model type, configuration, and a state dictionary (  the term comes from PyTorch, which is a OrderedDict of variable names to weights). load_model would either  lookup variable from the state dictionary, or initialize with configuration, recursively. As a result,  load_model! is removed.","category":"page"},{"location":"changelog/#Behavior-Changes","page":"ChangeLogs","title":"Behavior Changes","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"All text encoder (including HuggingFace one) process function returned NamedTuple: Some field name changed,  tok => token, mask => attention_mask.\nMost layer/model from Transformers.jl would be taking and returning NamedTuple.\nFor HuggingFace model: All input is basically NamedTuple. The returned NamedTuple field name from the forward  method is also changed.","category":"page"},{"location":"layers/#Transformers.Layers","page":"Layers","title":"Transformers.Layers","text":"","category":"section"},{"location":"layers/","page":"Layers","title":"Layers","text":"Layer building blocks of Transformers.jl. Most of the layers are designed to work with NamedTuples. It would take a  NamedTuple as input, finding correct names as its arguments for computation, ignoring extra fields in the  NamedTuple, store the computation result in the input NamedTuple with correct names (conceptually, since  NamedTuple is immutable) and return it.","category":"page"},{"location":"layers/","page":"Layers","title":"Layers","text":"These layer types are mostly compatible with Flux.","category":"page"},{"location":"layers/#API-Reference","page":"Layers","title":"API Reference","text":"","category":"section"},{"location":"layers/","page":"Layers","title":"Layers","text":"Modules = [Transformers.Layers]\nOrder = [:type, :function]","category":"page"},{"location":"layers/#Transformers.Layers.ApplyEmbed","page":"Layers","title":"Transformers.Layers.ApplyEmbed","text":"ApplyEmbed([apply = .+,] embed)\n\nA layer that help to get embedding and apply on the input. Used with position embeddings.\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.CrossAttention-Tuple{Int64, Int64}","page":"Layers","title":"Transformers.Layers.CrossAttention","text":"CrossAttention(head::Int, hidden_size::Int [, head_hidden_size::Int = hidden_size ÷ head ];\n               dropout::Union{Nothing, Float64} = nothing, return_score = false)\n\nCreate a multi-head cross attention layer with head heads and head_hidden_size per head.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.CrossAttention-Tuple{NeuralAttentionlib.AbstractAttenOp, Int64, Int64, Int64}","page":"Layers","title":"Transformers.Layers.CrossAttention","text":"CrossAttention(atten_op::AbstractAttenOp, head::Int, hidden_size::Int, head_hidden_size::Int)\n\nCreate a cross attention layer with given atten_op.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.Embed","page":"Layers","title":"Transformers.Layers.Embed","text":"Embed(hidden_size::Int, vocab_size::Int; scale = nothing)\n\nAn Embedding layer that take an array of integer / one-hot encoding and return a multi-dimensional array as  embedded vectors and scale with scale.\n\nSee also: EmbedDecoder\n\nExample\n\njulia> embed = Embed(7, 10; scale = 100)\nEmbed(7, 10, scale = 100)\n\njulia> embed([1,3,5])\n7×3 Matrix{Float32}:\n  0.86955    1.14728    0.43275\n -0.378461  -0.112709   3.33885\n -1.61534   -2.55506    1.08488\n -0.833164   0.565268  -1.32531\n  0.820126  -5.11536   -0.75666\n -2.13458    1.25796   -1.47247\n  3.20416    0.872459   0.980557\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.EmbedDecoder","page":"Layers","title":"Transformers.Layers.EmbedDecoder","text":"EmbedDecoder(embed::Embed; bias = false)\n\nA layer that share weight with an embedding layer embed and return the logit.\n\nSee also: Embed\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.FixedLenPositionEmbed","page":"Layers","title":"Transformers.Layers.FixedLenPositionEmbed","text":"FixedLenPositionEmbed(hidden_size::Int, max_length::Int = 1024)\n\nAn trainable position embedding layer.\n\nSee also: SinCosPositionEmbed\n\nExample\n\njulia> pe = FixedLenPositionEmbed(7)\nFixedLenPositionEmbed(7, 1024)\n\njulia> pe(5)\n7×5 Matrix{Float32}:\n -0.0330963    -0.0412815    -0.0110067    0.0299395   -0.0303213\n  0.0203617    -0.000259752  -0.0300242    0.00573144   0.0147597\n  0.00662918   -0.0222377    -9.40627f-5  -0.038285    -0.0467688\n -0.00358604    0.0344152     0.0101526   -0.00750311   0.0173139\n  0.000689436   0.0116299    -0.00478128  -0.0331492    0.0148091\n  0.000711651  -0.0198647    -0.0037188    0.00427536  -0.0172123\n -0.00987371   -0.0385056    -0.00103168   0.0578125    0.00286929\n\njulia> pe([1,3])\n7×2 Matrix{Float32}:\n -0.0330963    -0.0110067\n  0.0203617    -0.0300242\n  0.00662918   -9.40627f-5\n -0.00358604    0.0101526\n  0.000689436  -0.00478128\n  0.000711651  -0.0037188\n -0.00987371   -0.00103168\n\njulia> pe(randn(3,3))\n7×3 Matrix{Float32}:\n -0.0330963    -0.0412815    -0.0110067\n  0.0203617    -0.000259752  -0.0300242\n  0.00662918   -0.0222377    -9.40627f-5\n -0.00358604    0.0344152     0.0101526\n  0.000689436   0.0116299    -0.00478128\n  0.000711651  -0.0198647    -0.0037188\n -0.00987371   -0.0385056    -0.00103168\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.Fork","page":"Layers","title":"Transformers.Layers.Fork","text":"Fork(layers...)\n\nA layer for applying each layers to the same input and return a Tuple. For example (Fork(dense1, dense2))(x) is  equivalent to (dense1(x), dense2(x)).\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.NSplit","page":"Layers","title":"Transformers.Layers.NSplit","text":"NSplit(n::Integer, layer)\n\nA layer for splitting the result of layer into n parts in the first dimension and return a Tuple. For  example (NSplit(2, dense))(x) is equivalent to  y = dense(x); s1 = size(y, 1); (y[begin:div(s1, 2)-1, :], y[div(s1, 2):end, :].\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.PostNormTransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PostNormTransformerBlock","text":"PostTransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                     attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a post-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PostNormTransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PostNormTransformerDecoderBlock","text":"PostTransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                            attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                            return_score = false, return_self_attention_score = false)\n\nCreate a post-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PreNormTransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PreNormTransformerBlock","text":"PreNormTransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                        attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a pre-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PreNormTransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PreNormTransformerDecoderBlock","text":"PreTransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                           attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                           return_score = false, return_self_attention_score = false)\n\nCreate a pre-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SelfAttention-Tuple{Int64, Int64}","page":"Layers","title":"Transformers.Layers.SelfAttention","text":"SelfAttention(head::Int, hidden_size::Int [, head_hidden_size::Int = hidden_size ÷ head ];\n              dropout::Union{Nothing, Float64} = nothing, return_score = false, causal = false)\n\nCreate a multi-head self attention layer with head heads and head_hidden_size per head.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SelfAttention-Tuple{NeuralAttentionlib.AbstractAttenOp, Int64, Int64, Int64}","page":"Layers","title":"Transformers.Layers.SelfAttention","text":"SelfAttention(atten_op::AbstractAttenOp, head::Int, hidden_size::Int, head_hidden_size::Int)\n\nCreate a self attention layer with given atten_op.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SinCosPositionEmbed","page":"Layers","title":"Transformers.Layers.SinCosPositionEmbed","text":"SinCosPositionEmbed(hidden_size::Int)\n\nThe absolute sin cos postion embedding.\n\nSee also: FixedLenPositionEmbed\n\nExample\n\njulia> pe = SinCosPositionEmbed(7)\nSinCosPositionEmbed(default_position_func(static(7)), 7, normalized = false)\n\njulia> pe(5)\n7×5 Matrix{Float32}:\n 0.0  0.841471      0.909297      0.14112     -0.756802\n 1.0  0.540302     -0.416147     -0.989992    -0.653644\n 0.0  0.0719065     0.143441      0.214232     0.283915\n 1.0  0.997411      0.989659      0.976783     0.95885\n 0.0  0.00517945    0.0103588     0.0155378    0.0207164\n 1.0  0.999987      0.999946      0.999879     0.999785\n 0.0  0.000372759   0.000745519   0.00111828   0.00149104\n\njulia> pe([1,3])\n7×2 Matrix{Float32}:\n 0.0   0.909297\n 1.0  -0.416147\n 0.0   0.143441\n 1.0   0.989659\n 0.0   0.0103588\n 1.0   0.999946\n 0.0   0.000745519\n\njulia> pe(randn(3,3))\n7×3 Matrix{Float64}:\n 0.0  0.841471      0.909297\n 1.0  0.540302     -0.416147\n 0.0  0.0719065     0.143441\n 1.0  0.997411      0.989659\n 0.0  0.00517945    0.0103588\n 1.0  0.999987      0.999946\n 0.0  0.000372759   0.000745519\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.Transformer-Tuple{Type{<:Transformers.Layers.AbstractTransformerBlock}, Int64, Vararg{Any}}","page":"Layers","title":"Transformers.Layers.Transformer","text":"Transformer(T::Type{<:AbstractTransformerBlock}, n::Int, args...; kwargs...)\n\nCreate n layers of transformer blocks with T(args...; kwargs...).\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.TransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.TransformerBlock","text":"TransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                 attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a post-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.TransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.TransformerDecoderBlock","text":"TransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                        attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                        return_score = false, return_self_attention_score = false)\n\nCreate a post-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.no_dropout","page":"Layers","title":"Transformers.Layers.no_dropout","text":"no_dropout(model)\n\nCreating a new model sharing all parameters with model but disable all dropout.\n\n\n\n\n\n","category":"function"},{"location":"layers/#Transformers.Layers.set_dropout","page":"Layers","title":"Transformers.Layers.set_dropout","text":"set_dropout(model, p)\n\nCreating a new model sharing all parameters with model but set all dropout probability to p.\n\n\n\n\n\n","category":"function"},{"location":"layers/#Transformers.Layers.testmode-Tuple{Any}","page":"Layers","title":"Transformers.Layers.testmode","text":"testmode(model)\n\nCreating a new model sharing all parameters with model but used for testing. Currently this is just  no_dropout.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace","page":"HuggingFace","title":"Transformers.HuggingFace","text":"","category":"section"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"Module for loading pre-trained model from HuggingFace.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"info: Info\nWe provide a set of API to download & load a pretrain model from huggingface hub. This is mostly manually done, so we  only have a small set of available models. The most practical way to check if a model is available in Transformers  is to run the HuggingFaceValidation code in the example folder,  which use PyCall.jl to load the model in both Python and Julia. Open issues/PRs if you find a model you want is  not supported here.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"There are basically 3 main api for loading the model, HuggingFace.load_config, HuggingFace.load_tokenizer, HuggingFace.load_model. These are the underlying function of the HuggingFace.@hgf_str macro. You can get a better control of the loading process.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"We can load a specific config of a specific model, no matter it's actually supported by Transformers.jl.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"julia> load_config(\"google/pegasus-xsum\")\nTransformers.HuggingFace.HGFConfig{:pegasus, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}} with 45 entries:\n  :use_cache                       => true\n  :d_model                         => 1024\n  :scale_embedding                 => true\n  :add_bias_logits                 => false\n  :static_position_embeddings      => true\n  :encoder_attention_heads         => 16\n  :num_hidden_layers               => 16\n  :encoder_layerdrop               => 0\n  :num_beams                       => 8\n  :max_position_embeddings         => 512\n  :model_type                      => \"pegasus\"\n  ⋮                                => ⋮\n","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"This would give you all value available in the downloaded configuration file. This might be enough for a some model,  but there are other model that use the default value hard coded in their python code.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"Sometime you would want to add/overwrite  some of the value. This can be done be calling HGFConfig(old_config; key_to_update = new_value, ...). These is used  primary for customizing model loading. For example, you can load a bert-base-cased model for sequence classification  task. However, if you directly load the model:","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"julia> bert_model = hgf\"bert-base-cased:ForSequenceClassification\";\n\njulia> bert_model.cls.layer\nDense(W = (768, 2), b = true)","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"The model is default creating model for 2 class of label. So you would need to load the config and update the field  about number of labels and create the model with the new config:","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"julia> bertcfg = load_config(\"bert-base-cased\");\n\njulia> bertcfg.num_labels\n2\n\njulia> mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);\n\njulia> mycfg.num_labels\n3\n\njulia> _bert_model = load_model(\"bert-base-cased\", :ForSequenceClassification; config = mycfg);\n\njulia> _bert_model.cls.layer\nDense(W = (768, 3), b = true)\n","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"All config field name follow the same name as huggingface, so you might need to read their document for what  is available. However, not every configuration work in Transformers.jl. It's better to check the source  src/huggingface/implementation. All supported models would need to overload the load_model and provided an implementation in Julia to be  workable.","category":"page"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"For the tokenizer, load_tokenizer is basically the same as calling with @hgf_str. Currently providing customized  config doesn't change much stuff. The tokenizer might also work for unsupported model because some serialize the whole  tokenizer object, but not every model does that or they use something not covered by our implementation.","category":"page"},{"location":"huggingface/#API-Reference","page":"HuggingFace","title":"API Reference","text":"","category":"section"},{"location":"huggingface/","page":"HuggingFace","title":"HuggingFace","text":"Modules = [Transformers.HuggingFace]\nOrder = [:macro, :type, :function]","category":"page"},{"location":"huggingface/#Transformers.HuggingFace.@hgf_str-Tuple{Any}","page":"HuggingFace","title":"Transformers.HuggingFace.@hgf_str","text":"`hgf\"<model-name>:<item>\"`\n\nGet item from model-name. This will ensure the required data are downloaded. item can be \"config\",  \"tokenizer\", and model related like \"Model\", or \"ForMaskedLM\", etc. Use get_model_type to see what  model/task are supported. If item is omitted, return a Tuple of <model-name>:tokenizer and <model-name>:model.\n\n\n\n\n\n","category":"macro"},{"location":"huggingface/#Transformers.HuggingFace.HGFConfig","page":"HuggingFace","title":"Transformers.HuggingFace.HGFConfig","text":"HGFConfig{model_type}\n\nThe type for holding the configuration for huggingface model model_type.\n\nHGFConfig(base_cfg::HGFConfig; kwargs...)\n\nReturn a new HGFConfig object for the same model_type with fields updated with kwargs.\n\nExample\n\njulia> bertcfg = load_config(\"bert-base-cased\");\n\njulia> bertcfg.num_labels\n2\n\njulia> mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);\n\njulia> mycfg.num_labels\n3\n\n\n\n\n\n","category":"type"},{"location":"huggingface/#Transformers.HuggingFace.get_model_type","page":"HuggingFace","title":"Transformers.HuggingFace.get_model_type","text":"get_model_type(model_type)\n\nSee the list of supported model type of given model. For example, use get_mdoel_type(:gpt2) to see all model/task  that gpt2 support. The keys of the returned NamedTuple are all possible task which can be used in  load_model or @hgf_str.\n\nExample\n\njulia> HuggingFace.get_model_type(:gpt2)\n(model = Transformers.HuggingFace.HGFGPT2Model, lmheadmodel = Transformers.HuggingFace.HGFGPT2LMHeadModel)\n\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.get_state_dict","page":"HuggingFace","title":"Transformers.HuggingFace.get_state_dict","text":"get_state_dict(model)\n\nGet the state_dict of the model.\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.load_config-Tuple{Any}","page":"HuggingFace","title":"Transformers.HuggingFace.load_config","text":"load_config(model_name; local_files_only = false, cache = true)\n\nLoad the configuration file of model_name from huggingface hub. By default, this function would check if model_name  exists on huggingface hub, download the configuration file (and cache it if cache is set), and then load and return  the config::HGFConfig. If local_files_only = false, it would check whether the configuration file is up-to-date  and update if not (and thus require network access every time it is called). By setting local_files_only = true, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  HuggingFaceApi.jl package. This function would require the configuration file has a field about the model_type, if  not, use load_config(model_type, HuggingFace.load_config_dict(model_name; local_files_only, cache)) with model_type  manually provided.\n\nSee also: HGFConfig\n\nExample\n\njulia> load_config(\"bert-base-cased\")\nTransformers.HuggingFace.HGFConfig{:bert, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Nothing} with 19 entries:\n  :architectures                => [\"BertForMaskedLM\"]\n  :attention_probs_dropout_prob => 0.1\n  :gradient_checkpointing       => false\n  :hidden_act                   => \"gelu\"\n  :hidden_dropout_prob          => 0.1\n  :hidden_size                  => 768\n  :initializer_range            => 0.02\n  :intermediate_size            => 3072\n  :layer_norm_eps               => 1.0e-12\n  :max_position_embeddings      => 512\n  :model_type                   => \"bert\"\n  :num_attention_heads          => 12\n  :num_hidden_layers            => 12\n  :pad_token_id                 => 0\n  :position_embedding_type      => \"absolute\"\n  :transformers_version         => \"4.6.0.dev0\"\n  :type_vocab_size              => 2\n  :use_cache                    => true\n  :vocab_size                   => 28996\n\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_config-Tuple{Union{Symbol, Val}, Any}","page":"HuggingFace","title":"Transformers.HuggingFace.load_config","text":"load_config(model_type, cfg)\n\nLoad cfg as model_type. This is used for manually load a config when model_type is not specified in the config.  model_type is a Symbol of the model type like :bert, :gpt2, :t5, etc.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_hgf_pretrained-Tuple{Any}","page":"HuggingFace","title":"Transformers.HuggingFace.load_hgf_pretrained","text":"load_hgf_pretrained(name)\n\nThe underlying function of @hgf_str.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_model","page":"HuggingFace","title":"Transformers.HuggingFace.load_model","text":"load_model([model_type::Symbol,] model_name, task = :model [, state_dict];\n           trainmode = false, config = nothing, local_files_only = false, cache = true)\n\nLoad the model of model_name for task. This function would load the state_dict of model_name and build a new  model according to config, task, and the state_dict. local_files_only and cache kwargs would be pass directly  to both load_state_dict and load_config if not provided. This function would require the  configuration file has a field about the model_type, if not, use load_model(model_type, model_name, task; kwargs...)  with model_type manually provided. trainmode = false would disable all dropouts. The state_dict can be directly  provided, this is used when you want to create a new model with the state_dict in hand. Use get_model_type  to see what task is available.\n\nSee also: get_model_type, load_state_dict, load_config, HGFConfig\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.load_model-Tuple{Type, Any}","page":"HuggingFace","title":"Transformers.HuggingFace.load_model","text":"load_model(::Type{T}, config, state_dict = OrderedDict())\n\nCreate a new model of T according to config and state_dict. missing parameter would initialized according  to config. Set the JULIA_DEBUG=Transformers environment variable to see what parameters are missing.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_state_dict-Tuple{Any}","page":"HuggingFace","title":"Transformers.HuggingFace.load_state_dict","text":"load_state_dict(model_name; local_files_only = false, cache = true)\n\nLoad the state_dict from the given model_name from huggingface hub. By default, this function would check if  model_name exists on huggingface hub, download the model file (and cache it if cache is set), and then load  and return the state_dict. If local_files_only = false, it would check whether the model file is up-to-date and  update if not (and thus require network access every time it is called). By setting local_files_only = true, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  HuggingFaceApi.jl package.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_tokenizer","page":"HuggingFace","title":"Transformers.HuggingFace.load_tokenizer","text":"load_tokenizer(model_name; config = nothing, local_files_only = false, cache = true)\n\nLoad the text encoder of model_name from huggingface hub. By default, this function would check if model_name  exists on huggingface hub, download all required files for this text encoder (and cache these files if cache is  set), and then load and return the text encoder. If local_files_only = false, it would check whether all cached  files are up-to-date and update if not (and thus require network access every time it is called). By setting  local_files_only = true, it would try to find the files from the cache directly and error out if not found.  For managing the caches, see the HuggingFaceApi.jl package.\n\nExample\n\njulia> load_tokenizer(\"t5-small\")\nT5TextEncoder(\n├─ TextTokenizer(MatchTokenization(PrecompiledNormalizer(WordReplaceNormalizer(UnigramTokenization(EachSplitTokenization(splitter = isspace), unigram = Unigram(vocab_size = 32100, unk = <unk>)), pattern = r\"^(?!▁)(.*)$\" => s\"▁\u0001\"), precompiled = PrecompiledNorm(...)), 103 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 32100, unk = <unk>, unki = 3),\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[(token, segment)] := SequenceTemplate{String}(Input[1]:<type=1> </s>:<type=1> (Input[2]:<type=1> </s>:<type=1>)...)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.save_config-Tuple{Any, Any}","page":"HuggingFace","title":"Transformers.HuggingFace.save_config","text":"save_config(model_name, config; path = pwd(), config_name = CONFIG_NAME, force = false)\n\nSave the config at <path>/<model_name>/<config_name>. This would error out if the file already exists but force  not set.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.save_model-Tuple{Any, Any}","page":"HuggingFace","title":"Transformers.HuggingFace.save_model","text":"save_model(model_name, model; path = pwd(), weight_name = PYTORCH_WEIGHTS_NAME, force = false)\n\nsave the model statedict at `<path>/<modelname>/<weight_name>. This would error out if the file already exists  butforce` not set.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.state_dict_to_namedtuple-Tuple{Any}","page":"HuggingFace","title":"Transformers.HuggingFace.state_dict_to_namedtuple","text":"state_dict_to_namedtuple(state_dict)\n\nconvert state_dict into nested NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"#Transformers.jl","page":"Home","title":"Transformers.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia implementation of Transformers models","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of Transformers: The Julia solution  for using Transformer models.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add Transformers","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"getstarted.md\",\n  \"tutorial.md\",\n  \"layers.md\",\n  \"textencoders.md\",\n  \"huggingface.md\",\n]\nDepth = 3","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following content will cover the basic introductions about the Transformer model and the implementation.","category":"page"},{"location":"tutorial/#Transformer-model","page":"Tutorial","title":"Transformer model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model was proposed in the paper: Attention Is All You Need. In that paper they provide a new way of handling the sequence transduction problem (like the machine translation task) without complex recurrent or convolutional structure. Simply use a stack of attention mechanisms to get the latent structure in the input sentences and a special embedding (positional embedding) to get the locationality. The whole model architecture looks like this:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model architecture (picture from the origin paper)(Image: transformer)","category":"page"},{"location":"tutorial/#Multi-Head-Attention","page":"Tutorial","title":"Multi-Head Attention","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead of using the regular attention mechanism, they split the input vector to several pairs of subvector and perform a dot-product attention on each subvector pairs.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Regular attention v.s. Multi-Head attention (picture from the origin paper)(Image: mhatten)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For those who like mathematical expression, here is the formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Attention(Q K V) = softmax(fracQK^Tsqrtd_k)V","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"MultiHead(Q K V) = Concat(head_1 head_h)W^O\ntextwhere head_i = Attention(QW^Q_i KW^K_i VW^V_i)","category":"page"},{"location":"tutorial/#Positional-Embedding","page":"Tutorial","title":"Positional Embedding","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As we mentioned above, transformer model didn't depend on the recurrent or convolutional structure. On the other hand, we still need a way to differentiate two sequence with same words but different order. Therefore, they add the locational information on the embedding, i.e. the origin word embedding plus a special embedding that indicate the order of that word. The special embedding can be computed by some equations or just use another trainable embedding matrix. In the paper, the positional embedding use this formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"PE_(pos k) = begincases\nsin(fracpos10^4kd_k) textif k text is even\ncos(fracpos10^4kd_k)  textif k text is odd\nendcases","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where pos is the locational information that tells you the given word is the pos-th word, and k is the k-th dimension of the input vector. d_k is the total length of the word/positional embedding. So the new embedding will be computed as:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Embedding_k(word) = WordEmbedding_k(word) + PE(pos_of_word k)","category":"page"},{"location":"tutorial/#Transformers.jl","page":"Tutorial","title":"Transformers.jl","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we know how the transformer model looks like, let's take a look at the Transformers.jl.","category":"page"},{"location":"tutorial/#Example","page":"Tutorial","title":"Example","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"info: Info\nThis tutorial is just for demonstrating how the Transformer model looks like, not for using in real task.  The example code can be found in the  example folder.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To best illustrate the usage of Transformers.jl, we will start with building a two layer Transformer model on a sequence copy task. Before we start, we need to install all the package we need:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Pkg\nPkg.add(\"CUDA\")\nPkg.add(\"Flux\")\nPkg.add(\"Transformers\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use CUDA.jl for the GPU support.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux\nusing CUDA\nusing Transformers\nusing Transformers.Layers\nusing Transformers.TextEncoders\n\nenable_gpu(CUDA.functional()) # make `todevice` work on gpu if available","category":"page"},{"location":"tutorial/#Copy-task","page":"Tutorial","title":"Copy task","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The copy task is a toy test case of a sequence transduction problem that simply return the same sequence as the output. Here we define the input as a random sequence of white space separable number from 1~10 and length 10. we will also need a start and end symbol to indicate where is the begin and end of the sequence. We can use Transformers.TextEncoders.TransformerTextEncoder to preprocess the input (add start/end symbol, convert to one-hot encoding, ...).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"labels = map(string, 1:10)\nstartsym = \"<s>\"\nendsym = \"</s>\"\nunksym = \"<unk>\"\nlabels = [unksym, startsym, endsym, labels...]\n\ntextenc = TransformerTextEncoder(split, labels; startsym, endsym, unksym, padsym = unksym)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# function for generate training datas\nsample_data() = (d = join(map(string, rand(1:10, 10)), ' '); (d,d))\n\n@show sample = sample_data()\n# encode single sentence\n@show encoded_sample_1 = encode(textenc, sample[1])\n# encode for both encoder and decoder input\n@show encoded_sample = encode(textenc, sample[1], sample[2])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"sample = sample_data() = (\"5 1 10 10 7 3 3 4 9 6\", \"5 1 10 10 7 3 3 4 9 6\")\nencoded_sample_1 = encode(textenc, sample[1]) = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12]))\nencoded_sample = encode(textenc, sample[1], sample[2]) = (encoder_input = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12])), decoder_input = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12]), cross_attention_mask = NeuralAttentionlib.BiLengthMask{1, Vector{Int32}}(Int32[12], Int32[12])))","category":"page"},{"location":"tutorial/#Defining-the-model","page":"Tutorial","title":"Defining the model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With the Transformers.jl and Flux.jl, we can define the model easily. We use a Transformer with 512 hidden size and 8 head.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# model setting\nN = 2\nhidden_dim = 512\nhead_num = 8\nhead_dim = 64\nffn_dim = 2048\n\n# define a Word embedding layer which turn word index to word vector\nword_embed = Embed(hidden_dim, length(textenc.vocab)) |> todevice\n\n# define a position embedding layer metioned above\n# since sin/cos position embedding does not have any parameter, `todevice` is not needed.\npos_embed = SinCosPositionEmbed(hidden_dim)\n\n# define 2 layer of transformer\nencoder_trf = Transformer(TransformerBlock, N, head_num, hidden_dim, head_dim, ffn_dim) |> todevice\n\n# define 2 layer of transformer decoder\ndecoder_trf = Transformer(TransformerDecoderBlock, N, head_num, hidden_dim, head_dim, ffn_dim) |> todevice\n\n# define the layer to get the final output probabilities\n# sharing weights with `word_embed`, don't/can't use `todevice`.\nembed_decode = EmbedDecoder(word_embed)\n\nfunction embedding(input)\n    we = word_embed(input.token)\n    pe = pos_embed(we)\n    return we .+ pe\nend\n\nfunction encoder_forward(input)\n    attention_mask = get(input, :attention_mask, nothing)\n    e = embedding(input)\n    t = encoder_trf(e, attention_mask) # return a NamedTuples (hidden_state = ..., ...)\n    return t.hidden_state\nend\n\nfunction decoder_forward(input, m)\n    attention_mask = get(input, :attention_mask, nothing)\n    cross_attention_mask = get(input, :cross_attention_mask, nothing)\n    e = embedding(input)\n    t = decoder_trf(e, m, attention_mask, cross_attention_mask) # return a NamedTuple (hidden_state = ..., ...)\n    p = embed_decode(t.hidden_state)\n    return p\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Then run the model on the sample","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"enc = encoder_forward(todevice(encoded_sample.encoder_input))\nlogits = decoder_forward(todevice(encoded_sample.decoder_input), enc)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The whole model can be defined without those forward functions. See the example folder and docs of the Layer API for more information.","category":"page"},{"location":"tutorial/#define-the-loss-and-training-loop","page":"Tutorial","title":"define the loss and training loop","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For the last step, we need to define the loss function and training loop. We use the kl divergence for the output probability.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux.Losses # for logitcrossentropy\n\n# define loss function\nfunction shift_decode_loss(logits, trg, trg_mask)\n    label = trg[:, 2:end, :]\n    return logitcrossentropy(@view(logits[:, 1:end-1, :]), label, trg_mask - 1)\nend\n\nfunction loss(input)\n    enc = encoder_forward(input.encoder_input)\n    logits = decoder_forward(input.decoder_input, enc)\n    ce_loss = shift_decode_loss(logits, input.decoder_input.token, input.decoder_input.attention_mask)\n    return ce_loss\nend\n\n# collect all the parameters\nps = Flux.params(word_embed, encoder_trf, decoder_trf)\nopt = ADAM(1e-4)\n\n# function for created batched data\nusing Transformers.Datasets: batched\n\n# flux function for update parameters\nusing Flux: gradient\nusing Flux.Optimise: update!\n\npreprocess(sample) = todevice(encode(textenc, sample[1], sample[2]))\n\n# define training loop\nfunction train!()\n    @info \"start training\"\n    for i = 1:2000\n        sample = batched([sample_data() for i = 1:32]) # create 32 random sample and batched\n        input = preprocess(sample)\n        grad = gradient(()->loss(input), ps)\n        if i % 8 == 0\n            l = loss(input)\n            println(\"loss = $l\")\n        end\n        update!(opt, ps, grad)\n    end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"train!()","category":"page"},{"location":"tutorial/#Test-our-model","page":"Tutorial","title":"Test our model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After training, we can try to test the model.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function translate(x::AbstractString)\n    ix = todevice(encode(textenc, x).token)\n    seq = [startsym]\n\n    encoder_input = (token = ix,)\n    enc = encoder_forward(encoder_input)\n\n    len = size(ix, 2)\n    for i = 1:2len\n        decoder_input = (token = todevice(lookup(textenc, seq)),)\n        logit = decoder_forward(decoder_input, enc)\n        ntok = decode(textenc, argmax(logit[:, end]))\n        push!(seq, ntok)\n        ntok == endsym && break\n    end\n    return seq\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"translate(\"5 5 6 6 1 2 3 4 7 10\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"10-element Vector{String}:\n \"5\"\n \"5\"\n \"6\"\n \"6\"\n \"1\"\n \"2\"\n \"3\"\n \"4\"\n \"7\"\n \"10\"\n","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The result looks good!","category":"page"},{"location":"datasets/#Transformers.Datasets-(not-complete)","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"Functions for loading some common Datasets","category":"page"},{"location":"datasets/#Provide-datasets","page":"Transformers.Datasets (not complete)","title":"Provide datasets","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"WMT\nWMT14 (by Google Brain)\nIWSLT\nIWSLT 2016\nen <=> de\nen <=> cs\nen <=> fr\nen <=> ar\nGLUE\nCoLA\nDiagnostic\nGLUE\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSNLI\nSST\nSTS\nWNLI","category":"page"},{"location":"datasets/#example","page":"Transformers.Datasets (not complete)","title":"example","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"using Transformers.Datasets\nusing Transformers.Datasets.GLUE\n\ntask = GLUE.QNLI()\ndatas = dataset(Train, task)\nget_batch(datas, 4)","category":"page"},{"location":"datasets/#API-reference","page":"Transformers.Datasets (not complete)","title":"API reference","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"Modules=[Transformers.Datasets]\nOrder = [:type, :function, :macro]","category":"page"}]
}
