var documenterSearchIndex = {"docs":
[{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following content will cover the basic introductions about the Transformer model and the implementation.","category":"page"},{"location":"tutorial/#Transformer-model","page":"Tutorial","title":"Transformer model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model was proposed in the paper: Attention Is All You Need. In that paper they provide a new way of handling the sequence transduction problem (like the machine translation task) without complex recurrent or convolutional structure. Simply use a stack of attention mechanisms to get the latent structure in the input sentences and a special embedding (positional embedding) to get the locationality. The whole model architecture looks like this:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model architecture (picture from the origin paper)(Image: transformer)","category":"page"},{"location":"tutorial/#Multi-Head-Attention","page":"Tutorial","title":"Multi-Head Attention","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead of using the regular attention mechanism, they split the input vector to several pairs of subvector and perform a dot-product attention on each subvector pairs.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Regular attention v.s. Multi-Head attention (picture from the origin paper)(Image: mhatten)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For those who like mathematical expression, here is the formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Attention(Q K V) = softmax(fracQK^Tsqrtd_k)V","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"MultiHead(Q K V) = Concat(head_1 head_h)W^O\ntextwhere head_i = Attention(QW^Q_i KW^K_i VW^V_i)","category":"page"},{"location":"tutorial/#Positional-Embedding","page":"Tutorial","title":"Positional Embedding","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As we mentioned above, transformer model didn't depend on the recurrent or convolutional structure. On the other hand, we still need a way to differentiate two sequence with same words but different order. Therefore, they add the locational information on the embedding, i.e. the origin word embedding plus a special embedding that indicate the order of that word. The special embedding can be computed by some equations or just use another trainable embedding matrix. In the paper, the positional embedding use this formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"PE_(pos k) = begincases\nsin(fracpos10^4kd_k) textif k text is even\ncos(fracpos10^4kd_k)  textif k text is odd\nendcases","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where pos is the locational information that tells you the given word is the pos-th word, and k is the k-th dimension of the input vector. d_k is the total length of the word/positional embedding. So the new embedding will be computed as:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Embedding_k(word) = WordEmbedding_k(word) + PE(pos_of_word k)","category":"page"},{"location":"tutorial/#Transformers.jl","page":"Tutorial","title":"Transformers.jl","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we know how the transformer model looks like, let's take a look at the Transformers.jl.","category":"page"},{"location":"tutorial/#Example","page":"Tutorial","title":"Example","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"info: Info\nThis tutorial is just for demonstrating how the Transformer model looks like, not for using in real task.  The example code can be found in the  example folder.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To best illustrate the usage of Transformers.jl, we will start with building a two layer Transformer model on a sequence copy task. Before we start, we need to install all the package we need:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Pkg\nPkg.add(\"CUDA\")\nPkg.add(\"Flux\")\nPkg.add(\"Transformers\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use CUDA.jl for the GPU support.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux\nusing CUDA\nusing Transformers\nusing Transformers.Layers\nusing Transformers.TextEncoders\n\nenable_gpu(CUDA.functional()) # make `todevice` work on gpu if available","category":"page"},{"location":"tutorial/#Copy-task","page":"Tutorial","title":"Copy task","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The copy task is a toy test case of a sequence transduction problem that simply return the same sequence as the output. Here we define the input as a random sequence of white space separable number from 1~10 and length 10. we will also need a start and end symbol to indicate where is the begin and end of the sequence. We can use Transformers.TextEncoders.TransformerTextEncoder to preprocess the input (add start/end symbol, convert to one-hot encoding, ...).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"labels = map(string, 1:10)\nstartsym = \"<s>\"\nendsym = \"</s>\"\nunksym = \"<unk>\"\nlabels = [unksym, startsym, endsym, labels...]\n\ntextenc = TransformerTextEncoder(split, labels; startsym, endsym, unksym, padsym = unksym)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# function for generate training datas\nsample_data() = (d = join(map(string, rand(1:10, 10)), ' '); (d,d))\n\n@show sample = sample_data()\n# encode single sentence\n@show encoded_sample_1 = encode(textenc, sample[1])\n# encode for both encoder and decoder input\n@show encoded_sample = encode(textenc, sample[1], sample[2])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"sample = sample_data() = (\"5 1 10 10 7 3 3 4 9 6\", \"5 1 10 10 7 3 3 4 9 6\")\nencoded_sample_1 = encode(textenc, sample[1]) = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12]))\nencoded_sample = encode(textenc, sample[1], sample[2]) = (encoder_input = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12])), decoder_input = (token = Bool[0 0 0 0 0 0 0 0 0 0 0 0; 1 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 1; 0 0 1 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 1 1 0 0 0 0; 0 0 0 0 0 0 0 0 1 0 0 0; 0 1 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 1 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 0 0; 0 0 0 1 1 0 0 0 0 0 0 0], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[12]), cross_attention_mask = NeuralAttentionlib.BiLengthMask{1, Vector{Int32}}(Int32[12], Int32[12])))","category":"page"},{"location":"tutorial/#Defining-the-model","page":"Tutorial","title":"Defining the model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With the Transformers.jl and Flux.jl, we can define the model easily. We use a Transformer with 512 hidden size and 8 head.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# model setting\nN = 2\nhidden_dim = 512\nhead_num = 8\nhead_dim = 64\nffn_dim = 2048\n\n# define a Word embedding layer which turn word index to word vector\nword_embed = Embed(hidden_dim, length(textenc.vocab)) |> todevice\n\n# define a position embedding layer metioned above\n# since sin/cos position embedding does not have any parameter, `todevice` is not needed.\npos_embed = SinCosPositionEmbed(hidden_dim)\n\n# define 2 layer of transformer\nencoder_trf = Transformer(TransformerBlock, N, head_num, hidden_dim, head_dim, ffn_dim) |> todevice\n\n# define 2 layer of transformer decoder\ndecoder_trf = Transformer(TransformerDecoderBlock, N, head_num, hidden_dim, head_dim, ffn_dim) |> todevice\n\n# define the layer to get the final output probabilities\n# sharing weights with `word_embed`, don't/can't use `todevice`.\nembed_decode = EmbedDecoder(word_embed)\n\nfunction embedding(input)\n    we = word_embed(input.token)\n    pe = pos_embed(we)\n    return we .+ pe\nend\n\nfunction encoder_forward(input)\n    attention_mask = get(input, :attention_mask, nothing)\n    e = embedding(input)\n    t = encoder_trf(e, attention_mask) # return a NamedTuples (hidden_state = ..., ...)\n    return t.hidden_state\nend\n\nfunction decoder_forward(input, m)\n    attention_mask = get(input, :attention_mask, nothing)\n    cross_attention_mask = get(input, :cross_attention_mask, nothing)\n    e = embedding(input)\n    t = decoder_trf(e, m, attention_mask, cross_attention_mask) # return a NamedTuple (hidden_state = ..., ...)\n    p = embed_decode(t.hidden_state)\n    return p\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Then run the model on the sample","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"enc = encoder_forward(todevice(encoded_sample.encoder_input))\nlogits = decoder_forward(todevice(encoded_sample.decoder_input), enc)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The whole model can be defined without those forward functions. See the example folder and docs of the Layer API for more information.","category":"page"},{"location":"tutorial/#define-the-loss-and-training-loop","page":"Tutorial","title":"define the loss and training loop","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For the last step, we need to define the loss function and training loop. We use the kl divergence for the output probability.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux.Losses # for logitcrossentropy\n\n# define loss function\nfunction shift_decode_loss(logits, trg, trg_mask)\n    label = trg[:, 2:end, :]\n    return logitcrossentropy(@view(logits[:, 1:end-1, :]), label, trg_mask - 1)\nend\n\nfunction loss(input)\n    enc = encoder_forward(input.encoder_input)\n    logits = decoder_forward(input.decoder_input, enc)\n    ce_loss = shift_decode_loss(logits, input.decoder_input.token, input.decoder_input.attention_mask)\n    return ce_loss\nend\n\n# collect all the parameters\nps = Flux.params(word_embed, encoder_trf, decoder_trf)\nopt = ADAM(1e-4)\n\n# function for created batched data\nusing Transformers.Datasets: batched\n\n# flux function for update parameters\nusing Flux: gradient\nusing Flux.Optimise: update!\n\npreprocess(sample) = todevice(encode(textenc, sample[1], sample[2]))\n\n# define training loop\nfunction train!()\n    @info \"start training\"\n    for i = 1:2000\n        sample = batched([sample_data() for i = 1:32]) # create 32 random sample and batched\n        input = preprocess(sample)\n        grad = gradient(()->loss(input), ps)\n        if i % 8 == 0\n            l = loss(input)\n            println(\"loss = $l\")\n        end\n        update!(opt, ps, grad)\n    end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"train!()","category":"page"},{"location":"tutorial/#Test-our-model","page":"Tutorial","title":"Test our model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After training, we can try to test the model.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function translate(x::AbstractString)\n    ix = todevice(encode(textenc, x).token)\n    seq = [startsym]\n\n    encoder_input = (token = ix,)\n    enc = encoder_forward(encoder_input)\n\n    len = size(ix, 2)\n    for i = 1:2len\n        decoder_input = (token = todevice(lookup(textenc, seq)),)\n        logit = decoder_forward(decoder_input, enc)\n        ntok = decode(textenc, argmax(logit[:, end]))\n        push!(seq, ntok)\n        ntok == endsym && break\n    end\n    return seq\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"translate(\"5 5 6 6 1 2 3 4 7 10\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"10-element Vector{String}:\n \"5\"\n \"5\"\n \"6\"\n \"6\"\n \"1\"\n \"2\"\n \"3\"\n \"4\"\n \"7\"\n \"10\"\n","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The result looks good!","category":"page"},{"location":"huggingface_dev/#Adding-New-HuggingFace-Model","page":"Add New Models","title":"Adding New HuggingFace Model","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"This is a record of what I do to port the bloom model. This log serves as an tutorial for adding new hugginface model to Transformers.jl.","category":"page"},{"location":"huggingface_dev/#0.-Find-the-correct-model-type","page":"Add New Models","title":"0. Find the correct model type","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Our target is to port the bigscience/bloom model. We first find the model on the huggingface hub (https://huggingface.co/bigscience/bloom/tree/main). At the \"Files and versions\", we can find the config.json and in the model_type field we can see the model type is \"bloom\". We can find another model of same model type but with smaller model size, such as bigscience/bloom-560m, for testing.","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Once we get the model type, we can locate the corresponding python code in huggingface transformers github under the \"src/transformers/models\" folder (https://github.com/huggingface/transformers/tree/main/src/transformers/models). The code would usually be put in a folder using model type as the name, such as \"src/transformers/models/bloom\". On the other hand, our code will be put inside \"src/huggingface/implementation\" and also use model type as the folder name (https://github.com/chengchingwen/Transformers.jl/tree/master/src/huggingface/implementation/bloom).","category":"page"},{"location":"huggingface_dev/#1.-Porting-config","page":"Add New Models","title":"1. Porting config","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The first thing we need to port is the config object. Although we are actually able to load the config without defining the config object in Julia, there are some default values hardcoded in the python code and cannot be found in the config.json. Thus we copy the hardcoded default values and create our own config object in julia.","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Their config object is defined in configuration_<model_type>.py (https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/configuration_bloom.py) as class <model_type>Config(PretrainedConfig) (BloomConfig). The default values is either defined as the __init__ default arguments or assigned in the function body. We copy the default values to our julia definition. Generally, there are 3 part we need to check in the python class:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"variable model_type\nvariable attribute_map: A dictionary that map alias property name to the real property name.\nmethod __init__","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"In \"src/huggingface/implementation/bloom/\", we open a file \"config.jl\" and define the config object:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"@hgfcfg :bloom struct HGFBloomConfig\n    vocab_size::Int = 250880\n    hidden_size::Int = 64\n    [n_layer, num_hidden_layers]::Int = 2\n    [n_head, num_attention_heads]::Int = 8\n    layer_norm_epsilon::Float64 = 1e-5\n    initializer_range::Float64 = 0.02\n    use_cache::Bool = true\n    bos_token_id::Int = 1\n    eos_token_id::Int = 2\n    apply_residual_connection_post_layernorm::Bool = false\n    hidden_dropout::Float64 = 0.0\n    attention_dropout::Float64 = 0.0\n    pretraining_tp::Int = 1\n    slow_but_exact::Bool = false\n    clean_up_tokenization_spaces::Bool = false\nend","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The @hgfcfg take two argument: 1. a symbol for the model type (:bloom). 2. a Base.@kwdef-like struct definition, despite the name aliases (e.g. [n_layer, num_hidden_layers] where n_layer is the real field name and num_hidden_layers is an alias), with the default values copy from the python definition. The struct follow the name of HGF<model_type>Config. Notice that the config are not necessary used or implemented in our julia implementation.","category":"page"},{"location":"huggingface_dev/#2.-Check-the-tokenizer","page":"Add New Models","title":"2. Check the tokenizer","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Currently our tokenizer does not support much modification from the loading API. Our loading API should be able to directly load most of the tokenizer. We check the tokenizer by running our huggingface validation code. The validation code use PyCall to load the huggingface transformers and directly compare the result. The validation code for tokenizer take two argument, the model name and the a corpus. You can take any corpus for the test, but we recommand using the xnli dataset. We use the devset of xnli and extract all sentences in the dataset as the testing corpus, which is also available with the following Artifacts.toml:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"[xnli_dev]\ngit-tree-sha1 = \"fb42e6f4a4522b30eb09d21786b90617c4792113\"\nlazy = true\n\n    [[xnli_dev.download]]\n    sha256 = \"9fa4c2b8ff5194eb3eb9cd63a264a2f1e2b9e24f5d71781d524cfe0f4b268c25\"\n    url = \"https://gist.github.com/chengchingwen/27796c1d39efdae744e5abec94ecfdb6/raw/fb42e6f4a4522b30eb09d21786b90617c4792113.tar.gz\"","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"If the validation code passed, we are fine. If the tokenizer cannot be loaded, or the code does not pass the validation, then issues should be opened.","category":"page"},{"location":"huggingface_dev/#3.-Porting-model","page":"Add New Models","title":"3. Porting model","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"To port the model, we need two things: the model weights and an implementation of the model in Julia. Currently we only support loading model weights stored in pytorch pickle format with Transformers.HuggingFace.load_state_dict and it should be able to load the model weights (that support pytorch) without problems. On the other hand, the implementation of the model need to be done manually.","category":"page"},{"location":"huggingface_dev/#Types","page":"Add New Models","title":"Types","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The first thing we do is checking the python implementation source code, in \"src/huggingface/implementation/bloom/modeling_<model_type>.py\" (https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py). We search for class <model_type> (class Bloom) in the file and we can see there are 10 matches and organized into 3 categories:","category":"page"},{"location":"huggingface_dev/#1.-Implementation-Detail-Types-(BloomGelu,-BloomAttention,-BloomMLP,-BloomBlock)","page":"Add New Models","title":"1. Implementation Detail Types (BloomGelu, BloomAttention, BloomMLP, BloomBlock)","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"These types are part of the implementation detail of the model. We won't directly translate these types into Julia. Instead, we use/implement the julia functions/types that perform the similar operations. For example, we would directly use NNlib.gelu for BloomGelu. Usually the attention (BloomAttention) is the most important part we need to look at because it determine whether the model requires a different implementation. We can see that BloomAttention use an attention variant with alibi position embedding. The attention variants would usually be implemented with NeuralAttentionlib.jl (NAlib). If the components for implementing the attention variant are not found in NAlib, open issue/PR in NAlib. Then we would use the components provided from NAlib to implement the attention operation/type.","category":"page"},{"location":"huggingface_dev/#2.-Model-Type-(BloomPreTrainedModel,-BloomModel)","page":"Add New Models","title":"2. Model Type (BloomPreTrainedModel, BloomModel)","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The <model_type>PreTrainedModel (BloomPreTrainedModel) is a python class for inheritance, so we will convert it into a abstract type in julia like (for illustration only):","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"abstract type HGFBloomPreTrainedModel <: HGFPreTrainedModel end","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The \"HGF\" prefix indicate that this type correspond to a python class in huggingface transformers. The <model_type>Model (BloomModel) is the base model for each task-specific model. We'll translate it into a julia type HGF<model_type>Model (HGFBloomModel). For example, the julia type for bloom model is:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"struct HGFBloomModel{E, D} <: HGFBloomPreTrainedModel\n    embed::E\n    decoder::D\nend\n@functor HGFBloomModel\n\n(model::HGFBloomModel)(nt::NamedTuple) = model.decoder(model.embed(nt))","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"This specify the fields of bloom, mark it as a layer, and define the interface. The actual computation are carry out by the embedding and decoder (embed::E and decoder::D), so at this point we can't even tell what computation does bloom perform. This means we can actually directly use the constructor to create a model that is not a \"bloom\" but has the HGFBloomModel type, and this should usually be avoided. The \"real\" bloom model is constructed by the Transformers.HuggingFace.load_model api, which would be discussed in the following sections.","category":"page"},{"location":"huggingface_dev/#3.-Task-Specific-Types-(BloomForCausalLM,-BloomForSequenceClassification,-BloomForTokenClassification,-BloomForQuestionAnswering)","page":"Add New Models","title":"3. Task Specific Types (BloomForCausalLM, BloomForSequenceClassification, BloomForTokenClassification, BloomForQuestionAnswering)","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Currently, each task specific class would also be translated into a julia type having the based model (model field) and a classifier (cls field). For example, the HGFBloomForCausalLM can be defined as:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"struct HGFBloomForCausalLM{M, C} <: HGFBloomPreTrainedModel\n\tmodel::M\n\tcls::C\nend\n@functor HGFBloomForCausalLM\n\n(model::HGFBloomForCausalLM)(nt::NamedTuple) = model.cls(model.model(nt))","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"Similar to HGFBloomModel, this definition does not include the computation and require to use Transformers.HuggingFace.load_model for the correct construction.","category":"page"},{"location":"huggingface_dev/#Implementation","page":"Add New Models","title":"Implementation","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"There are basically 3 steps to implement a new model:","category":"page"},{"location":"huggingface_dev/#1.-Define-Types","page":"Add New Models","title":"1. Define Types","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"We provide a macro Transformers.HuggingFace.@hgfdef that generate the model types for us. For example, the above Julia types is defined with:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"@hgfdef Bloom (\n    Model => (embed, decoder),\n    ForCausalLM,\n)","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"which would be expanded to:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"const HGFBloomPreTrainedModel = HGFPreTrained{:bloom}\n\nstruct HGFBloomModel{EMBED, DECODER} <: HGFPreTrained{:bloom, :model}\n    embed::EMBED\n    decoder::DECODER\nend\n@functor HGFBloomModel\n\n@inline function Transformers.HuggingFace.hgf_model_forward(model::HGFBloomModel, nt::NamedTuple)\n\treturn model.decoder(model.embed(nt))\nend\n\nstruct HGFBloomForCausalLM{MODEL, CLS} <: HGFPreTrained{:bloom, :forcausallm}\n    model::MODEL\n    cls::CLS\nend\n@functor HGFBloomForCausalLM\n\n@inline function Transformers.HuggingFace.hgf_model_forward(model::HGFBloomForCausalLM, nt::NamedTuple)\n\treturn model.cls(model.model(nt))\nend","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The HGFPreTrained is the real abstract type for our huggingface models. It take two type parameters – HGFPreTrained{model_type, task} (e.g. HGFPreTrained{:bloom, :forcausallm}). This allow use to query the supported model through subtypes. Moreover, we can define behaviors for all model of a specific task. For example, Transformers.HuggingFace.hgf_model_loss(model::HGFPreTrained{M, :forcausallm} where M) return the loss function for all model for causal LM task.","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"The @hgfdef macro take 3 arguments: the model type (:bloom), capitalized name (Bloom), and a tuple of tasks. If the model type is omitted, it will use the lowercase of capitalized name. Each task in the tuple is a pair of task name and the forward function body. It collect all getproperty on model in the function body for the field names of the type. If the function body is omitted, it will use (model, cls) as default. If the function body is a tuple of field names, it will convert them into a chain of function call (e.g. (embed, decoder) to model.decoder(model.embed(nt))).","category":"page"},{"location":"huggingface_dev/#2.-Overload-Transformers.HuggingFace.load_model","page":"Add New Models","title":"2. Overload Transformers.HuggingFace.load_model","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"As mentioned above, the Transformers.HuggingFace.load_model serves as the actual constructor for our huggingface models. load_model is dispatch on the type itself. For example:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"function load_model(_type::Type{HGFBloomModel}, cfg, state_dict, prefix)\n    embed = load_model(_type, CompositeEmbedding, cfg, state_dict, prefix)\n    decoder = load_model(_type, TransformerBlock, cfg, state_dict, prefix)\n    return HGFBloomModel(embed, decoder)\nend\n\nfunction load_model(_type::Type{<:HGFBloomPreTrainedModel}, ::Type{<:CompositeEmbedding}, cfg, state_dict, prefix)\n    vocab_size, dims = cfg[:vocab_size], cfg[:hidden_size]\n    factor = Float32(cfg[:initializer_range])\n    token_embed = _load_embed(state_dict, joinname(prefix, \"word_embeddings\"), vocab_size, dims, factor)\n    embed = CompositeEmbedding(token = token_embed)\n    ln = load_model(_type, Layers.LayerNorm, cfg, state_dict, joinname(prefix, \"word_embeddings_layernorm\"))\n    return Layers.Chain(embed, ln)\nend\n\n...","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"load_model(_type::Type{HGFBloomModel}, cfg, state_dict, prefix) isa the main function to overload for loading HGFBloomModel. cfg is the HGFConfig of the loaded model. state_dict is the model weights loaded by Transformers.HuggingFace.load_state_dict which is a dictionay map from flattened python property names to the weight array (e.g. \"word_embeddings.weight\" => Float16[-0.00987 -0.00481 …). Since our model does not follow the same hierarchy and field names, we use prefix and Transformers.HuggingFace.joinname(prefix, another_field_name) to mimic the traversal. We use the corresponding information to reconstruct the correct model with Transformers.Layers. The weight access is hide in the _load_embed function, which is defined as:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"function _load_embed(state_dict, prefix, w_init, pad_idx0 = nothing)\n    embedding = getweight(Layers.Embed, state_dict, joinname(prefix, \"weight\")) do\n        weight = w_init()\n        if !isnothing(pad_idx0)\n            weight[:, pad_idx0 + 1] .= 0\n        end\n        return weight\n    end\n    return Layers.Embed(embedding)\nend","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"All weight accesses are done with getweight which check if the name (joinname(prefix, \"weight\")) is present in the state_dict, and if not, create a new array and store it in the state_dict with the name. This allow us to handle the case that some weight are missing, like using the pretrained model for finetuning on a new task. Besides, we need to overload basemodelkey(::Type{<:HGFPreTrained{:bloom}}) = :transformer for loading the model correctly. This is equivalent to the base_model_prefix class variable of BloomPreTrainedModel in \"modeling_bloom.py\" (https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py).","category":"page"},{"location":"huggingface_dev/#3.-Overload-Transformers.HuggingFace.get_state_dict","page":"Add New Models","title":"3. Overload Transformers.HuggingFace.get_state_dict","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"After finishing the loader, we also need to overload the Transformers.HuggingFace.get_state_dict which extract all weights in a model and store in a flat dictionary. Essentially, model::HGFBloomModel == load_model(HGFBloomModel, cfg, get_state_dict(model)). get_state_dict is dispatch on the model. For example:","category":"page"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"function get_state_dict(m::HGFBloomModel, state_dict, prefix)\n    get_state_dict(HGFBloomModel, m.embed[1], state_dict, prefix)\n    get_state_dict(HGFBloomModel, m.embed[2], state_dict, joinname(prefix, \"word_embeddings_layernorm\"))\n    get_state_dict(HGFBloomModel, m.decoder[1], state_dict, prefix)\n    get_state_dict(HGFBloomModel, m.decoder[2], state_dict, joinname(prefix, \"ln_f\"))\n    return state_dict\nend\n\nfunction get_state_dict(p::Type{<:HGFBloomPreTrainedModel}, m::CompositeEmbedding, state_dict, prefix)\n    get_state_dict(p, m.token, state_dict, joinname(prefix, \"word_embeddings\"))\n    return state_dict\nend\n\nget_state_dict(_, m::Layers.Embed, state_dict, prefix) = get_state_dict(m, state_dict, prefix)\nfunction get_state_dict(m::Layers.Embed, state_dict, prefix)\n    state_dict[joinname(prefix, \"weight\")] = m.embeddings'\n    return state_dict\nend\n...","category":"page"},{"location":"huggingface_dev/#Validation","page":"Add New Models","title":"Validation","text":"","category":"section"},{"location":"huggingface_dev/","page":"Add New Models","title":"Add New Models","text":"After implementing the model, we use the same script mentioned in the tokenizer part to check if our model perform the same computation as Python.","category":"page"},{"location":"getstarted/#Get-Started","page":"Get Started","title":"Get Started","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Transformers.jl contain multiple functionalities, from basic building block for a transformer model to using pretrained  model from huggingface. Each of them is put under different submodule in Transformers.jl.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"You should find more examples in the example folder.","category":"page"},{"location":"getstarted/#Create-a-N-layered-Transformer-model","page":"Get Started","title":"Create a N-layered Transformer model","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"All model building block are in the Layers module. Here we create a simple 3 layered vanilla transformer  (multi-head self-attention + MLP) model, where each attention have 4 heads:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"using Flux\nusing Transformers\n\nnum_layer = 3\nhidden_size = 128\nnum_head = 4\nhead_hidden_size = div(hidden_size, num_head)\nintermediate_size = 4hidden_size\n\ntrf_blocks = Transformer(Layers.TransformerBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"The Transformer layer, by default take a NamedTuple as input, and always return a NamedTuple.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x = randn(Float32, hidden_size, 10 #= sequence length =#, 2 #= batch size =#);\n\njulia> y = trf_blocks(\n    (; hidden_state = x) ); # The model would be apply on the `hidden_state` field.\n\njulia> keys(y)\n(:hidden_state,)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"It also works on high dimension input data:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x2 = reshape(x, hidden_size, 5, 2 #= width 5 x heigh 2 =#, 2 #= batch size =#);\n\njulia> y2 = trf_blocks( (; hidden_state = x2) );\n\njulia> y.hidden_state ≈ reshape(y2.hidden_state, size(y.hidden_state))\ntrue","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Some times you might want to see how the attention score looks like, this can be done by creating a model that return  the attention score as well. The attention score would usually be in shape (key length, query length, head,  batch size):","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"# creating new model\ntrf_blocks_ws = Transformer(Layer.TransformerBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size;\n    return_score = true)\n\n# or transform an old model\ntrf_blocks_ws = Layers.WithScore(trf_blocks)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> y = trf_blocks_ws( (; hidden_state = x) );\n\njulia> keys(y)\n(:hidden_state, :attention_score)\n","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"The model can also take an attention mask to avoid attention looking at the padding tokens. The attention mask would need  construct with NeuralAttentionlib.Masks:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> mask = Masks.LengthMask([5, 7]); # specify the sequence length of each sample in the batch\n\njulia> y3 = trf_blocks_ws( (; hidden_state = x, attention_mask = mask) );\n\njulia> keys(y3)\n(:hidden_state, :attention_mask, :attention_score)\n","category":"page"},{"location":"getstarted/#Create-a-N-layered-Transformer-Decoder-model","page":"Get Started","title":"Create a N-layered Transformer Decoder model","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"For constructing the transformer decoder in the encoder-decoder architecture:","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"trf_dec_blocks_ws = Transformer(Layers.TransformerDecoderBlock,\n    num_layer, relu, num_head, hidden_size, head_hidden_size, intermediate_size;\n    return_score = true)","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> x3 = x = randn(Float32, hidden_size, 7 #= sequence length =#, 2 #= batch size =#);\n\njulia> z = trf_dec_blocks_ws( (; hidden_state = x3, memory = y.hidden_state #= encoder output =#) );\n\njulia> keys(z)\n(:hidden_state, :memory, :cross_attention_score)\n\njulia> size(z.cross_attention_score) # (key length, query length, head, batch size)\n(10, 7, 4, 2)\n","category":"page"},{"location":"getstarted/#Preprocessing-Text","page":"Get Started","title":"Preprocessing Text","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Text processing functionalities are in the TextEncoders module. The TransformerTextEncoder take a tokenize function  and a list of String as the vocabulary. If the tokenize function is omitted, it would use WordTokenizers.tokenize  as the default. Here we create a text encoder that split on every Char and only know 4 characters.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"using Transformers.TextEncoders\n\nchar_tenc = TransformerTextEncoder(Base.Fix2(split, \"\"), map(string, ['A', 'T', 'C', 'G']))","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> char_tenc\nTransformerTextEncoder(\n├─ TextTokenizer(WordTokenization(split_sentences = WordTokenizers.split_sentences, tokenize = Base.Fix2{typeof(split), String}(split, \"\"))),\n├─ vocab = Vocab{String, SizedArray}(size = 8, unk = <unk>, unki = 6),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := TextEncodeBase.with_head_tail(<s>, </s>)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\njulia> data = encode(char_tenc, \"ATCG\")\n(token = Bool[0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 1], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6]))\n\njulia> data.token\n8x6 OneHotArray{8, 2, Vector{OneHot{0x00000008}}}:\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  0\n 0  0  0  0  0  0\n 1  0  0  0  0  0\n 0  0  0  0  0  1\n\njulia> decode(char_tenc, data.token)\n6-element Vector{String}:\n \"<s>\"\n \"A\"\n \"T\"\n \"C\"\n \"G\"\n \"</s>\"\n\njulia> data2 = encode(char_tenc, [\"ATCG\", \"AAAXXXX\"])\n(token = [0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 0;;; 0 1 … 0 0; 0 0 … 0 0; … ; 1 0 … 0 0; 0 0 … 0 1], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6, 9]))\n\njulia> decode(char_tenc, data2.token)\n9×2 Matrix{String}:\n \"<s>\"    \"<s>\"\n \"A\"      \"A\"\n \"T\"      \"A\"\n \"C\"      \"A\"\n \"G\"      \"<unk>\"\n \"</s>\"   \"<unk>\"\n \"<pad>\"  \"<unk>\"\n \"<pad>\"  \"<unk>\"\n \"<pad>\"  \"</s>\"\n","category":"page"},{"location":"getstarted/#Using-(HuggingFace)-Pre-trained-Models","page":"Get Started","title":"Using (HuggingFace) Pre-trained Models","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Use the HuggingFace module for loading the pretrained model. The @hgf_str return a text encoder of the model, and  the model itself.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"julia> bertenc, bert_model = hgf\"bert-base-cased\";\n\njulia> bert_model(encode(bertenc, \"Peter Piper picked a peck of pickled peppers\"))\n(hidden_state = [0.54055643 -0.3517502 … 0.2955708 1.601667; 0.05538677 -0.1114391 … -0.2139448 0.3692414; … ; 0.34500372 0.38523915 … 0.2224255 0.7384993; -0.18260899 -0.05137573 … -0.2833455 -0.23427412;;;], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[13]), pooled = Float32[-0.6727301; 0.42062035; … ; -0.902852; 0.99214816;;])\n","category":"page"},{"location":"getstarted/#GPU","page":"Get Started","title":"GPU","text":"","category":"section"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"Transformers relies on CUDA.jl (or AMDGPU.jl/Metal.jl) for the GPU stuffs.  In Flux we normally use Flux.gpu to convert model or data to the device.  In Transformers, we provide another 2 api (enable_gpu and todevice) for this.  If enable_gpu(true) is set, todevice will be moving data to GPU device, otherwise it is copying data on CPU.  The backend is selected by Flux.gpu_backend!. When calling enable_gpu(), corresponding GPU package (e.g. CUDA.jl)  will be loaded (equivalent to using CUDA in REPL), which requires GPU packages to be installed in the environment.  notice: enable_gpu should only be called in script, it cannot be used during precompilation.","category":"page"},{"location":"getstarted/","page":"Get Started","title":"Get Started","text":"enable_gpu\ntodevice\nTransformers.togpudevice","category":"page"},{"location":"getstarted/#Transformers.enable_gpu","page":"Get Started","title":"Transformers.enable_gpu","text":"enable_gpu(t=true)\n\nEnable gpu for todevice, disable with enable_gpu(false). The backend is selected by Flux.gpu_backend!.  Should only be used in user scripts.\n\n\n\n\n\n","category":"function"},{"location":"getstarted/#Transformers.todevice","page":"Get Started","title":"Transformers.todevice","text":"todevice(x)\n\nMove data to device, only when gpu is enable with enable_gpu, basically equal Flux.gpu. Otherwise just Flux.cpu.\n\n\n\n\n\n","category":"function"},{"location":"getstarted/#Transformers.togpudevice","page":"Get Started","title":"Transformers.togpudevice","text":"togpudevice(x)\n\nMove data to gpu device, backend selected by Flux.gpu_backend!.\n\n\n\n\n\n","category":"function"},{"location":"textencoders/#Transformers.TextEncoders","page":"TextEncoders","title":"Transformers.TextEncoders","text":"","category":"section"},{"location":"textencoders/","page":"TextEncoders","title":"TextEncoders","text":"Text processing module.","category":"page"},{"location":"textencoders/#API-Reference","page":"TextEncoders","title":"API Reference","text":"","category":"section"},{"location":"textencoders/","page":"TextEncoders","title":"TextEncoders","text":"Modules = [Transformers.TextEncoders]\nOrder = [:type, :function]","category":"page"},{"location":"textencoders/#Transformers.TextEncoders.TrfTextEncoder","page":"TextEncoders","title":"Transformers.TextEncoders.TrfTextEncoder","text":"struct TrfTextEncoder{\n    T <: AbstractTokenizer,\n    V <: AbstractVocabulary{String},\n    C, A, EP, OP, DP, TP\n} <: AbstractTransformerTextEncoder\n    tokenizer::T\n    vocab::V\n    config::C\n    annotate::A\n    process::EP\n    onehot::OP\n    decode::DP\n    textprocess::TP\nend\n\nThe general text encoder. TrfTextEncoder has multiple fields that can modify the encode/decode process:\n\n.annotate (default to TextEncoders.annotate_strings): Annotate the input string for the tokenizer,\n\ne.g. String would be treated as a single sentence, not a single word.\n\n.process (default to TextEncodeBase.nestedcall(TextEncoders.string_getvalue)): The pre-process\n\nfunction applied to the tokenization results, e.g. adding special end-of-sentence token, computing attention mask...\n\n.onehot (default to TextEncoders.lookup_fist): Apply onehot encoding on the preprocess result,\n\nthe default behavior takes the first element from the proprocess result and applies onehot encoding.\n\n.decode (default to identity): The function that converts each token id back to string. This can\n\nbe used to handle some tokenizers that use a different set of vocabulary such as gpt2's byte-level vocabulary.\n\n.textprocess (default to TextEncodeBase.join_text): the function that joins the decode-d result\n\nin complete sentence(s).\n\n\n\n\n\n","category":"type"},{"location":"textencoders/#Transformers.TextEncoders.TrfTextEncoder-Tuple{TextEncodeBase.AbstractTokenizer, TextEncodeBase.AbstractVocabulary{String}}","page":"TextEncoders","title":"Transformers.TextEncoders.TrfTextEncoder","text":"TrfTextEncoder(\n    tokenizer     :: AbstractTokenizer ,\n    vocab         :: AbstractVocabulary{String} ,\n    [ annotate    =  TextEncoders.annotate_string ,\n    [ process     =  TextEncodeBase.nestedcall(TextEncoders.string_getvalue) ,\n    [ onehot      =  TextEncoders.lookup_first ,\n    [ decode      =  identity ,\n    [ textprocess =  TextEncodeBase.join_text, ]]]]]\n    ; config...)\n\nConstructor of TrfTextEncoder. All keyword arguments are store in the .config field.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.bert_cased_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.bert_cased_tokenizer","text":"bert_cased_tokenizer(input)\n\nGoogle bert tokenizer which remain the case during tokenization. Recommended for multi-lingual data.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.bert_uncased_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.bert_uncased_tokenizer","text":"bert_uncased_tokenizer(input)\n\nGoogle bert tokenizer which do lower case on input before tokenization.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.gpt_tokenizer-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.gpt_tokenizer","text":"gpt_tokenizer(x)\n\nAn alternative for origin tokenizer (spacy tokenizer) used in gpt model.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_annotate-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_annotate","text":"set_annotate(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the annotate field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_config-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_config","text":"set_config(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the config field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_decode-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_decode","text":"set_decode(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the decode field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_onehot-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_onehot","text":"set_onehot(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the onehot field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_process-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_process","text":"set_process(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the process field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_textprocess-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_textprocess","text":"set_textprocess(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the textprocess field replaced with builder(e).\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_tokenizer-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_tokenizer","text":"set_tokenizer(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the tokenizer field replaced with builder(e). builder can either return  a AbstractTokenizer or a AbstractTokenization.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.set_vocab-Tuple{Any, Transformers.TextEncoders.TrfTextEncoder}","page":"TextEncoders","title":"Transformers.TextEncoders.set_vocab","text":"set_vocab(builder, e::TrfTextEncoder)\n\nReturn a new text encoder with the vocab field replaced with builder(e). builder can either return  a AbstractVocabulary{String} or a AbstractVector{String}.\n\n\n\n\n\n","category":"method"},{"location":"textencoders/#Transformers.TextEncoders.text_standardize-Tuple{Any}","page":"TextEncoders","title":"Transformers.TextEncoders.text_standardize","text":"text_standardize(text)\n\nThe function in the origin gpt code. Fixes some issues the spacy tokenizer had on books corpus also does  some whitespace standardization.\n\n\n\n\n\n","category":"method"},{"location":"changelog/#ChangeLogs-(from-0.1.x-to-0.2.0)","page":"ChangeLogs","title":"ChangeLogs (from 0.1.x to 0.2.0)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"v0.2 is a rewrite of the whole package. Most layers and API in 0.1 is removed or changed. Some of them are replaced  with new one. The basic policy is, if a functionality is achievable with a well-maintained package easily, or there  isn't much gain by self-hosting/maintaining it, then we remove the functionality from Transformers.jl.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Here is list of the changes with brief explanation:","category":"page"},{"location":"changelog/#Transformers.Pretrain","page":"ChangeLogs","title":"Transformers.Pretrain","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Pretrain module is entirely removed, due to the duplication of functionality v.s. Transformers.HuggingFace.  We do not host the small list of the origin official released pretrained weights anymore. All use that require a  pretrained weight should refer to HuggingFace module. This is a table of the old pretrain name and corresponding  huggingface model name:","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"old pretrain name corresponding huggingface model name\ncased_L-12_H-768_A-12 bert-base-cased\nuncased_L-12_H-768_A-12 bert-base-uncased\nchinese_L-12_H-768_A-12 bert-base-chinese\nmulti_cased_L-12_H-768_A-12 bert-base-multilingual-cased\nmultilingual_L-12_H-768_A-12 bert-base-multilingual-uncased\ncased_L-24_H-1024_A-16 bert-large-cased\nuncased_L-24_H-1024_A-16 bert-large-uncased\nwwm_cased_L-24_H-1024_A-16 bert-large-cased-whole-word-masking\nwwm_uncased_L-24_H-1024_A-16 bert-large-uncased-whole-word-masking\nscibert_scivocab_cased allenai/scibert_scivocab_cased\nscibert_scivocab_uncased allenai/scibert_scivocab_uncased\nscibert_basevocab_cased N/A\nscibert_basevocab_uncased N/A\nOpenAIftlm openai-gpt","category":"page"},{"location":"changelog/#Transformers.Stacks","page":"ChangeLogs","title":"Transformers.Stacks","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Stacks module is entirely removed. Stacks provide a small DSL for creating nontrivial Chain of layers.  However, the DSL isn't intuitive enough and it also doesn't seems worth maintaining a DSL. We don't provide  direct replacement for this, but for the specific use case of building transformer models, we have a few new  constructors/layers in Transformers.Layers.","category":"page"},{"location":"changelog/#Transformers.Basic","page":"ChangeLogs","title":"Transformers.Basic","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Basic module is now destructed and most of the elements in Basic is separated to other module/package.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Transformer and TransformerDecoder: The Transformer/TransformerDecoder layer is replaced with the new  implementation in Layers (the Layers.TransformerBlock, Layers.TransformerDecoderBlock, and friends).\nMultiheadAttention: The implementation of attention operations are move out to  NeuralAttentionlib. In NeuralAttentionlib, we can use  multihead_qkv_attention to do the same computation. Since most transformer variant only use a modified version  of self or cross attention, we do not provied the MultiheadAttention layer type. One should be able to redefine  the MultiheadAttention layer type with Flux and NeuralAttentionlib easily. For example:\nusing Flux, Functors\nusing NeuralAttentionlib: multihead_qkv_attention, CausalMask\n\nstruct MultiheadAttention{Q,K,V,O}\n    head::Int\n    future::Bool\n    iqproj::Q\n    ikproj::K\n    ivproj::V\n    oproj::O\nend\n@functor MultiheadAttention (iqproj, ikproj, ivporj, oproj)\nMultiheadAttention(head, hidden_size, head_size; future = true) =\n    MultiheadAttention(head, future,\n        Dense(hidden_size, head_size * head),\n        Dense(hidden_size, head_size * head),\n        Dense(hidden_size, head_size * head),\n        Dense(head_size * head, hidden_size),\n    )\n\n(mha::MultiheadAttention)(q, k, v) = mha.oproj(multihead_qkv_attention(mha.head,\n    mha.iqproj(q), mha.ikproj(k), mha.ivproj(v), mha.future ? nothing : CausalMask()))\nTransformerModel: This is just a Flux layer with embedding layer, transformer layer, and classifier layer   bundle together. One can define this easily with Flux/Functors API, thus removed.\nPositionwise, PwFFN, and @toNd: This was originally designed for applying Flux.Dense on 3-dim arrays,  but since Flux.Dense support multi-dim input now. This isn't needed and thus removed.\nEmbeddingDecoder: Replaced with Layers.EmbedDecoder. Name change and support extra trainable bias parameter.\nPositionEmbedding: This is replace with Layers.SinCosPositionEmbed and Layers.FixedLenPositionEmbed for  the old trainable keyword argument setting.\ncrossentropy with masking: We extend Flux.logitcrossentropy and Flux.crossentropy with 3-args  input (the prediction, label, and mask) and 4-args input (sum or mean, prediciton, label, and mask).\nkldivergence: In our use case (i.e. training language model), this is equivalent to cross-entropy, thus removed.\nlogcrossentropy/logkldivergence: This is a fault design. Originally I would put a logsoftmax at the head of  the prediction head. However, that is not only unnecessary but also increasing the amount of memory needed.  One should use Flux.logitcrossentropy without the logsoftmax directly.\nVocabulary: Replaced with TextEncodeBase.Vocab.\nwith_firsthead_tail/segment_and_concat/concat: These can be implemented with TextEncodeBase.SequenceTemplate   and friends thus removed.\ngetmask: The attention mask functionality is moved to NeuralAttentionlib. Manually construct attention mask   should use constructor in NeuralAttentionlib.Masks.","category":"page"},{"location":"changelog/#Transformers.Layers-(new)","page":"ChangeLogs","title":"Transformers.Layers (new)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The Layers module is a new module introduced in v0.2.0. It provide a set layer types for construct transformer  model variants.","category":"page"},{"location":"changelog/#Transformers.TextEncoders-(new)","page":"ChangeLogs","title":"Transformers.TextEncoders (new)","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The TextEncoders module is a new module introduced in v0.2.0. Basically all old functionality about text preprocessing  are moved to this module, including WordPiece, Unigram, BertTextEncoder, GPT2TextEncoder, etc.","category":"page"},{"location":"changelog/#Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain","page":"ChangeLogs","title":"Transformers.BidirectionalEncoder / Transformers.GenerativePreTrain","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"These modules are removed since we are switching to the Transformers.HuggingFace for the pretrained model. The text  encoder are moved to Transformers.TextEncoders. Weight loading and conversion functionality are removed. If you  need that, use the tools that huggingface transformers python package provided and make sure the model can be loaded  with pytorch. Then we can use the weight in pytorch format.","category":"page"},{"location":"changelog/#Transformers.HuggingFace","page":"ChangeLogs","title":"Transformers.HuggingFace","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"The changes in Transformers.HuggingFace are mainly about the configurations and models. The tokenizer/textencoder part  are mostly the same, except the process functions.","category":"page"},{"location":"changelog/#Configuration","page":"ChangeLogs","title":"Configuration","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"For the configuration, the loading mechanism is changed. In previous version, each model type need to define a specific  HGF<XXModelType>Config struct where XXModelType is the model type name. The reason for that is, for some reason,  huggingface transformers doesn't serialize all the configuration values into the file, but rely on their constructor  with pre-defined default values instead. As a result, some model only need the configuration file, while some need the  python code for the defaults as well. The hgf config struct was more like a interal data carrier. You usually  won't (and actually can't) manipulate the model with it.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"In v0.2, we tried to make the process for adding model more automatic, and enable the ability to build model with  different configurations. The struct for holding the configuration is now changed to a parametric struct depending  on a Symbol parameter specifying the model type (e.g. HGFConfig{:bert}). With this, the specific  HGF<XXModelType>config can be constructed on the fly. The HGFConfig has 2 field, one for storing the read-only  deserialized object loaded from the configuration file, and another for the overwritten values. This should turn the  config struct into a user level interface.","category":"page"},{"location":"changelog/#Model","page":"ChangeLogs","title":"Model","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"For the model part, the main change is that we do not make a 1-1 mapping between the python model/layer class and our  julia layer struct. When one wants to add a new model type, there are actually 2 things need to be done. One is  defining a model forward method that can do the same computation as the python model, and another is defining a  mapping between the python model and the julia model (so that the model parameters/weights can be transferred between  2 language). In the previous version, we chose to make a 1-1 mapping between the model, so that the parameters/weights  loading process can be fully automatic. However, for some reason, huggingface transformers is not reusing their  attention or transformer implementation for each model type. Which means for different model type, even if they are  actually doing the same computation (i.e. the computation graph is the same), the model layout can be different  (e.g. consider the differences between Chain(Chain(dense1, dense2), dense3) and Chain(dense1, dense2, dense3)).  As a result, these make implementing the model forward method a real pain, and also it's hard to apply optimizations.","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"We noticed that the model forward method is more important and difficult than the model mapping. On the other hand,  though manually defining model mapping is tedious, it's less prone to go wrong. So instead of making a 1-1 mapping for  fully automatic model loading, we choose to reduce the work needed for forward method. In v0.2, the attention  implementation is switched to NeuralAttentionlib's modulated implementation and we build all internal layers with layer  from Transformers.Layers. As a result, layers like FakeTH<XXLayer> or HGF<XXModelType>Attention/MLP/... are  removed, only the outer-most types remain (e.g. HGFBertModel, HGFGPT2LMHeadModel...).","category":"page"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"Since we want to make it possible to finetune a pretrained model on new dataset/task easily, the model loading would  be a combination of initialization and parameters/weights loading. In normal Flux workflow, you would build a complete  new model and then inplace load the parameter/weight values into the specific layers/arrays in the model. In v0.2, we  combine the 2 step into one load_model function, which take the model type, configuration, and a state dictionary (  the term comes from PyTorch, which is a OrderedDict of variable names to weights). load_model would either  lookup variable from the state dictionary, or initialize with configuration, recursively. As a result,  load_model! is removed.","category":"page"},{"location":"changelog/#Behavior-Changes","page":"ChangeLogs","title":"Behavior Changes","text":"","category":"section"},{"location":"changelog/","page":"ChangeLogs","title":"ChangeLogs","text":"All text encoder (including HuggingFace one) process function returned NamedTuple: Some field name changed,  tok => token, mask => attention_mask.\nMost layer/model from Transformers.jl would be taking and returning NamedTuple.\nFor HuggingFace model: All input is basically NamedTuple. The returned NamedTuple field name from the forward  method is also changed.","category":"page"},{"location":"layers/#Transformers.Layers","page":"Layers","title":"Transformers.Layers","text":"","category":"section"},{"location":"layers/","page":"Layers","title":"Layers","text":"Layer building blocks of Transformers.jl. Most of the layers are designed to work with NamedTuples. It would take a  NamedTuple as input, finding correct names as its arguments for computation, ignoring extra fields in the  NamedTuple, store the computation result in the input NamedTuple with correct names (conceptually, since  NamedTuple is immutable) and return it.","category":"page"},{"location":"layers/","page":"Layers","title":"Layers","text":"These layer types are mostly compatible with Flux.","category":"page"},{"location":"layers/#API-Reference","page":"Layers","title":"API Reference","text":"","category":"section"},{"location":"layers/","page":"Layers","title":"Layers","text":"Modules = [Transformers.Layers]\nOrder = [:type, :function]","category":"page"},{"location":"layers/#Transformers.Layers.ApplyEmbed","page":"Layers","title":"Transformers.Layers.ApplyEmbed","text":"ApplyEmbed([apply = .+,] embed)\n\nA layer that help to get embedding and apply on the input. Used with position embeddings.\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.CrossAttention-Tuple{Int64, Int64}","page":"Layers","title":"Transformers.Layers.CrossAttention","text":"CrossAttention(head::Int, hidden_size::Int [, head_hidden_size::Int = hidden_size ÷ head ];\n               dropout::Union{Nothing, Float64} = nothing, return_score = false)\n\nCreate a multi-head cross attention layer with head heads and head_hidden_size per head.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.CrossAttention-Tuple{NeuralAttentionlib.AbstractAttenOp, Int64, Int64, Int64}","page":"Layers","title":"Transformers.Layers.CrossAttention","text":"CrossAttention(atten_op::AbstractAttenOp, head::Int, hidden_size::Int, head_hidden_size::Int)\n\nCreate a cross attention layer with given atten_op.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.Embed","page":"Layers","title":"Transformers.Layers.Embed","text":"Embed(hidden_size::Int, vocab_size::Int; scale = nothing)\n\nAn Embedding layer that take an array of integer / one-hot encoding and return a multi-dimensional array as  embedded vectors and scale with scale.\n\nSee also: EmbedDecoder\n\nExample\n\njulia> embed = Embed(7, 10; scale = 100)\nEmbed(7, 10, scale = 100)\n\njulia> embed([1,3,5])\n7×3 Matrix{Float32}:\n  0.86955    1.14728    0.43275\n -0.378461  -0.112709   3.33885\n -1.61534   -2.55506    1.08488\n -0.833164   0.565268  -1.32531\n  0.820126  -5.11536   -0.75666\n -2.13458    1.25796   -1.47247\n  3.20416    0.872459   0.980557\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.EmbedDecoder","page":"Layers","title":"Transformers.Layers.EmbedDecoder","text":"EmbedDecoder(embed::Embed; bias = false)\n\nA layer that share weight with an embedding layer embed and return the logit.\n\nSee also: Embed\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.FixedLenPositionEmbed","page":"Layers","title":"Transformers.Layers.FixedLenPositionEmbed","text":"FixedLenPositionEmbed(hidden_size::Int, max_length::Int = 1024)\n\nAn trainable position embedding layer.\n\nSee also: SinCosPositionEmbed\n\nExample\n\njulia> pe = FixedLenPositionEmbed(7)\nFixedLenPositionEmbed(7, 1024)\n\njulia> pe(5)\n7×5 Matrix{Float32}:\n -0.0330963    -0.0412815    -0.0110067    0.0299395   -0.0303213\n  0.0203617    -0.000259752  -0.0300242    0.00573144   0.0147597\n  0.00662918   -0.0222377    -9.40627f-5  -0.038285    -0.0467688\n -0.00358604    0.0344152     0.0101526   -0.00750311   0.0173139\n  0.000689436   0.0116299    -0.00478128  -0.0331492    0.0148091\n  0.000711651  -0.0198647    -0.0037188    0.00427536  -0.0172123\n -0.00987371   -0.0385056    -0.00103168   0.0578125    0.00286929\n\njulia> pe([1,3])\n7×2 Matrix{Float32}:\n -0.0330963    -0.0110067\n  0.0203617    -0.0300242\n  0.00662918   -9.40627f-5\n -0.00358604    0.0101526\n  0.000689436  -0.00478128\n  0.000711651  -0.0037188\n -0.00987371   -0.00103168\n\njulia> pe(randn(3,3))\n7×3 Matrix{Float32}:\n -0.0330963    -0.0412815    -0.0110067\n  0.0203617    -0.000259752  -0.0300242\n  0.00662918   -0.0222377    -9.40627f-5\n -0.00358604    0.0344152     0.0101526\n  0.000689436   0.0116299    -0.00478128\n  0.000711651  -0.0198647    -0.0037188\n -0.00987371   -0.0385056    -0.00103168\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.Fork","page":"Layers","title":"Transformers.Layers.Fork","text":"Fork(layers...)\n\nA layer for applying each layers to the same input and return a Tuple. For example (Fork(dense1, dense2))(x) is  equivalent to (dense1(x), dense2(x)).\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.NSplit","page":"Layers","title":"Transformers.Layers.NSplit","text":"NSplit(n::Integer, layer)\n\nA layer for splitting the result of layer into n parts in the first dimension and return a Tuple. For  example (NSplit(2, dense))(x) is equivalent to  y = dense(x); s1 = size(y, 1); (y[begin:div(s1, 2)-1, :], y[div(s1, 2):end, :].\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.PostNormTransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PostNormTransformerBlock","text":"PostTransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                     attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a post-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PostNormTransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PostNormTransformerDecoderBlock","text":"PostTransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                            attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                            return_score = false, return_self_attention_score = false)\n\nCreate a post-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PreNormTransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PreNormTransformerBlock","text":"PreNormTransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                        attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a pre-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.PreNormTransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.PreNormTransformerDecoderBlock","text":"PreTransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                           attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                           return_score = false, return_self_attention_score = false)\n\nCreate a pre-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SelfAttention-Tuple{Int64, Int64}","page":"Layers","title":"Transformers.Layers.SelfAttention","text":"SelfAttention(head::Int, hidden_size::Int [, head_hidden_size::Int = hidden_size ÷ head ];\n              dropout::Union{Nothing, Float64} = nothing, return_score = false, causal = false)\n\nCreate a multi-head self attention layer with head heads and head_hidden_size per head.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SelfAttention-Tuple{NeuralAttentionlib.AbstractAttenOp, Int64, Int64, Int64}","page":"Layers","title":"Transformers.Layers.SelfAttention","text":"SelfAttention(atten_op::AbstractAttenOp, head::Int, hidden_size::Int, head_hidden_size::Int)\n\nCreate a self attention layer with given atten_op.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.SinCosPositionEmbed","page":"Layers","title":"Transformers.Layers.SinCosPositionEmbed","text":"SinCosPositionEmbed(hidden_size::Int)\n\nThe absolute sin cos postion embedding.\n\nSee also: FixedLenPositionEmbed\n\nExample\n\njulia> pe = SinCosPositionEmbed(7)\nSinCosPositionEmbed(default_position_func(static(7)), 7, normalized = false)\n\njulia> pe(5)\n7×5 Matrix{Float32}:\n 0.0  0.841471      0.909297      0.14112     -0.756802\n 1.0  0.540302     -0.416147     -0.989992    -0.653644\n 0.0  0.0719065     0.143441      0.214232     0.283915\n 1.0  0.997411      0.989659      0.976783     0.95885\n 0.0  0.00517945    0.0103588     0.0155378    0.0207164\n 1.0  0.999987      0.999946      0.999879     0.999785\n 0.0  0.000372759   0.000745519   0.00111828   0.00149104\n\njulia> pe([1,3])\n7×2 Matrix{Float32}:\n 0.0   0.909297\n 1.0  -0.416147\n 0.0   0.143441\n 1.0   0.989659\n 0.0   0.0103588\n 1.0   0.999946\n 0.0   0.000745519\n\njulia> pe(randn(3,3))\n7×3 Matrix{Float64}:\n 0.0  0.841471      0.909297\n 1.0  0.540302     -0.416147\n 0.0  0.0719065     0.143441\n 1.0  0.997411      0.989659\n 0.0  0.00517945    0.0103588\n 1.0  0.999987      0.999946\n 0.0  0.000372759   0.000745519\n\n\n\n\n\n\n","category":"type"},{"location":"layers/#Transformers.Layers.Transformer-Tuple{Type{<:Transformers.Layers.AbstractTransformerBlock}, Int64, Vararg{Any}}","page":"Layers","title":"Transformers.Layers.Transformer","text":"Transformer(T::Type{<:AbstractTransformerBlock}, n::Int, args...; kwargs...)\n\nCreate n layers of transformer blocks with T(args...; kwargs...).\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.TransformerBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.TransformerBlock","text":"TransformerBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                 attention_dropout = nothing, dropout = nothing, return_score = false)\n\nCreate a post-LN transformer encoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention. intermediate_size, hidden_size (and act) would be use to create the 2 layered  feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.TransformerDecoderBlock-NTuple{4, Int64}","page":"Layers","title":"Transformers.Layers.TransformerDecoderBlock","text":"TransformerDecoderBlock([act,] head::Int, hidden_size::Int [, head_hidden_size::Int], intermediate_size::Int;\n                        attention_dropout = nothing, dropout = nothing, cross_attention_dropout = nothing,\n                        return_score = false, return_self_attention_score = false)\n\nCreate a post-LN transformer decoder block. head, hidden_size (and head_hidden_size) are parameters of  SelfAttention and CrossAttention. intermediate_size, hidden_size (and act) would  be use to create the 2 layered feed-forward layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Transformers.Layers.no_dropout","page":"Layers","title":"Transformers.Layers.no_dropout","text":"no_dropout(model)\n\nCreating a new model sharing all parameters with model but disable all dropout.\n\n\n\n\n\n","category":"function"},{"location":"layers/#Transformers.Layers.set_dropout","page":"Layers","title":"Transformers.Layers.set_dropout","text":"set_dropout(model, p)\n\nCreating a new model sharing all parameters with model but set all dropout probability to p.\n\n\n\n\n\n","category":"function"},{"location":"layers/#Transformers.Layers.testmode-Tuple{Any}","page":"Layers","title":"Transformers.Layers.testmode","text":"testmode(model)\n\nCreating a new model sharing all parameters with model but used for testing. Currently this is just  no_dropout.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace","page":"User Interface","title":"Transformers.HuggingFace","text":"","category":"section"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"Module for loading pre-trained model from HuggingFace.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"info: Info\nWe provide a set of API to download & load a pretrain model from huggingface hub. This is mostly manually done, so we  only have a small set of available models. The most practical way to check if a model is available in Transformers  is to run the HuggingFaceValidation code in the example folder,  which use PyCall.jl to load the model in both Python and Julia. Open issues/PRs if you find a model you want is  not supported here.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"There are basically 3 main api for loading the model, HuggingFace.load_config, HuggingFace.load_tokenizer, HuggingFace.load_model. These are the underlying function of the HuggingFace.@hgf_str macro. You can get a better control of the loading process.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"We can load a specific config of a specific model, no matter it's actually supported by Transformers.jl.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"julia> load_config(\"google/pegasus-xsum\")\nTransformers.HuggingFace.HGFConfig{:pegasus, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}} with 45 entries:\n  :use_cache                       => true\n  :d_model                         => 1024\n  :scale_embedding                 => true\n  :add_bias_logits                 => false\n  :static_position_embeddings      => true\n  :encoder_attention_heads         => 16\n  :num_hidden_layers               => 16\n  :encoder_layerdrop               => 0\n  :num_beams                       => 8\n  :max_position_embeddings         => 512\n  :model_type                      => \"pegasus\"\n  ⋮                                => ⋮\n","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"This would give you all value available in the downloaded configuration file. This might be enough for a some model,  but there are other model that use the default value hard coded in their python code.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"Sometime you would want to add/overwrite  some of the value. This can be done be calling HGFConfig(old_config; key_to_update = new_value, ...). These is used  primary for customizing model loading. For example, you can load a bert-base-cased model for sequence classification  task. However, if you directly load the model:","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"julia> bert_model = hgf\"bert-base-cased:ForSequenceClassification\";\n\njulia> bert_model.cls.layer\nDense(W = (768, 2), b = true)","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"The model is default creating model for 2 class of label. So you would need to load the config and update the field  about number of labels and create the model with the new config:","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"julia> bertcfg = load_config(\"bert-base-cased\");\n\njulia> bertcfg.num_labels\n2\n\njulia> mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);\n\njulia> mycfg.num_labels\n3\n\njulia> _bert_model = load_model(\"bert-base-cased\", :ForSequenceClassification; config = mycfg);\n\njulia> _bert_model.cls.layer\nDense(W = (768, 3), b = true)\n","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"All config field name follow the same name as huggingface, so you might need to read their document for what  is available. However, not every configuration work in Transformers.jl. It's better to check the source  src/huggingface/implementation. All supported models would need to overload the load_model and provided an implementation in Julia to be  workable.","category":"page"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"For the tokenizer, load_tokenizer is basically the same as calling with @hgf_str. Currently providing customized  config doesn't change much stuff. The tokenizer might also work for unsupported model because some serialize the whole  tokenizer object, but not every model does that or they use something not covered by our implementation.","category":"page"},{"location":"huggingface/#API-Reference","page":"User Interface","title":"API Reference","text":"","category":"section"},{"location":"huggingface/","page":"User Interface","title":"User Interface","text":"Modules = [Transformers.HuggingFace]\nOrder = [:macro, :type, :function]","category":"page"},{"location":"huggingface/#Transformers.HuggingFace.@hgf_str-Tuple{Any}","page":"User Interface","title":"Transformers.HuggingFace.@hgf_str","text":"`hgf\"<model-name>:<item>\"`\n\nGet item from model-name. This will ensure the required data are downloaded. item can be \"config\",  \"tokenizer\", and model related like \"Model\", or \"ForMaskedLM\", etc. Use get_model_type to see what  model/task are supported. If item is omitted, return a Tuple of <model-name>:tokenizer and <model-name>:model.\n\n\n\n\n\n","category":"macro"},{"location":"huggingface/#Transformers.HuggingFace.HGFConfig","page":"User Interface","title":"Transformers.HuggingFace.HGFConfig","text":"HGFConfig{model_type} <: AbstractDict{Symbol, Any}\n\nThe type for holding the configuration for huggingface model model_type.\n\nHGFConfig(base_cfg::HGFConfig; kwargs...)\n\nReturn a new HGFConfig object for the same model_type with fields updated with kwargs.\n\nExample\n\njulia> bertcfg = load_config(\"bert-base-cased\");\n\njulia> bertcfg.num_labels\n2\n\njulia> mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);\n\njulia> mycfg.num_labels\n3\n\n\nExtended help\n\nEach HGFConfig has a pre-defined set of type-dependent default field values and some field name aliases. For example,  (cfg::HGFConfig{:gpt2}).hidden_size is an alias of (cfg::HGFConfig{:gpt2}).n_embd. Using propertynames,  hasproperty, getproperty, getindex will access the default field values if the key is not present in the loaded  configuration. On the other hand, using length, keys, haskey, get, iterate will not interact with the  default values (while the name aliases still work).\n\njulia> fakegpt2cfg = HuggingFace.HGFConfig{:gpt2}((a=3,b=5))\nTransformers.HuggingFace.HGFConfig{:gpt2, @NamedTuple{a::Int64, b::Int64}, Nothing} with 2 entries:\n  :a => 3\n  :b => 5\n\njulia> myfakegpt2cfg = HuggingFace.HGFConfig(fakegpt2cfg; hidden_size = 7)\nTransformers.HuggingFace.HGFConfig{:gpt2, @NamedTuple{a::Int64, b::Int64}, @NamedTuple{n_embd::Int64}} with 3 entries:\n  :a      => 3\n  :b      => 5\n  :n_embd => 7\n\njulia> myfakegpt2cfg[:hidden_size] == myfakegpt2cfg.hidden_size == myfakegpt2cfg.n_embd\ntrue\n\njulia> myfakegpt2cfg.n_layer\n12\n\njulia> get(myfakegpt2cfg, :n_layer, \"NOT_FOUND\")\n\"NOT_FOUND\"\n\n\n\n\n\n\n","category":"type"},{"location":"huggingface/#Transformers.HuggingFace.get_model_type","page":"User Interface","title":"Transformers.HuggingFace.get_model_type","text":"get_model_type(model_type)\n\nSee the list of supported model type of given model. For example, use get_mdoel_type(:gpt2) to see all model/task  that gpt2 support. The keys of the returned NamedTuple are all possible task which can be used in  load_model or @hgf_str.\n\nExample\n\njulia> HuggingFace.get_model_type(:gpt2)\n(model = Transformers.HuggingFace.HGFGPT2Model, lmheadmodel = Transformers.HuggingFace.HGFGPT2LMHeadModel)\n\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.get_state_dict","page":"User Interface","title":"Transformers.HuggingFace.get_state_dict","text":"get_state_dict(model)\n\nGet the state_dict of the model.\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.load_config-Tuple{Any}","page":"User Interface","title":"Transformers.HuggingFace.load_config","text":"load_config(model_name; local_files_only = false, cache = true)\n\nLoad the configuration file of model_name from huggingface hub. By default, this function would check if model_name  exists on huggingface hub, download the configuration file (and cache it if cache is set), and then load and return  the config::HGFConfig. If local_files_only = false, it would check whether the configuration file is up-to-date  and update if not (and thus require network access every time it is called). By setting local_files_only = true, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  HuggingFaceApi.jl package. This function would require the configuration file has a field about the model_type, if  not, use load_config(model_type, HuggingFace.load_config_dict(model_name; local_files_only, cache)) with model_type  manually provided.\n\nSee also: HGFConfig\n\nExample\n\njulia> load_config(\"bert-base-cased\")\nTransformers.HuggingFace.HGFConfig{:bert, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Nothing} with 19 entries:\n  :architectures                => [\"BertForMaskedLM\"]\n  :attention_probs_dropout_prob => 0.1\n  :gradient_checkpointing       => false\n  :hidden_act                   => \"gelu\"\n  :hidden_dropout_prob          => 0.1\n  :hidden_size                  => 768\n  :initializer_range            => 0.02\n  :intermediate_size            => 3072\n  :layer_norm_eps               => 1.0e-12\n  :max_position_embeddings      => 512\n  :model_type                   => \"bert\"\n  :num_attention_heads          => 12\n  :num_hidden_layers            => 12\n  :pad_token_id                 => 0\n  :position_embedding_type      => \"absolute\"\n  :transformers_version         => \"4.6.0.dev0\"\n  :type_vocab_size              => 2\n  :use_cache                    => true\n  :vocab_size                   => 28996\n\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_config-Tuple{Union{Symbol, Val}, Any}","page":"User Interface","title":"Transformers.HuggingFace.load_config","text":"load_config(model_type, cfg)\n\nLoad cfg as model_type. This is used for manually load a config when model_type is not specified in the config.  model_type is a Symbol of the model type like :bert, :gpt2, :t5, etc.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_hgf_pretrained-Tuple{Any}","page":"User Interface","title":"Transformers.HuggingFace.load_hgf_pretrained","text":"load_hgf_pretrained(name)\n\nThe underlying function of @hgf_str.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_model","page":"User Interface","title":"Transformers.HuggingFace.load_model","text":"load_model([model_type::Symbol,] model_name, task = :model [, state_dict];\n           trainmode = false, config = nothing, local_files_only = false, cache = true)\n\nLoad the model of model_name for task. This function would load the state_dict of model_name and build a new  model according to config, task, and the state_dict. local_files_only and cache kwargs would be pass directly  to both load_state_dict and load_config if not provided. This function would require the  configuration file has a field about the model_type, if not, use load_model(model_type, model_name, task; kwargs...)  with model_type manually provided. trainmode = false would disable all dropouts. The state_dict can be directly  provided, this is used when you want to create a new model with the state_dict in hand. Use get_model_type  to see what task is available.\n\nSee also: get_model_type, load_state_dict, load_config, HGFConfig\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.load_model-Tuple{Type, Any}","page":"User Interface","title":"Transformers.HuggingFace.load_model","text":"load_model(::Type{T}, config, state_dict = OrderedDict())\n\nCreate a new model of T according to config and state_dict. missing parameter would initialized according  to config. Set the JULIA_DEBUG=Transformers environment variable to see what parameters are missing.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_state_dict-Tuple{Any}","page":"User Interface","title":"Transformers.HuggingFace.load_state_dict","text":"load_state_dict(model_name; local_files_only = false, force_format = :auto, cache = true)\n\nLoad the state_dict from the given model_name from huggingface hub. By default, this function would check if  model_name exists on huggingface hub, download the model file (and cache it if cache is set), and then load  and return the state_dict. If local_files_only = false, it would check whether the model file is up-to-date and  update if not (and thus require network access every time it is called). By setting local_files_only = true, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  HuggingFaceApi.jl package. If force_format is :auto it will automatically selects the format from which the  weights will be loaded. If force_format is :pickle or :safetensor, it will prefer relevant file.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.load_tokenizer","page":"User Interface","title":"Transformers.HuggingFace.load_tokenizer","text":"load_tokenizer(model_name; config = nothing, local_files_only = false, cache = true)\n\nLoad the text encoder of model_name from huggingface hub. By default, this function would check if model_name  exists on huggingface hub, download all required files for this text encoder (and cache these files if cache is  set), and then load and return the text encoder. If local_files_only = false, it would check whether all cached  files are up-to-date and update if not (and thus require network access every time it is called). By setting  local_files_only = true, it would try to find the files from the cache directly and error out if not found.  For managing the caches, see the HuggingFaceApi.jl package.\n\nExample\n\njulia> load_tokenizer(\"t5-small\")\nT5TextEncoder(\n├─ TextTokenizer(MatchTokenization(PrecompiledNormalizer(WordReplaceNormalizer(UnigramTokenization(EachSplitTokenization(splitter = isspace), unigram = Unigram(vocab_size = 32100, unk = <unk>)), pattern = r\"^(?!▁)(.*)$\" => s\"▁\u0001\"), precompiled = PrecompiledNorm(...)), 103 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 32100, unk = <unk>, unki = 3),\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[(token, segment)] := SequenceTemplate{String}(Input[1]:<type=1> </s>:<type=1> (Input[2]:<type=1> </s>:<type=1>)...)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n)\n\n\n\n\n\n\n","category":"function"},{"location":"huggingface/#Transformers.HuggingFace.save_config-Tuple{Any, Any}","page":"User Interface","title":"Transformers.HuggingFace.save_config","text":"save_config(model_name, config; path = pwd(), config_name = CONFIG_NAME, force = false)\n\nSave the config at <path>/<model_name>/<config_name>. This would error out if the file already exists but force  not set.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.save_model-Tuple{Any, Any}","page":"User Interface","title":"Transformers.HuggingFace.save_model","text":"save_model(model_name, model; path = pwd(), weight_name = PYTORCH_WEIGHTS_NAME, force = false)\n\nsave the model statedict at `<path>/<modelname>/<weight_name>. This would error out if the file already exists  butforce` not set.\n\n\n\n\n\n","category":"method"},{"location":"huggingface/#Transformers.HuggingFace.state_dict_to_namedtuple-Tuple{Any}","page":"User Interface","title":"Transformers.HuggingFace.state_dict_to_namedtuple","text":"state_dict_to_namedtuple(state_dict)\n\nconvert state_dict into nested NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Transformers","category":"page"},{"location":"#Transformers.jl","page":"Home","title":"Transformers.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia implementation of Transformers models","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of Transformers: The Julia solution  for using Transformer models.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add Transformers","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"getstarted.md\",\n  \"tutorial.md\",\n  \"layers.md\",\n  \"textencoders.md\",\n  \"huggingface.md\",\n  \"huggingface_dev.md\",\n]\nDepth = 3","category":"page"},{"location":"api_ref/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api_ref/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api_ref/","page":"API Reference","title":"API Reference","text":"Modules = [Transformers]","category":"page"},{"location":"api_ref/#Flux.Losses.crossentropy-Tuple{AbstractArray, AbstractArray, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Flux.Losses.crossentropy","text":"crossentropy(ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask; ϵ)\ncrossentropy(sum, ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask; ϵ)\n\nFlux.crossentropy with an extra sequence mask for masking out non-needed token loss. y is the labels. By default  it take the mean by dividing the number of valid tokens. This can be change to simply sum the valid losses by add  the first argument sum. See also safe_crossentropy\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Flux.Losses.logitcrossentropy-Tuple{AbstractArray, AbstractArray, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Flux.Losses.logitcrossentropy","text":"logitcrossentropy(ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask)\nlogitcrossentropy(sum, ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask)\n\nFlux.logitcrossentropy with an extra sequence mask for masking out non-needed token loss. y is the labels. By  default it take the mean by dividing the number of valid tokens. This can be change to simply sum the valid losses  by add the first argument sum. See also safe_logitcrossentropy\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.enable_gpu","page":"API Reference","title":"Transformers.enable_gpu","text":"enable_gpu(t=true)\n\nEnable gpu for todevice, disable with enable_gpu(false). The backend is selected by Flux.gpu_backend!.  Should only be used in user scripts.\n\n\n\n\n\n","category":"function"},{"location":"api_ref/#Transformers.firsttoken-Tuple{Any}","page":"API Reference","title":"Transformers.firsttoken","text":"firsttoken(x)\n\nSlice the first tokens from the hidden states, normally equivalent to x[:, 1, :].\n\nSee also: lengthselect, skipboundarytoken\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.lengthselect-Tuple{AbstractArray, Any}","page":"API Reference","title":"Transformers.lengthselect","text":"lengthselect(x, i)\n\nselectdim on the \"length\" dimension (2 for most array and 1 for integer array).\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.safe_crossentropy-Tuple{AbstractArray, AbstractArray, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Transformers.safe_crossentropy","text":"safe_crossentropy(ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask; ϵ)\nsafe_crossentropy(sum, ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask; ϵ)\n\ncrossentropy. If the label y is an integer array, then it would also call maximum on the label to  make sure no label number is large then the first dimension of ŷ. See also unsafe_crossentropy.\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.safe_logitcrossentropy-Tuple{AbstractArray, AbstractArray, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Transformers.safe_logitcrossentropy","text":"safe_logitcrossentropy(ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask)\nsafe_logitcrossentropy(sum, ŷ::AbstractArray, y::AbstractArray, m::AbstractSeqMask)\n\nlogitcrossentropy. If the label y is an integer array, then it would also call maximum on the label to  make sure no label number is large then the first dimension of ŷ. See also unsafe_logitcrossentropy.\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.skipboundarytoken-Tuple{Any}","page":"API Reference","title":"Transformers.skipboundarytoken","text":"skipboundarytoken(x; first=1, last=1)\n\nSelect (lengthselect) the non-boundary tokens from the hidden states, normally equivalent  to x[:, begin+first:end-last, :].\n\nSee also: lengthselect\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.skipfirsttoken-Tuple{Any}","page":"API Reference","title":"Transformers.skipfirsttoken","text":"skipfirsttoken(x)\n\nSlice the non-first tokens from the hidden states, normally equivalent to x[:, 2:end, :].\n\nSee also: lengthselect, skipboundarytoken, skiplasttoken\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.skiplasttoken-Tuple{Any}","page":"API Reference","title":"Transformers.skiplasttoken","text":"skiplasttoken(x)\n\nSlice the non-last tokens from the hidden states, normally equivalent to x[:, 1:end-1, :].\n\nSee also: lengthselect, skipboundarytoken, skipfirsttoken\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.todevice-Tuple","page":"API Reference","title":"Transformers.todevice","text":"todevice(x)\n\nMove data to device, only when gpu is enable with enable_gpu, basically equal Flux.gpu. Otherwise just Flux.cpu.\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.togpudevice-Tuple","page":"API Reference","title":"Transformers.togpudevice","text":"togpudevice(x)\n\nMove data to gpu device, backend selected by Flux.gpu_backend!.\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.unsafe_crossentropy-Tuple{AbstractArray, AbstractArray{<:Integer}, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Transformers.unsafe_crossentropy","text":"unsafe_crossentropy(ŷ::AbstractArray, y::AbstractArray{<:Integer}, m::AbstractSeqMask; ϵ)\nunsafe_crossentropy(sum, ŷ::AbstractArray, y::AbstractArray{<:Integer}, m::AbstractSeqMask; ϵ)\n\nCompute crossentropy with integer labels. The prefix \"unsafe\" means that if y contain any number larger  than the first dimension of ŷ, the behavior is undefined. See also safe_crossentropy.\n\n\n\n\n\n","category":"method"},{"location":"api_ref/#Transformers.unsafe_logitcrossentropy-Tuple{AbstractArray, AbstractArray{<:Integer}, NeuralAttentionlib.AbstractMask{D, NeuralAttentionlib.SEQUENCE} where D}","page":"API Reference","title":"Transformers.unsafe_logitcrossentropy","text":"unsafe_logitcrossentropy(ŷ::AbstractArray, y::AbstractArray{<:Integer}, m::AbstractSeqMask)\nunsafe_logitcrossentropy(sum, ŷ::AbstractArray, y::AbstractArray{<:Integer}, m::AbstractSeqMask)\n\nCompute logitcrossentropy with integer labels. The prefix \"unsafe\" means that if y contain any number  larger than the first dimension of ŷ, the behavior is undefined. See also safe_logitcrossentropy.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Transformers.Datasets-(not-complete)","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"Functions for loading some common Datasets","category":"page"},{"location":"datasets/#Provide-datasets","page":"Transformers.Datasets (not complete)","title":"Provide datasets","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"WMT\nWMT14 (by Google Brain)\nIWSLT\nIWSLT 2016\nen <=> de\nen <=> cs\nen <=> fr\nen <=> ar\nGLUE\nCoLA\nDiagnostic\nGLUE\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSNLI\nSST\nSTS\nWNLI","category":"page"},{"location":"datasets/#example","page":"Transformers.Datasets (not complete)","title":"example","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"using Transformers.Datasets\nusing Transformers.Datasets.GLUE\n\ntask = GLUE.QNLI()\ndatas = dataset(Train, task)\nget_batch(datas, 4)","category":"page"},{"location":"datasets/#API-reference","page":"Transformers.Datasets (not complete)","title":"API reference","text":"","category":"section"},{"location":"datasets/","page":"Transformers.Datasets (not complete)","title":"Transformers.Datasets (not complete)","text":"Modules=[Transformers.Datasets]\nOrder = [:type, :function, :macro]","category":"page"}]
}
