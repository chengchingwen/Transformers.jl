<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Interface · Transformers.jl</title><meta name="title" content="User Interface · Transformers.jl"/><meta property="og:title" content="User Interface · Transformers.jl"/><meta property="twitter:title" content="User Interface · Transformers.jl"/><meta name="description" content="Documentation for Transformers.jl."/><meta property="og:description" content="Documentation for Transformers.jl."/><meta property="twitter:description" content="Documentation for Transformers.jl."/><meta property="og:url" content="https://chengchingwen.github.io/Transformers.jl/huggingface/"/><meta property="twitter:url" content="https://chengchingwen.github.io/Transformers.jl/huggingface/"/><link rel="canonical" href="https://chengchingwen.github.io/Transformers.jl/huggingface/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Transformers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getstarted/">Get Started</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../layers/">Layers</a></li><li><a class="tocitem" href="../textencoders/">TextEncoders</a></li><li><span class="tocitem">HuggingFace</span><ul><li class="is-active"><a class="tocitem" href>User Interface</a><ul class="internal"><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../huggingface_dev/">Add New Models</a></li></ul></li><li><a class="tocitem" href="../api_ref/">API Reference</a></li><li><a class="tocitem" href="../changelog/">ChangeLogs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">HuggingFace</a></li><li class="is-active"><a href>User Interface</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>User Interface</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/huggingface.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers.HuggingFace"><a class="docs-heading-anchor" href="#Transformers.HuggingFace">Transformers.HuggingFace</a><a id="Transformers.HuggingFace-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.HuggingFace" title="Permalink"></a></h1><p>Module for loading pre-trained model from HuggingFace.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>We provide a set of API to download &amp; load a pretrain model from huggingface hub. This is mostly manually done, so we  only have a small set of available models. The most practical way to check if a model is available in Transformers  is to run the <a href="https://github.com/chengchingwen/Transformers.jl/tree/master/example/HuggingFaceValidation"><code>HuggingFaceValidation</code> code in the example folder</a>,  which use <code>PyCall.jl</code> to load the model in both Python and Julia. Open issues/PRs if you find a model you want is  not supported here.</p></div></div><p>There are basically 3 main api for loading the model, <a href="#Transformers.HuggingFace.load_config-Tuple{Any}"><code>HuggingFace.load_config</code></a>, <a href="#Transformers.HuggingFace.load_tokenizer"><code>HuggingFace.load_tokenizer</code></a>, <a href="#Transformers.HuggingFace.load_model"><code>HuggingFace.load_model</code></a>. These are the underlying function of the <a href="#Transformers.HuggingFace.@hgf_str-Tuple{Any}"><code>HuggingFace.@hgf_str</code></a> macro. You can get a better control of the loading process.</p><p>We can load a specific config of a specific model, no matter it&#39;s actually supported by Transformers.jl.</p><pre><code class="language-julia-repl hljs">julia&gt; load_config(&quot;google/pegasus-xsum&quot;)
Transformers.HuggingFace.HGFConfig{:pegasus, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}} with 45 entries:
  :use_cache                       =&gt; true
  :d_model                         =&gt; 1024
  :scale_embedding                 =&gt; true
  :add_bias_logits                 =&gt; false
  :static_position_embeddings      =&gt; true
  :encoder_attention_heads         =&gt; 16
  :num_hidden_layers               =&gt; 16
  :encoder_layerdrop               =&gt; 0
  :num_beams                       =&gt; 8
  :max_position_embeddings         =&gt; 512
  :model_type                      =&gt; &quot;pegasus&quot;
  ⋮                                =&gt; ⋮
</code></pre><p>This would give you all value available in the downloaded configuration file. This might be enough for a some model,  but there are other model that use the default value hard coded in their python code.</p><p>Sometime you would want to add/overwrite  some of the value. This can be done be calling <code>HGFConfig(old_config; key_to_update = new_value, ...)</code>. These is used  primary for customizing model loading. For example, you can load a <code>bert-base-cased</code> model for sequence classification  task. However, if you directly load the model:</p><pre><code class="language-julia-repl hljs">julia&gt; bert_model = hgf&quot;bert-base-cased:ForSequenceClassification&quot;;

julia&gt; bert_model.cls.layer
Dense(W = (768, 2), b = true)</code></pre><p>The model is default creating model for 2 class of label. So you would need to load the config and update the field  about number of labels and create the model with the new config:</p><pre><code class="language-julia-repl hljs">julia&gt; bertcfg = load_config(&quot;bert-base-cased&quot;);

julia&gt; bertcfg.num_labels
2

julia&gt; mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);

julia&gt; mycfg.num_labels
3

julia&gt; _bert_model = load_model(&quot;bert-base-cased&quot;, :ForSequenceClassification; config = mycfg);

julia&gt; _bert_model.cls.layer
Dense(W = (768, 3), b = true)
</code></pre><p>All config field name follow the same name as huggingface, so you might need to read their document for what  is available. However, not every configuration work in Transformers.jl. It&#39;s better to check <a href="https://github.com/chengchingwen/Transformers.jl/tree/master/src/huggingface/implementation">the source  <code>src/huggingface/implementation</code></a>. All supported models would need to overload the <code>load_model</code> and provided an implementation in Julia to be  workable.</p><p>For the tokenizer, <code>load_tokenizer</code> is basically the same as calling with <code>@hgf_str</code>. Currently providing customized  config doesn&#39;t change much stuff. The tokenizer might also work for unsupported model because some serialize the whole  tokenizer object, but not every model does that or they use something not covered by our implementation.</p><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.@hgf_str-Tuple{Any}" href="#Transformers.HuggingFace.@hgf_str-Tuple{Any}"><code>Transformers.HuggingFace.@hgf_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">`hgf&quot;&lt;model-name&gt;:&lt;item&gt;&quot;`</code></pre><p>Get <code>item</code> from <code>model-name</code>. This will ensure the required data are downloaded. <code>item</code> can be &quot;config&quot;,  &quot;tokenizer&quot;, and model related like &quot;Model&quot;, or &quot;ForMaskedLM&quot;, etc. Use <a href="#Transformers.HuggingFace.get_model_type"><code>get_model_type</code></a> to see what  model/task are supported. If <code>item</code> is omitted, return a <code>Tuple</code> of <code>&lt;model-name&gt;:tokenizer</code> and <code>&lt;model-name&gt;:model</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/HuggingFace.jl#L22-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.HGFConfig" href="#Transformers.HuggingFace.HGFConfig"><code>Transformers.HuggingFace.HGFConfig</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HGFConfig{model_type} &lt;: AbstractDict{Symbol, Any}</code></pre><p>The type for holding the configuration for huggingface model <code>model_type</code>.</p><pre><code class="nohighlight hljs">HGFConfig(base_cfg::HGFConfig; kwargs...)</code></pre><p>Return a new <code>HGFConfig</code> object for the same <code>model_type</code> with fields updated with <code>kwargs</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; bertcfg = load_config(&quot;bert-base-cased&quot;);

julia&gt; bertcfg.num_labels
2

julia&gt; mycfg = HuggingFace.HGFConfig(bertcfg; num_labels = 3);

julia&gt; mycfg.num_labels
3
</code></pre><p><strong>Extended help</strong></p><p>Each <code>HGFConfig</code> has a pre-defined set of type-dependent default field values and some field name aliases. For example,  <code>(cfg::HGFConfig{:gpt2}).hidden_size</code> is an alias of <code>(cfg::HGFConfig{:gpt2}).n_embd</code>. Using <code>propertynames</code>,  <code>hasproperty</code>, <code>getproperty</code>, <code>getindex</code> will access the default field values if the key is not present in the loaded  configuration. On the other hand, using <code>length</code>, <code>keys</code>, <code>haskey</code>, <code>get</code>, <code>iterate</code> will not interact with the  default values (while the name aliases still work).</p><pre><code class="language-julia-repl hljs">julia&gt; fakegpt2cfg = HuggingFace.HGFConfig{:gpt2}((a=3,b=5))
Transformers.HuggingFace.HGFConfig{:gpt2, @NamedTuple{a::Int64, b::Int64}, Nothing} with 2 entries:
  :a =&gt; 3
  :b =&gt; 5

julia&gt; myfakegpt2cfg = HuggingFace.HGFConfig(fakegpt2cfg; hidden_size = 7)
Transformers.HuggingFace.HGFConfig{:gpt2, @NamedTuple{a::Int64, b::Int64}, @NamedTuple{n_embd::Int64}} with 3 entries:
  :a      =&gt; 3
  :b      =&gt; 5
  :n_embd =&gt; 7

julia&gt; myfakegpt2cfg[:hidden_size] == myfakegpt2cfg.hidden_size == myfakegpt2cfg.n_embd
true

julia&gt; myfakegpt2cfg.n_layer
12

julia&gt; get(myfakegpt2cfg, :n_layer, &quot;NOT_FOUND&quot;)
&quot;NOT_FOUND&quot;
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/configs/config.jl#L152-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.get_model_type" href="#Transformers.HuggingFace.get_model_type"><code>Transformers.HuggingFace.get_model_type</code></a> — <span class="docstring-category">Function</span></header><section><div><p><code>get_model_type(model_type)</code></p><p>See the list of supported model type of given model. For example, use <code>get_mdoel_type(:gpt2)</code> to see all model/task  that <code>gpt2</code> support. The <code>keys</code> of the returned <code>NamedTuple</code> are all possible task which can be used in  <a href="#Transformers.HuggingFace.load_model"><code>load_model</code></a> or <a href="#Transformers.HuggingFace.@hgf_str-Tuple{Any}"><code>@hgf_str</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; HuggingFace.get_model_type(:gpt2)
(model = Transformers.HuggingFace.HGFGPT2Model, lmheadmodel = Transformers.HuggingFace.HGFGPT2LMHeadModel)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/models/models.jl#L13-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.get_state_dict" href="#Transformers.HuggingFace.get_state_dict"><code>Transformers.HuggingFace.get_state_dict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_state_dict(model)</code></pre><p>Get the state_dict of the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/models/models.jl#L145-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_config-Tuple{Any}" href="#Transformers.HuggingFace.load_config-Tuple{Any}"><code>Transformers.HuggingFace.load_config</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">load_config(model_name; local_files_only = false, cache = true)</code></pre><p>Load the configuration file of <code>model_name</code> from huggingface hub. By default, this function would check if <code>model_name</code>  exists on huggingface hub, download the configuration file (and cache it if <code>cache</code> is set), and then load and return  the config<code>::HGFConfig</code>. If <code>local_files_only = false</code>, it would check whether the configuration file is up-to-date  and update if not (and thus require network access every time it is called). By setting <code>local_files_only = true</code>, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  <code>HuggingFaceApi.jl</code> package. This function would require the configuration file has a field about the <code>model_type</code>, if  not, use <code>load_config(model_type, HuggingFace.load_config_dict(model_name; local_files_only, cache))</code> with <code>model_type</code>  manually provided.</p><p>See also: <a href="#Transformers.HuggingFace.HGFConfig"><code>HGFConfig</code></a></p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; load_config(&quot;bert-base-cased&quot;)
Transformers.HuggingFace.HGFConfig{:bert, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Nothing} with 19 entries:
  :architectures                =&gt; [&quot;BertForMaskedLM&quot;]
  :attention_probs_dropout_prob =&gt; 0.1
  :gradient_checkpointing       =&gt; false
  :hidden_act                   =&gt; &quot;gelu&quot;
  :hidden_dropout_prob          =&gt; 0.1
  :hidden_size                  =&gt; 768
  :initializer_range            =&gt; 0.02
  :intermediate_size            =&gt; 3072
  :layer_norm_eps               =&gt; 1.0e-12
  :max_position_embeddings      =&gt; 512
  :model_type                   =&gt; &quot;bert&quot;
  :num_attention_heads          =&gt; 12
  :num_hidden_layers            =&gt; 12
  :pad_token_id                 =&gt; 0
  :position_embedding_type      =&gt; &quot;absolute&quot;
  :transformers_version         =&gt; &quot;4.6.0.dev0&quot;
  :type_vocab_size              =&gt; 2
  :use_cache                    =&gt; true
  :vocab_size                   =&gt; 28996
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/configs/auto.jl#L1-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_config-Tuple{Union{Symbol, Val}, Any}" href="#Transformers.HuggingFace.load_config-Tuple{Union{Symbol, Val}, Any}"><code>Transformers.HuggingFace.load_config</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">load_config(model_type, cfg)</code></pre><p>Load <code>cfg</code> as <code>model_type</code>. This is used for manually load a config when <code>model_type</code> is not specified in the config.  <code>model_type</code> is a <code>Symbol</code> of the model type like <code>:bert</code>, <code>:gpt2</code>, <code>:t5</code>, etc.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/configs/auto.jl#L61-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_hgf_pretrained-Tuple{Any}" href="#Transformers.HuggingFace.load_hgf_pretrained-Tuple{Any}"><code>Transformers.HuggingFace.load_hgf_pretrained</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>load_hgf_pretrained(name)</code></p><p>The underlying function of <a href="#Transformers.HuggingFace.@hgf_str-Tuple{Any}"><code>@hgf_str</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/HuggingFace.jl#L33-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_model" href="#Transformers.HuggingFace.load_model"><code>Transformers.HuggingFace.load_model</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">load_model([model_type::Symbol,] model_name, task = :model [, state_dict];
           trainmode = false, config = nothing, local_files_only = false, cache = true)</code></pre><p>Load the model of <code>model_name</code> for <code>task</code>. This function would load the <code>state_dict</code> of <code>model_name</code> and build a new  model according to <code>config</code>, <code>task</code>, and the <code>state_dict</code>. <code>local_files_only</code> and <code>cache</code> kwargs would be pass directly  to both <a href="#Transformers.HuggingFace.load_state_dict-Tuple{Any}"><code>load_state_dict</code></a> and <a href="#Transformers.HuggingFace.load_config-Tuple{Any}"><code>load_config</code></a> if not provided. This function would require the  configuration file has a field about the <code>model_type</code>, if not, use <code>load_model(model_type, model_name, task; kwargs...)</code>  with <code>model_type</code> manually provided. <code>trainmode = false</code> would disable all dropouts. The <code>state_dict</code> can be directly  provided, this is used when you want to create a new model with the <code>state_dict</code> in hand. Use <a href="#Transformers.HuggingFace.get_model_type"><code>get_model_type</code></a>  to see what <code>task</code> is available.</p><p>See also: <a href="#Transformers.HuggingFace.get_model_type"><code>get_model_type</code></a>, <a href="#Transformers.HuggingFace.load_state_dict-Tuple{Any}"><code>load_state_dict</code></a>, <a href="#Transformers.HuggingFace.load_config-Tuple{Any}"><code>load_config</code></a>, <a href="#Transformers.HuggingFace.HGFConfig"><code>HGFConfig</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/models/models.jl#L129-L142">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_model-Tuple{Type, Any}" href="#Transformers.HuggingFace.load_model-Tuple{Type, Any}"><code>Transformers.HuggingFace.load_model</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">load_model(::Type{T}, config, state_dict = OrderedDict())</code></pre><p>Create a new model of <code>T</code> according to <code>config</code> and <code>state_dict</code>. missing parameter would initialized according  to <code>config</code>. Set the <code>JULIA_DEBUG=Transformers</code> environment variable to see what parameters are missing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/models/models.jl#L152-L157">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_state_dict-Tuple{Any}" href="#Transformers.HuggingFace.load_state_dict-Tuple{Any}"><code>Transformers.HuggingFace.load_state_dict</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>load_state_dict(model_name; local_files_only = false, force_format = :auto, cache = true)</code></p><p>Load the <code>state_dict</code> from the given <code>model_name</code> from huggingface hub. By default, this function would check if  <code>model_name</code> exists on huggingface hub, download the model file (and cache it if <code>cache</code> is set), and then load  and return the <code>state_dict</code>. If <code>local_files_only = false</code>, it would check whether the model file is up-to-date and  update if not (and thus require network access every time it is called). By setting <code>local_files_only = true</code>, it  would try to find the files from the cache directly and error out if not found. For managing the caches, see the  <code>HuggingFaceApi.jl</code> package. If <code>force_format</code> is <code>:auto</code> it will automatically selects the format from which the  weights will be loaded. If <code>force_format</code> is <code>:pickle</code> or <code>:safetensor</code>, it will prefer relevant file.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/weight.jl#L6-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.load_tokenizer" href="#Transformers.HuggingFace.load_tokenizer"><code>Transformers.HuggingFace.load_tokenizer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">load_tokenizer(model_name; config = nothing, local_files_only = false, cache = true)</code></pre><p>Load the text encoder of <code>model_name</code> from huggingface hub. By default, this function would check if <code>model_name</code>  exists on huggingface hub, download all required files for this text encoder (and cache these files if <code>cache</code> is  set), and then load and return the text encoder. If <code>local_files_only = false</code>, it would check whether all cached  files are up-to-date and update if not (and thus require network access every time it is called). By setting  <code>local_files_only = true</code>, it would try to find the files from the cache directly and error out if not found.  For managing the caches, see the <code>HuggingFaceApi.jl</code> package.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; load_tokenizer(&quot;t5-small&quot;)
T5TextEncoder(
├─ TextTokenizer(MatchTokenization(PrecompiledNormalizer(WordReplaceNormalizer(UnigramTokenization(EachSplitTokenization(splitter = isspace), unigram = Unigram(vocab_size = 32100, unk = &lt;unk&gt;)), pattern = r&quot;^(?!▁)(.*)$&quot; =&gt; s&quot;▁&quot;), precompiled = PrecompiledNorm(...)), 103 patterns)),
├─ vocab = Vocab{String, SizedArray}(size = 32100, unk = &lt;unk&gt;, unki = 3),
├─ endsym = &lt;/s&gt;,
├─ padsym = &lt;pad&gt;,
└─ process = Pipelines:
  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)
  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)
  ╰─ target[(token, segment)] := SequenceTemplate{String}(Input[1]:&lt;type=1&gt; &lt;/s&gt;:&lt;type=1&gt; (Input[2]:&lt;type=1&gt; &lt;/s&gt;:&lt;type=1&gt;)...)(target.token)
  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(nothing))(target.token)
  ╰─ target[token] := TextEncodeBase.trunc_and_pad(nothing, &lt;pad&gt;, tail, tail)(target.token)
  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)
  ╰─ target := (target.token, target.attention_mask)
)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/tokenizer/tokenizer.jl#L122-L152">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.save_config-Tuple{Any, Any}" href="#Transformers.HuggingFace.save_config-Tuple{Any, Any}"><code>Transformers.HuggingFace.save_config</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">save_config(model_name, config; path = pwd(), config_name = CONFIG_NAME, force = false)</code></pre><p>Save the <code>config</code> at <code>&lt;path&gt;/&lt;model_name&gt;/&lt;config_name&gt;</code>. This would error out if the file already exists but <code>force</code>  not set.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/configs/auto.jl#L78-L83">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.save_model-Tuple{Any, Any}" href="#Transformers.HuggingFace.save_model-Tuple{Any, Any}"><code>Transformers.HuggingFace.save_model</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>save_model(model_name, model; path = pwd(), weight_name = PYTORCH_WEIGHTS_NAME, force = false)</code></p><p>save the <code>model</code> state<em>dict at `&lt;path&gt;/&lt;model</em>name&gt;/&lt;weight_name&gt;<code>. This would error out if the file already exists  but</code>force` not set.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/models/models.jl#L97-L102">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Transformers.HuggingFace.state_dict_to_namedtuple-Tuple{Any}" href="#Transformers.HuggingFace.state_dict_to_namedtuple-Tuple{Any}"><code>Transformers.HuggingFace.state_dict_to_namedtuple</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>state_dict_to_namedtuple(state_dict)</code></p><p>convert state_dict into nested <code>NamedTuple</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/fe41df525c7da5839d1078ef1524f4503ce59d9e/src/huggingface/weight.jl#L143-L147">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../textencoders/">« TextEncoders</a><a class="docs-footer-nextpage" href="../huggingface_dev/">Add New Models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 27 June 2024 19:09">Thursday 27 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
