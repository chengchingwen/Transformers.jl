<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>ChangeLogs · Transformers.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://chengchingwen.github.io/Transformers.jl/changelog/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Transformers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getstarted/">Get Started</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../layers/">Layers</a></li><li><a class="tocitem" href="../textencoders/">TextEncoders</a></li><li><a class="tocitem" href="../huggingface/">HuggingFace</a></li><li class="is-active"><a class="tocitem" href>ChangeLogs</a><ul class="internal"><li><a class="tocitem" href="#Transformers.Pretrain"><span>Transformers.Pretrain</span></a></li><li><a class="tocitem" href="#Transformers.Stacks"><span>Transformers.Stacks</span></a></li><li><a class="tocitem" href="#Transformers.Basic"><span>Transformers.Basic</span></a></li><li><a class="tocitem" href="#Transformers.Layers-(new)"><span>Transformers.Layers (new)</span></a></li><li><a class="tocitem" href="#Transformers.TextEncoders-(new)"><span>Transformers.TextEncoders (new)</span></a></li><li><a class="tocitem" href="#Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain"><span>Transformers.BidirectionalEncoder / Transformers.GenerativePreTrain</span></a></li><li><a class="tocitem" href="#Transformers.HuggingFace"><span>Transformers.HuggingFace</span></a></li><li><a class="tocitem" href="#Behavior-Changes"><span>Behavior Changes</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>ChangeLogs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>ChangeLogs</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/CHANGELOG.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ChangeLogs-(from-0.1.x-to-0.2.0)"><a class="docs-heading-anchor" href="#ChangeLogs-(from-0.1.x-to-0.2.0)">ChangeLogs (from 0.1.x to 0.2.0)</a><a id="ChangeLogs-(from-0.1.x-to-0.2.0)-1"></a><a class="docs-heading-anchor-permalink" href="#ChangeLogs-(from-0.1.x-to-0.2.0)" title="Permalink"></a></h1><p>v0.2 is a rewrite of the whole package. Most layers and API in 0.1 is removed or changed. Some of them are replaced  with new one. The basic policy is, if a functionality is achievable with a well-maintained package easily, or there  isn&#39;t much gain by self-hosting/maintaining it, then we remove the functionality from Transformers.jl.</p><p>Here is list of the changes with brief explanation:</p><h2 id="Transformers.Pretrain"><a class="docs-heading-anchor" href="#Transformers.Pretrain">Transformers.Pretrain</a><a id="Transformers.Pretrain-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Pretrain" title="Permalink"></a></h2><p>The <code>Pretrain</code> module is entirely removed, due to the duplication of functionality v.s. <code>Transformers.HuggingFace</code>.  We do not host the small list of the origin official released pretrained weights anymore. All use that require a  pretrained weight should refer to <code>HuggingFace</code> module. This is a table of the old pretrain name and corresponding  huggingface model name:</p><table><tr><th style="text-align: right">old pretrain name</th><th style="text-align: right">corresponding huggingface model name</th></tr><tr><td style="text-align: right"><code>cased_L-12_H-768_A-12</code></td><td style="text-align: right"><code>bert-base-cased</code></td></tr><tr><td style="text-align: right"><code>uncased_L-12_H-768_A-12</code></td><td style="text-align: right"><code>bert-base-uncased</code></td></tr><tr><td style="text-align: right"><code>chinese_L-12_H-768_A-12</code></td><td style="text-align: right"><code>bert-base-chinese</code></td></tr><tr><td style="text-align: right"><code>multi_cased_L-12_H-768_A-12</code></td><td style="text-align: right"><code>bert-base-multilingual-cased</code></td></tr><tr><td style="text-align: right"><code>multilingual_L-12_H-768_A-12</code></td><td style="text-align: right"><code>bert-base-multilingual-uncased</code></td></tr><tr><td style="text-align: right"><code>cased_L-24_H-1024_A-16</code></td><td style="text-align: right"><code>bert-large-cased</code></td></tr><tr><td style="text-align: right"><code>uncased_L-24_H-1024_A-16</code></td><td style="text-align: right"><code>bert-large-uncased</code></td></tr><tr><td style="text-align: right"><code>wwm_cased_L-24_H-1024_A-16</code></td><td style="text-align: right"><code>bert-large-cased-whole-word-masking</code></td></tr><tr><td style="text-align: right"><code>wwm_uncased_L-24_H-1024_A-16</code></td><td style="text-align: right"><code>bert-large-uncased-whole-word-masking</code></td></tr><tr><td style="text-align: right"><code>scibert_scivocab_cased</code></td><td style="text-align: right"><code>allenai/scibert_scivocab_cased</code></td></tr><tr><td style="text-align: right"><code>scibert_scivocab_uncased</code></td><td style="text-align: right"><code>allenai/scibert_scivocab_uncased</code></td></tr><tr><td style="text-align: right"><code>scibert_basevocab_cased</code></td><td style="text-align: right">N/A</td></tr><tr><td style="text-align: right"><code>scibert_basevocab_uncased</code></td><td style="text-align: right">N/A</td></tr><tr><td style="text-align: right"><code>OpenAIftlm</code></td><td style="text-align: right"><code>openai-gpt</code></td></tr></table><h2 id="Transformers.Stacks"><a class="docs-heading-anchor" href="#Transformers.Stacks">Transformers.Stacks</a><a id="Transformers.Stacks-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Stacks" title="Permalink"></a></h2><p>The <code>Stacks</code> module is entirely removed. <code>Stacks</code> provide a small DSL for creating nontrivial <code>Chain</code> of layers.  However, the DSL isn&#39;t intuitive enough and it also doesn&#39;t seems worth maintaining a DSL. We don&#39;t provide  direct replacement for this, but for the specific use case of building transformer models, we have a few new  constructors/layers in <code>Transformers.Layers</code>.</p><h2 id="Transformers.Basic"><a class="docs-heading-anchor" href="#Transformers.Basic">Transformers.Basic</a><a id="Transformers.Basic-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Basic" title="Permalink"></a></h2><p>The <code>Basic</code> module is now destructed and most of the elements in <code>Basic</code> is separated to other module/package.</p><ol><li><p><code>Transformer</code> and <code>TransformerDecoder</code>: The <code>Transformer</code>/<code>TransformerDecoder</code> layer is replaced with the new  implementation in <code>Layers</code> (the <code>Layers.TransformerBlock</code>, <code>Layers.TransformerDecoderBlock</code>, and friends).</p></li><li><p><code>MultiheadAttention</code>: The implementation of attention operations are move out to  <a href="https://github.com/chengchingwen/NeuralAttentionlib.jl">NeuralAttentionlib</a>. In NeuralAttentionlib, we can use  <code>multihead_qkv_attention</code> to do the same computation. Since most transformer variant only use a modified version  of self or cross attention, we do not provied the <code>MultiheadAttention</code> layer type. One should be able to redefine  the <code>MultiheadAttention</code> layer type with Flux and NeuralAttentionlib easily. For example:</p><pre><code class="language-julia hljs">using Flux, Functors
using NeuralAttentionlib: multihead_qkv_attention, CausalMask

struct MultiheadAttention{Q,K,V,O}
    head::Int
    future::Bool
    iqproj::Q
    ikproj::K
    ivproj::V
    oproj::O
end
@functor MultiheadAttention (iqproj, ikproj, ivporj, oproj)
MultiheadAttention(head, hidden_size, head_size; future = true) =
    MultiheadAttention(head, future,
        Dense(hidden_size, head_size * head),
        Dense(hidden_size, head_size * head),
        Dense(hidden_size, head_size * head),
        Dense(head_size * head, hidden_size),
    )

(mha::MultiheadAttention)(q, k, v) = mha.oproj(multihead_qkv_attention(mha.head,
    mha.iqproj(q), mha.ikproj(k), mha.ivproj(v), mha.future ? nothing : CausalMask()))</code></pre></li><li><p><code>TransformerModel</code>: This is just a Flux layer with embedding layer, transformer layer, and classifier layer   bundle together. One can define this easily with Flux/Functors API, thus removed.</p></li><li><p><code>Positionwise</code>, <code>PwFFN</code>, and <code>@toNd</code>: This was originally designed for applying <code>Flux.Dense</code> on 3-dim arrays,  but since <code>Flux.Dense</code> support multi-dim input now. This isn&#39;t needed and thus removed.</p></li><li><p><code>EmbeddingDecoder</code>: Replaced with <code>Layers.EmbedDecoder</code>. Name change and support extra trainable <code>bias</code> parameter.</p></li><li><p><code>PositionEmbedding</code>: This is replace with <code>Layers.SinCosPositionEmbed</code> and <code>Layers.FixedLenPositionEmbed</code> for  the old <code>trainable</code> keyword argument setting.</p></li><li><p><code>crossentropy</code> with masking: We extend <code>Flux.logitcrossentropy</code> and <code>Flux.crossentropy</code> with 3-args  input (the prediction, label, and mask) and 4-args input (<code>sum</code> or <code>mean</code>, prediciton, label, and mask).</p></li><li><p><code>kldivergence</code>: In our use case (i.e. training language model), this is equivalent to cross-entropy, thus removed.</p></li><li><p><code>logcrossentropy</code>/<code>logkldivergence</code>: This is a fault design. Originally I would put a <code>logsoftmax</code> at the head of  the prediction head. However, that is not only unnecessary but also increasing the amount of memory needed.  One should use <code>Flux.logitcrossentropy</code> without the <code>logsoftmax</code> directly.</p></li><li><p><code>Vocabulary</code>: Replaced with <code>TextEncodeBase.Vocab</code>.</p></li><li><p><code>with_firsthead_tail</code>/<code>segment_and_concat</code>/<code>concat</code>: These can be implemented with <code>TextEncodeBase.SequenceTemplate</code>   and friends thus removed.</p></li><li><p><code>getmask</code>: The attention mask functionality is moved to NeuralAttentionlib. Manually construct attention mask   should use constructor in <code>NeuralAttentionlib.Masks</code>.</p></li></ol><h2 id="Transformers.Layers-(new)"><a class="docs-heading-anchor" href="#Transformers.Layers-(new)">Transformers.Layers (new)</a><a id="Transformers.Layers-(new)-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Layers-(new)" title="Permalink"></a></h2><p>The <code>Layers</code> module is a new module introduced in v0.2.0. It provide a set layer types for construct transformer  model variants.</p><h2 id="Transformers.TextEncoders-(new)"><a class="docs-heading-anchor" href="#Transformers.TextEncoders-(new)">Transformers.TextEncoders (new)</a><a id="Transformers.TextEncoders-(new)-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.TextEncoders-(new)" title="Permalink"></a></h2><p>The <code>TextEncoders</code> module is a new module introduced in v0.2.0. Basically all old functionality about text preprocessing  are moved to this module, including <code>WordPiece</code>, <code>Unigram</code>, <code>BertTextEncoder</code>, <code>GPT2TextEncoder</code>, etc.</p><h2 id="Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain"><a class="docs-heading-anchor" href="#Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain">Transformers.BidirectionalEncoder / Transformers.GenerativePreTrain</a><a id="Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.BidirectionalEncoder-/-Transformers.GenerativePreTrain" title="Permalink"></a></h2><p>These modules are removed since we are switching to the <code>Transformers.HuggingFace</code> for the pretrained model. The text  encoder are moved to <code>Transformers.TextEncoders</code>. Weight loading and conversion functionality are removed. If you  need that, use the tools that huggingface transformers python package provided and make sure the model can be loaded  with pytorch. Then we can use the weight in pytorch format.</p><h2 id="Transformers.HuggingFace"><a class="docs-heading-anchor" href="#Transformers.HuggingFace">Transformers.HuggingFace</a><a id="Transformers.HuggingFace-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.HuggingFace" title="Permalink"></a></h2><p>The changes in <code>Transformers.HuggingFace</code> are mainly about the configurations and models. The tokenizer/textencoder part  are mostly the same, except the process functions.</p><h3 id="Configuration"><a class="docs-heading-anchor" href="#Configuration">Configuration</a><a id="Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration" title="Permalink"></a></h3><p>For the configuration, the loading mechanism is changed. In previous version, each model type need to define a specific  <code>HGF&lt;XXModelType&gt;Config</code> struct where <code>XXModelType</code> is the model type name. The reason for that is, for some reason,  huggingface transformers doesn&#39;t serialize all the configuration values into the file, but rely on their constructor  with pre-defined default values instead. As a result, some model only need the configuration file, while some need the  python code for the defaults as well. The hgf config struct was more like a interal data carrier. You usually  won&#39;t (and actually can&#39;t) manipulate the model with it.</p><p>In v0.2, we tried to make the process for adding model more automatic, and enable the ability to build model with  different configurations. The struct for holding the configuration is now changed to a parametric struct depending  on a <code>Symbol</code> parameter specifying the model type (e.g. <code>HGFConfig{:bert}</code>). With this, the specific  <code>HGF&lt;XXModelType&gt;config</code> can be constructed on the fly. The <code>HGFConfig</code> has 2 field, one for storing the read-only  deserialized object loaded from the configuration file, and another for the overwritten values. This should turn the  config struct into a user level interface.</p><h3 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h3><p>For the model part, the main change is that we do not make a 1-1 mapping between the python model/layer class and our  julia layer struct. When one wants to add a new model type, there are actually 2 things need to be done. One is  defining a model forward method that can do the same computation as the python model, and another is defining a  mapping between the python model and the julia model (so that the model parameters/weights can be transferred between  2 language). In the previous version, we chose to make a 1-1 mapping between the model, so that the parameters/weights  loading process can be fully automatic. However, for some reason, huggingface transformers is not reusing their  attention or transformer implementation for each model type. Which means for different model type, even if they are  actually doing the same computation (i.e. the computation graph is the same), the model layout can be different  (e.g. consider the differences between <code>Chain(Chain(dense1, dense2), dense3)</code> and <code>Chain(dense1, dense2, dense3)</code>).  As a result, these make implementing the model forward method a real pain, and also it&#39;s hard to apply optimizations.</p><p>We noticed that the model forward method is more important and difficult than the model mapping. On the other hand,  though manually defining model mapping is tedious, it&#39;s less prone to go wrong. So instead of making a 1-1 mapping for  fully automatic model loading, we choose to reduce the work needed for forward method. In v0.2, the attention  implementation is switched to NeuralAttentionlib&#39;s modulated implementation and we build all internal layers with layer  from <code>Transformers.Layers</code>. As a result, layers like <code>FakeTH&lt;XXLayer&gt;</code> or <code>HGF&lt;XXModelType&gt;Attention/MLP/...</code> are  removed, only the outer-most types remain (e.g. <code>HGFBertModel</code>, <code>HGFGPT2LMHeadModel</code>...).</p><p>Since we want to make it possible to finetune a pretrained model on new dataset/task easily, the model loading would  be a combination of initialization and parameters/weights loading. In normal Flux workflow, you would build a complete  new model and then inplace load the parameter/weight values into the specific layers/arrays in the model. In v0.2, we  combine the 2 step into one <code>load_model</code> function, which take the model type, configuration, and a state dictionary (  the term comes from PyTorch, which is a <code>OrderedDict</code> of variable names to weights). <code>load_model</code> would either  lookup variable from the state dictionary, or initialize with configuration, recursively. As a result,  <code>load_model!</code> is removed.</p><h2 id="Behavior-Changes"><a class="docs-heading-anchor" href="#Behavior-Changes">Behavior Changes</a><a id="Behavior-Changes-1"></a><a class="docs-heading-anchor-permalink" href="#Behavior-Changes" title="Permalink"></a></h2><ul><li>All text encoder (including <code>HuggingFace</code> one) process function returned <code>NamedTuple</code>: Some field name changed,  <code>tok</code> =&gt; <code>token</code>, <code>mask</code> =&gt; <code>attention_mask</code>.</li><li>Most layer/model from Transformers.jl would be taking and returning <code>NamedTuple</code>.</li><li>For <code>HuggingFace</code> model: All input is basically <code>NamedTuple</code>. The returned <code>NamedTuple</code> field name from the forward  method is also changed.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../huggingface/">« HuggingFace</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 26 February 2023 08:32">Sunday 26 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
