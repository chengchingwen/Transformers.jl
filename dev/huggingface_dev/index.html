<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Add New Models · Transformers.jl</title><meta name="title" content="Add New Models · Transformers.jl"/><meta property="og:title" content="Add New Models · Transformers.jl"/><meta property="twitter:title" content="Add New Models · Transformers.jl"/><meta name="description" content="Documentation for Transformers.jl."/><meta property="og:description" content="Documentation for Transformers.jl."/><meta property="twitter:description" content="Documentation for Transformers.jl."/><meta property="og:url" content="https://chengchingwen.github.io/Transformers.jl/huggingface_dev/"/><meta property="twitter:url" content="https://chengchingwen.github.io/Transformers.jl/huggingface_dev/"/><link rel="canonical" href="https://chengchingwen.github.io/Transformers.jl/huggingface_dev/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Transformers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getstarted/">Get Started</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../layers/">Layers</a></li><li><a class="tocitem" href="../textencoders/">TextEncoders</a></li><li><span class="tocitem">HuggingFace</span><ul><li><a class="tocitem" href="../huggingface/">User Interface</a></li><li class="is-active"><a class="tocitem" href>Add New Models</a><ul class="internal"><li><a class="tocitem" href="#0.-Find-the-correct-model-type"><span>0. Find the correct model type</span></a></li><li><a class="tocitem" href="#1.-Porting-config"><span>1. Porting config</span></a></li><li><a class="tocitem" href="#2.-Check-the-tokenizer"><span>2. Check the tokenizer</span></a></li><li><a class="tocitem" href="#3.-Porting-model"><span>3. Porting model</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../api_ref/">API Reference</a></li><li><a class="tocitem" href="../changelog/">ChangeLogs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">HuggingFace</a></li><li class="is-active"><a href>Add New Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Add New Models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/huggingface_dev.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Adding-New-HuggingFace-Model"><a class="docs-heading-anchor" href="#Adding-New-HuggingFace-Model">Adding New HuggingFace Model</a><a id="Adding-New-HuggingFace-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-New-HuggingFace-Model" title="Permalink"></a></h1><p>This is a record of what I do to port the bloom model. This log serves as an tutorial for adding new hugginface model to Transformers.jl.</p><h2 id="0.-Find-the-correct-model-type"><a class="docs-heading-anchor" href="#0.-Find-the-correct-model-type">0. Find the correct model type</a><a id="0.-Find-the-correct-model-type-1"></a><a class="docs-heading-anchor-permalink" href="#0.-Find-the-correct-model-type" title="Permalink"></a></h2><p>Our target is to port the <code>bigscience/bloom</code> model. We first find the model on the huggingface hub (<code>https://huggingface.co/bigscience/bloom/tree/main</code>). At the &quot;Files and versions&quot;, we can find the <code>config.json</code> and in the <code>model_type</code> field we can see the model type is <code>&quot;bloom&quot;</code>. We can find another model of same model type but with smaller model size, such as <code>bigscience/bloom-560m</code>, for testing.</p><p>Once we get the model type, we can locate the corresponding python code in huggingface transformers github under the <code>&quot;src/transformers/models&quot;</code> folder (<code>https://github.com/huggingface/transformers/tree/main/src/transformers/models</code>). The code would usually be put in a folder using model type as the name, such as <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/bloom"><code>&quot;src/transformers/models/bloom&quot;</code></a>. On the other hand, our code will be put inside <code>&quot;src/huggingface/implementation&quot;</code> and also use model type as the folder name (<code>https://github.com/chengchingwen/Transformers.jl/tree/master/src/huggingface/implementation/bloom</code>).</p><h2 id="1.-Porting-config"><a class="docs-heading-anchor" href="#1.-Porting-config">1. Porting config</a><a id="1.-Porting-config-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Porting-config" title="Permalink"></a></h2><p>The first thing we need to port is the config object. Although we are actually able to load the config without defining the config object in Julia, there are some default values hardcoded in the python code and cannot be found in the <code>config.json</code>. Thus we copy the hardcoded default values and create our own config object in julia.</p><p>Their config object is defined in <code>configuration_&lt;model_type&gt;.py</code> (<code>https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/configuration_bloom.py</code>) as <code>class &lt;model_type&gt;Config(PretrainedConfig)</code> (<code>BloomConfig</code>). The default values is either defined as the <code>__init__</code> default arguments or assigned in the function body. We copy the default values to our julia definition. Generally, there are 3 part we need to check in the python class:</p><ol><li>variable <a href="https://github.com/huggingface/transformers/blob/29e7a1e1834f331a4916853ecd58549ed78235d6/src/transformers/models/bloom/configuration_bloom.py#L106"><code>model_type</code></a></li><li>variable <a href="https://github.com/huggingface/transformers/blob/29e7a1e1834f331a4916853ecd58549ed78235d6/src/transformers/models/bloom/configuration_bloom.py#L108-L111"><code>attribute_map</code></a>: A dictionary that map alias property name to the real property name.</li><li>method <a href="https://github.com/huggingface/transformers/blob/29e7a1e1834f331a4916853ecd58549ed78235d6/src/transformers/models/bloom/configuration_bloom.py#L113-L149"><code>__init__</code></a></li></ol><p>In <code>&quot;src/huggingface/implementation/bloom/&quot;</code>, we open a file <code>&quot;config.jl&quot;</code> and define the config object:</p><pre><code class="language-julia hljs">@hgfcfg :bloom struct HGFBloomConfig
    vocab_size::Int = 250880
    hidden_size::Int = 64
    [n_layer, num_hidden_layers]::Int = 2
    [n_head, num_attention_heads]::Int = 8
    layer_norm_epsilon::Float64 = 1e-5
    initializer_range::Float64 = 0.02
    use_cache::Bool = true
    bos_token_id::Int = 1
    eos_token_id::Int = 2
    apply_residual_connection_post_layernorm::Bool = false
    hidden_dropout::Float64 = 0.0
    attention_dropout::Float64 = 0.0
    pretraining_tp::Int = 1
    slow_but_exact::Bool = false
    clean_up_tokenization_spaces::Bool = false
end</code></pre><p>The <code>@hgfcfg</code> take two argument: 1. a symbol for the model type (<code>:bloom</code>). 2. a <code>Base.@kwdef</code>-like struct definition, despite the name aliases (e.g. <code>[n_layer, num_hidden_layers]</code> where <code>n_layer</code> is the real field name and <code>num_hidden_layers</code> is an alias), with the default values copy from the python definition. The struct follow the name of <code>HGF&lt;model_type&gt;Config</code>. Notice that the config are not necessary used or implemented in our julia implementation.</p><h2 id="2.-Check-the-tokenizer"><a class="docs-heading-anchor" href="#2.-Check-the-tokenizer">2. Check the tokenizer</a><a id="2.-Check-the-tokenizer-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Check-the-tokenizer" title="Permalink"></a></h2><p>Currently our tokenizer does not support much modification from the loading API. Our loading API should be able to directly load most of the tokenizer. We check the tokenizer by running <a href="https://github.com/chengchingwen/Transformers.jl/tree/master/example/HuggingFaceValidation">our huggingface validation code</a>. The validation code use PyCall to load the huggingface transformers and directly compare the result. The validation code for tokenizer take two argument, the model name and the a corpus. You can take any corpus for the test, but we recommand using the <a href="https://github.com/facebookresearch/XNLI">xnli dataset</a>. We use the devset of xnli and extract all sentences in the dataset as the testing corpus, which is also available with the following <code>Artifacts.toml</code>:</p><pre><code class="language-toml hljs">[xnli_dev]
git-tree-sha1 = &quot;fb42e6f4a4522b30eb09d21786b90617c4792113&quot;
lazy = true

    [[xnli_dev.download]]
    sha256 = &quot;9fa4c2b8ff5194eb3eb9cd63a264a2f1e2b9e24f5d71781d524cfe0f4b268c25&quot;
    url = &quot;https://gist.github.com/chengchingwen/27796c1d39efdae744e5abec94ecfdb6/raw/fb42e6f4a4522b30eb09d21786b90617c4792113.tar.gz&quot;</code></pre><p>If the validation code passed, we are fine. If the tokenizer cannot be loaded, or the code does not pass the validation, then issues should be opened.</p><h2 id="3.-Porting-model"><a class="docs-heading-anchor" href="#3.-Porting-model">3. Porting model</a><a id="3.-Porting-model-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Porting-model" title="Permalink"></a></h2><p>To port the model, we need two things: the model weights and an implementation of the model in Julia. Currently we only support loading model weights stored in pytorch pickle format with <code>Transformers.HuggingFace.load_state_dict</code> and it should be able to load the model weights (that support pytorch) without problems. On the other hand, the implementation of the model need to be done manually.</p><h3 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h3><p>The first thing we do is checking the python implementation source code, in <code>&quot;src/huggingface/implementation/bloom/modeling_&lt;model_type&gt;.py&quot;</code> (<code>https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py</code>). We search for <code>class &lt;model_type&gt;</code> (<code>class Bloom</code>) in the file and we can see there are 10 matches and organized into 3 categories:</p><h4 id="1.-Implementation-Detail-Types-(BloomGelu,-BloomAttention,-BloomMLP,-BloomBlock)"><a class="docs-heading-anchor" href="#1.-Implementation-Detail-Types-(BloomGelu,-BloomAttention,-BloomMLP,-BloomBlock)">1. Implementation Detail Types (<code>BloomGelu</code>, <code>BloomAttention</code>, <code>BloomMLP</code>, <code>BloomBlock</code>)</a><a id="1.-Implementation-Detail-Types-(BloomGelu,-BloomAttention,-BloomMLP,-BloomBlock)-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Implementation-Detail-Types-(BloomGelu,-BloomAttention,-BloomMLP,-BloomBlock)" title="Permalink"></a></h4><p>These types are part of the implementation detail of the model. We won&#39;t directly translate these types into Julia. Instead, we use/implement the julia functions/types that perform the similar operations. For example, we would directly use <code>NNlib.gelu</code> for <code>BloomGelu</code>. Usually the attention (<code>BloomAttention</code>) is the most important part we need to look at because it determine whether the model requires a different implementation. We can see that <code>BloomAttention</code> use an attention variant with alibi position embedding. The attention variants would usually be implemented with <a href="https://github.com/chengchingwen/NeuralAttentionlib.jl">NeuralAttentionlib.jl (NAlib)</a>. If the components for implementing the attention variant are not found in NAlib, open issue/PR in NAlib. Then we would use the components provided from NAlib to implement the attention operation/type.</p><h4 id="2.-Model-Type-(BloomPreTrainedModel,-BloomModel)"><a class="docs-heading-anchor" href="#2.-Model-Type-(BloomPreTrainedModel,-BloomModel)">2. Model Type (<code>BloomPreTrainedModel</code>, <code>BloomModel</code>)</a><a id="2.-Model-Type-(BloomPreTrainedModel,-BloomModel)-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Model-Type-(BloomPreTrainedModel,-BloomModel)" title="Permalink"></a></h4><p>The <code>&lt;model_type&gt;PreTrainedModel</code> (<code>BloomPreTrainedModel</code>) is a python class for inheritance, so we will convert it into a abstract type in julia like (for illustration only):</p><pre><code class="language-julia hljs">abstract type HGFBloomPreTrainedModel &lt;: HGFPreTrainedModel end</code></pre><p>The <code>&quot;HGF&quot;</code> prefix indicate that this type correspond to a python class in huggingface transformers. The <code>&lt;model_type&gt;Model</code> (<code>BloomModel</code>) is the base model for each task-specific model. We&#39;ll translate it into a julia type <code>HGF&lt;model_type&gt;Model</code> (<code>HGFBloomModel</code>). For example, the julia type for bloom model is:</p><pre><code class="language-julia hljs">struct HGFBloomModel{E, D} &lt;: HGFBloomPreTrainedModel
    embed::E
    decoder::D
end
@functor HGFBloomModel

(model::HGFBloomModel)(nt::NamedTuple) = model.decoder(model.embed(nt))</code></pre><p>This specify the fields of bloom, mark it as a layer, and define the interface. The actual computation are carry out by the embedding and decoder (<code>embed::E</code> and <code>decoder::D</code>), so at this point we can&#39;t even tell what computation does bloom perform. This means we can actually directly use the constructor to create a model that is not a &quot;bloom&quot; but has the <code>HGFBloomModel</code> type, and this should usually be avoided. The &quot;real&quot; bloom model is constructed by the <code>Transformers.HuggingFace.load_model</code> api, which would be discussed in the following sections.</p><h4 id="3.-Task-Specific-Types-(BloomForCausalLM,-BloomForSequenceClassification,-BloomForTokenClassification,-BloomForQuestionAnswering)"><a class="docs-heading-anchor" href="#3.-Task-Specific-Types-(BloomForCausalLM,-BloomForSequenceClassification,-BloomForTokenClassification,-BloomForQuestionAnswering)">3. Task Specific Types (<code>BloomForCausalLM</code>, <code>BloomForSequenceClassification</code>, <code>BloomForTokenClassification</code>, <code>BloomForQuestionAnswering</code>)</a><a id="3.-Task-Specific-Types-(BloomForCausalLM,-BloomForSequenceClassification,-BloomForTokenClassification,-BloomForQuestionAnswering)-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Task-Specific-Types-(BloomForCausalLM,-BloomForSequenceClassification,-BloomForTokenClassification,-BloomForQuestionAnswering)" title="Permalink"></a></h4><p>Currently, each task specific class would also be translated into a julia type having the based model (<code>model</code> field) and a classifier (<code>cls</code> field). For example, the <code>HGFBloomForCausalLM</code> can be defined as:</p><pre><code class="language-julia hljs">struct HGFBloomForCausalLM{M, C} &lt;: HGFBloomPreTrainedModel
	model::M
	cls::C
end
@functor HGFBloomForCausalLM

(model::HGFBloomForCausalLM)(nt::NamedTuple) = model.cls(model.model(nt))</code></pre><p>Similar to <code>HGFBloomModel</code>, this definition does not include the computation and require to use <code>Transformers.HuggingFace.load_model</code> for the correct construction.</p><h3 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h3><p>There are basically 3 steps to implement a new model:</p><h4 id="1.-Define-Types"><a class="docs-heading-anchor" href="#1.-Define-Types">1. Define Types</a><a id="1.-Define-Types-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Define-Types" title="Permalink"></a></h4><p>We provide a macro <code>Transformers.HuggingFace.@hgfdef</code> that generate the model types for us. For example, the above Julia types is defined with:</p><pre><code class="language-julia hljs">@hgfdef Bloom (
    Model =&gt; (embed, decoder),
    ForCausalLM,
)</code></pre><p>which would be expanded to:</p><pre><code class="language-julia hljs">const HGFBloomPreTrainedModel = HGFPreTrained{:bloom}

struct HGFBloomModel{EMBED, DECODER} &lt;: HGFPreTrained{:bloom, :model}
    embed::EMBED
    decoder::DECODER
end
@functor HGFBloomModel

@inline function Transformers.HuggingFace.hgf_model_forward(model::HGFBloomModel, nt::NamedTuple)
	return model.decoder(model.embed(nt))
end

struct HGFBloomForCausalLM{MODEL, CLS} &lt;: HGFPreTrained{:bloom, :forcausallm}
    model::MODEL
    cls::CLS
end
@functor HGFBloomForCausalLM

@inline function Transformers.HuggingFace.hgf_model_forward(model::HGFBloomForCausalLM, nt::NamedTuple)
	return model.cls(model.model(nt))
end</code></pre><p>The <code>HGFPreTrained</code> is the real abstract type for our huggingface models. It take two type parameters – <code>HGFPreTrained{model_type, task}</code> (e.g. <code>HGFPreTrained{:bloom, :forcausallm}</code>). This allow use to query the supported model through <code>subtypes</code>. Moreover, we can define behaviors for all model of a specific <code>task</code>. For example, <code>Transformers.HuggingFace.hgf_model_loss(model::HGFPreTrained{M, :forcausallm} where M)</code> return the loss function for all model for causal LM task.</p><p>The <code>@hgfdef</code> macro take 3 arguments: the model type (<code>:bloom</code>), capitalized name (<code>Bloom</code>), and a tuple of tasks. If the model type is omitted, it will use the lowercase of capitalized name. Each task in the tuple is a pair of task name and the forward function body. It collect all <code>getproperty</code> on <code>model</code> in the function body for the field names of the type. If the function body is omitted, it will use <code>(model, cls)</code> as default. If the function body is a tuple of field names, it will convert them into a chain of function call (e.g. <code>(embed, decoder)</code> to <code>model.decoder(model.embed(nt))</code>).</p><h4 id="2.-Overload-Transformers.HuggingFace.load_model"><a class="docs-heading-anchor" href="#2.-Overload-Transformers.HuggingFace.load_model">2. Overload <code>Transformers.HuggingFace.load_model</code></a><a id="2.-Overload-Transformers.HuggingFace.load_model-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Overload-Transformers.HuggingFace.load_model" title="Permalink"></a></h4><p>As mentioned above, the <code>Transformers.HuggingFace.load_model</code> serves as the actual constructor for our huggingface models. <code>load_model</code> is dispatch on the type itself. For example:</p><pre><code class="language-julia hljs">function load_model(_type::Type{HGFBloomModel}, cfg, state_dict, prefix)
    embed = load_model(_type, CompositeEmbedding, cfg, state_dict, prefix)
    decoder = load_model(_type, TransformerBlock, cfg, state_dict, prefix)
    return HGFBloomModel(embed, decoder)
end

function load_model(_type::Type{&lt;:HGFBloomPreTrainedModel}, ::Type{&lt;:CompositeEmbedding}, cfg, state_dict, prefix)
    vocab_size, dims = cfg[:vocab_size], cfg[:hidden_size]
    factor = Float32(cfg[:initializer_range])
    token_embed = _load_embed(state_dict, joinname(prefix, &quot;word_embeddings&quot;), vocab_size, dims, factor)
    embed = CompositeEmbedding(token = token_embed)
    ln = load_model(_type, Layers.LayerNorm, cfg, state_dict, joinname(prefix, &quot;word_embeddings_layernorm&quot;))
    return Layers.Chain(embed, ln)
end

...</code></pre><p><code>load_model(_type::Type{HGFBloomModel}, cfg, state_dict, prefix)</code> isa the main function to overload for loading <code>HGFBloomModel</code>. <code>cfg</code> is the <code>HGFConfig</code> of the loaded model. <code>state_dict</code> is the model weights loaded by <code>Transformers.HuggingFace.load_state_dict</code> which is a dictionay map from flattened python property names to the weight array (e.g. <code>&quot;word_embeddings.weight&quot; =&gt; Float16[-0.00987 -0.00481 …</code>). Since our model does not follow the same hierarchy and field names, we use <code>prefix</code> and <code>Transformers.HuggingFace.joinname(prefix, another_field_name)</code> to mimic the traversal. We use the corresponding information to reconstruct the correct model with <code>Transformers.Layers</code>. The weight access is hide in the <code>_load_embed</code> function, which is defined as:</p><pre><code class="language-julia hljs">function _load_embed(state_dict, prefix, w_init, pad_idx0 = nothing)
    embedding = getweight(Layers.Embed, state_dict, joinname(prefix, &quot;weight&quot;)) do
        weight = w_init()
        if !isnothing(pad_idx0)
            weight[:, pad_idx0 + 1] .= 0
        end
        return weight
    end
    return Layers.Embed(embedding)
end</code></pre><p>All weight accesses are done with <code>getweight</code> which check if the name (<code>joinname(prefix, &quot;weight&quot;)</code>) is present in the <code>state_dict</code>, and if not, create a new array and store it in the <code>state_dict</code> with the name. This allow us to handle the case that some weight are missing, like using the pretrained model for finetuning on a new task. Besides, we need to overload <code>basemodelkey(::Type{&lt;:HGFPreTrained{:bloom}}) = :transformer</code> for loading the model correctly. This is equivalent to the <a href="https://github.com/huggingface/transformers/blob/fa21ead73db473d88f8eca1ec244aba776fd9047/src/transformers/models/bloom/modeling_bloom.py#L446"><code>base_model_prefix</code></a> class variable of <code>BloomPreTrainedModel</code> in <code>&quot;modeling_bloom.py&quot;</code> (<code>https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py</code>).</p><h4 id="3.-Overload-Transformers.HuggingFace.get_state_dict"><a class="docs-heading-anchor" href="#3.-Overload-Transformers.HuggingFace.get_state_dict">3. Overload <code>Transformers.HuggingFace.get_state_dict</code></a><a id="3.-Overload-Transformers.HuggingFace.get_state_dict-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Overload-Transformers.HuggingFace.get_state_dict" title="Permalink"></a></h4><p>After finishing the loader, we also need to overload the <code>Transformers.HuggingFace.get_state_dict</code> which extract all weights in a model and store in a flat dictionary. Essentially, <code>model::HGFBloomModel == load_model(HGFBloomModel, cfg, get_state_dict(model))</code>. <code>get_state_dict</code> is dispatch on the model. For example:</p><pre><code class="language-julia hljs">function get_state_dict(m::HGFBloomModel, state_dict, prefix)
    get_state_dict(HGFBloomModel, m.embed[1], state_dict, prefix)
    get_state_dict(HGFBloomModel, m.embed[2], state_dict, joinname(prefix, &quot;word_embeddings_layernorm&quot;))
    get_state_dict(HGFBloomModel, m.decoder[1], state_dict, prefix)
    get_state_dict(HGFBloomModel, m.decoder[2], state_dict, joinname(prefix, &quot;ln_f&quot;))
    return state_dict
end

function get_state_dict(p::Type{&lt;:HGFBloomPreTrainedModel}, m::CompositeEmbedding, state_dict, prefix)
    get_state_dict(p, m.token, state_dict, joinname(prefix, &quot;word_embeddings&quot;))
    return state_dict
end

get_state_dict(_, m::Layers.Embed, state_dict, prefix) = get_state_dict(m, state_dict, prefix)
function get_state_dict(m::Layers.Embed, state_dict, prefix)
    state_dict[joinname(prefix, &quot;weight&quot;)] = m.embeddings&#39;
    return state_dict
end
...</code></pre><h3 id="Validation"><a class="docs-heading-anchor" href="#Validation">Validation</a><a id="Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Validation" title="Permalink"></a></h3><p>After implementing the model, we use the same <a href="https://github.com/chengchingwen/Transformers.jl/tree/master/example/HuggingFaceValidation">script</a> mentioned in the tokenizer part to check if our model perform the same computation as Python.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../huggingface/">« User Interface</a><a class="docs-footer-nextpage" href="../api_ref/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 27 June 2024 19:09">Thursday 27 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
