<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic · Transformers.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Transformers.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>Basic</a><ul class="internal"><li><a class="tocitem" href="#Transformer-1"><span>Transformer</span></a></li><li><a class="tocitem" href="#Positionwise-1"><span>Positionwise</span></a></li><li><a class="tocitem" href="#PositionEmbedding-1"><span>PositionEmbedding</span></a></li><li><a class="tocitem" href="#API-Reference-1"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../stacks/">Stacks</a></li><li><a class="tocitem" href="../pretrain/">Pretrain</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../gpt/">GPT</a></li><li><a class="tocitem" href="../bert/">BERT</a></li></ul></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Basic</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basic</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/basic.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers.Basic-1"><a class="docs-heading-anchor" href="#Transformers.Basic-1">Transformers.Basic</a><a class="docs-heading-anchor-permalink" href="#Transformers.Basic-1" title="Permalink"></a></h1><p>Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.</p><h2 id="Transformer-1"><a class="docs-heading-anchor" href="#Transformer-1">Transformer</a><a class="docs-heading-anchor-permalink" href="#Transformer-1" title="Permalink"></a></h2><p>The <code>Transformer</code> and <code>TransformerDecoder</code> is the encoder and decoder block of the origin paper, and they are all implement as the  regular Flux Layer, so you can treat them just as the <code>Dense</code> layer. See the docstring for the argument. However, for the sequence  data input, we usually have a 3 dimensional input of shape <code>(hidden size, sequence length, batch size)</code> instead of just <code>(hidden size, batch size)</code>.  Therefore, we implement both 2d &amp; 3d operation according to the input type (The <code>N</code> of <code>Array{T, N}</code>). We are able to handle both input of shape  <code>(hidden size, sequence length, batch size)</code> and <code>(hidden size, sequence length)</code> for the case with only 1 input.</p><pre><code class="language-julia">using Transfomers

m = Transformer(512, 8, 64, 2048) #define a Transformer block with 8 head and 64 neuron for each head
x = randn(512, 30, 3) #fake data of length 30

y = m(x)</code></pre><h2 id="Positionwise-1"><a class="docs-heading-anchor" href="#Positionwise-1">Positionwise</a><a class="docs-heading-anchor-permalink" href="#Positionwise-1" title="Permalink"></a></h2><p>For the sequential task, we need to handle the 3 dimensional input. However, most of the layer in Flux only support input with shape  <code>(hidden size, batch size)</code>. In order to tackle this problem, we implement the <code>Positionwise</code> helper function that is almost the same  as <code>Flux.Chain</code> but it will run the model position-wisely. (internally it just reshape the input to 2d and apply the model then reshape  back). </p><pre><code class="language-julia">using Transformers
using Flux

m = Positionwise(Dense(10, 5), Dense(5, 2), softmax)
x = randn(10, 30, 3)

y = m(x)

# which is equivalent to 
# 
# m = Chain(Dense(10, 5), Dense(5, 2), softmax)
# x1 = randn(10, 30)
# x2 = randn(10, 30)
# x3 = randn(10, 30)
# y = cat(m(x1), m(x2), m(x3); dims=3)</code></pre><h2 id="PositionEmbedding-1"><a class="docs-heading-anchor" href="#PositionEmbedding-1">PositionEmbedding</a><a class="docs-heading-anchor-permalink" href="#PositionEmbedding-1" title="Permalink"></a></h2><p>We implement two kinds of position embedding, one is based on the sin/cos function (mentioned in the paper,  attention is all you need). Another one is just like regular word embedding but with the position index. The  first argument is the <code>size</code>. Since the position embedding is only related to the length of the input ( we use <code>size(input, 2)</code> as the length), the return value of the layer will be the embedding of the given  length without duplicate to the batch size. you can/should use broadcast add to get the desired output.</p><pre><code class="language-julia"># sin/cos based position embedding which is not trainable
pe = PositionEmbedding(10) # or PositionEmbedding(10; trainable = false)

# trainable position embedding need to specify the maximum length
pe = PositionEmbedding(10, 1024; trainable = true)

x = randn(Float32, 10, 6, 3) #fake data of shape (10, length = 6, batched_size = 3)

e = pe(x) #get the position embedding
y = x .+ e # add the position embedding to each sample</code></pre><h2 id="API-Reference-1"><a class="docs-heading-anchor" href="#API-Reference-1">API Reference</a><a class="docs-heading-anchor-permalink" href="#API-Reference-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.CompositeEmbedding-Tuple{}" href="#Transformers.Basic.CompositeEmbedding-Tuple{}"><code>Transformers.Basic.CompositeEmbedding</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">CompositeEmbedding(;postprocessor=identity, es...)</code></pre><p>composite several embedding into one embedding according the aggregate methods and apply <code>postprocessor</code> on it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/etype.jl#L14-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Embed" href="#Transformers.Basic.Embed"><code>Transformers.Basic.Embed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Embed(size::Int, vocab_size::Int)</code></pre><p>The Embedding Layer, <code>size</code> is the hidden size. <code>vocab_size</code> is the number of the vocabulary. Just a wrapper for embedding matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/embed.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.PositionEmbedding" href="#Transformers.Basic.PositionEmbedding"><code>Transformers.Basic.PositionEmbedding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PositionEmbedding(size::Int, max_len::Int = 1024; trainable::Bool = false)</code></pre><p>The position embedding layer. <code>size</code> is the number of neuron. <code>max_len</code> is the maximum acceptable length of input. If is not <code>trainable</code>, <code>max_len</code> will dynamically adjust to the longest input length. If <code>trainable</code>, use a random init embedding value, otherwise use a sin/cos position encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/position_embed.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Positionwise" href="#Transformers.Basic.Positionwise"><code>Transformers.Basic.Positionwise</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Positionwise(layers)</code></pre><p>just like <code>Flux.Chain</code>, but reshape input to 2d and reshape back when output. Work exactly the same as <code>Flux.Chain</code> when input is 2d array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/extend3d.jl#L26-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Transformer-Tuple{Int64,Int64,Int64}" href="#Transformers.Basic.Transformer-Tuple{Int64,Int64,Int64}"><code>Transformers.Basic.Transformer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Transformer(size::Int, head::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)
Transformer(size::Int, head::Int, hs::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)</code></pre><p>Transformer layer.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  When <code>future</code> is <code>false</code>, the k-th token can&#39;t see the j-th tokens where j &gt; k. <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/transformer.jl#L36-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.TransformerDecoder-Tuple{Int64,Int64,Int64}" href="#Transformers.Basic.TransformerDecoder-Tuple{Int64,Int64,Int64}"><code>Transformers.Basic.TransformerDecoder</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)
TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)</code></pre><p>TransformerDecoder layer. Decode the value from a Encoder.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/transformer.jl#L108-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.TransformerModel" href="#Transformers.Basic.TransformerModel"><code>Transformers.Basic.TransformerModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TransformerModel(embed::AbstractEmbed, transformers::AbstractTransformer)
TransformerModel(
                  embed::AbstractEmbed,
                  transformers::AbstractTransformer,
                  classifier
                 )</code></pre><p>a structure for putting everything together</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/model.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Vocabulary" href="#Transformers.Basic.Vocabulary"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Vocabulary{T}(voc::Vector{T}, unk::T) where T</code></pre><p>struct for holding the vocabulary list to encode/decode input tokens.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/vocab.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Vocabulary-Tuple{Any}" href="#Transformers.Basic.Vocabulary-Tuple{Any}"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(vocab::Vocabulary)(x)</code></pre><p>encode the given data to the index encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/vocab.jl#L47-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.clear_classifier-Tuple{TransformerModel}" href="#Transformers.Basic.clear_classifier-Tuple{TransformerModel}"><code>Transformers.Basic.clear_classifier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">clear_classifier(model::TransformerModel)</code></pre><p>return a new TransformerModel without classifier.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/model.jl#L30-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.encode-Union{Tuple{W}, Tuple{T}, Tuple{Vocabulary{T},Union{T, W}}} where W where T" href="#Transformers.Basic.encode-Union{Tuple{W}, Tuple{T}, Tuple{Vocabulary{T},Union{T, W}}} where W where T"><code>Transformers.Basic.encode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">encode(vocab::Vocabulary, x)</code></pre><p>encode the given data to the index encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/vocab.jl#L27-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{Int64,N} where N}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{Int64,N} where N}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">gather(w::AbstractMatrix{T}, xs) where</code></pre><p>getting vector at the given indices, <code>xs</code> is a array of indices(<code>Int</code> type).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/gather.jl#L8-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},OneHotArray}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},OneHotArray}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">gather(w::AbstractMatrix{T}, xs::OneHotArray) where</code></pre><p>getting vector at the given onehot encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/gather.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,N} where N,AbstractArray{var&quot;#s17&quot;,N} where N where var&quot;#s17&quot;&lt;:Tuple}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,N} where N,AbstractArray{var&quot;#s17&quot;,N} where N where var&quot;#s17&quot;&lt;:Tuple}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">gather(w::AbstractArray{T}, xs) where</code></pre><p>getting vector at the given indices, <code>xs</code> is a array of cartesian indices(<code>Tuple{Int}</code> type).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/gather.jl#L23-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.getmask-Tuple{Union{Array{var&quot;#s17&quot;,1}, Tuple{Vararg{var&quot;#s17&quot;,N}}} where N where var&quot;#s17&quot;&lt;:(Union{Array{T,1}, Tuple{Vararg{T,N}}} where N where T)}" href="#Transformers.Basic.getmask-Tuple{Union{Array{var&quot;#s17&quot;,1}, Tuple{Vararg{var&quot;#s17&quot;,N}}} where N where var&quot;#s17&quot;&lt;:(Union{Array{T,1}, Tuple{Vararg{T,N}}} where N where T)}"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">getmask(ls::Container{&lt;:Container})</code></pre><p>get the mask for batched data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/vocab.jl#L99-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A,A}} where A&lt;:(AbstractArray{T,3} where T)" href="#Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A,A}} where A&lt;:(AbstractArray{T,3} where T)"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">getmask(m1::A, m2::A) where A &lt;: Abstract3DTensor</code></pre><p>get the mask for the covariance matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/vocab.jl#L114-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T" href="#Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the cross entropy with mask where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/loss.jl#L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T" href="#Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the cross entropy where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/loss.jl#L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T" href="#Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the kl divergence with mask where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/loss.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T" href="#Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the kl divergence where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/loss.jl#L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.onehot2indices-Tuple{OneHotArray}" href="#Transformers.Basic.onehot2indices-Tuple{OneHotArray}"><code>Transformers.Basic.onehot2indices</code></a> — <span class="docstring-category">Method</span></header><section><div><p>turn one hot encoding to indices</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/embeds/onehot.jl#L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.set_classifier-Tuple{TransformerModel,Any}" href="#Transformers.Basic.set_classifier-Tuple{TransformerModel,Any}"><code>Transformers.Basic.set_classifier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">set_classifier(model::TransformerModel, classifier)</code></pre><p>return a new TransformerModel whose classifier is set to <code>classifier</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/model.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.@toNd" href="#Transformers.Basic.@toNd"><code>Transformers.Basic.@toNd</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@toNd f(x, y, z...; a=a, b=b, c=c...) n</code></pre><p>macro for calling 2-d array function on N-d array by reshape input with reshape(x, size(x, 1), :) and reshape back with reshape(out, :, input[n][2:end]...) where n is the n-th input(default=1).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/extend3d.jl#L6-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.MultiheadAttention-NTuple{4,Int64}" href="#Transformers.Basic.MultiheadAttention-NTuple{4,Int64}"><code>Transformers.Basic.MultiheadAttention</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;
                   future::Bool=true, pdrop = 0.1)</code></pre><p>Multihead dot product Attention Layer, <code>head</code> is the number of head,  <code>is</code> is the input size, <code>hs</code> is the hidden size of input projection layer of each head,  <code>os</code> is the output size. When <code>future</code> is <code>false</code>, the k-th token can&#39;t see tokens at &gt; k.  <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/mh_atten.jl#L33-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.PwFFN" href="#Transformers.Basic.PwFFN"><code>Transformers.Basic.PwFFN</code></a> — <span class="docstring-category">Type</span></header><section><div><p>just a wrapper for two dense layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/576e3158e168a33fca044ba2b92c63cac4aa3c4d/src/basic/transformer.jl#L14">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Tutorial</a><a class="docs-footer-nextpage" href="../stacks/">Stacks »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 27 August 2020 19:11">Thursday 27 August 2020</span>. Using Julia version 1.5.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
